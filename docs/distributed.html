<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><title>Distributed Computing - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="distributed-computing">
<h1>Distributed Computing</h1>
<p>Background: decentralised computation</p>
<p>In this chapter, we will cover two topics:</p>
<ol type="1">
<li>Actor Engine</li>
<li>Barrier control, especially PSP</li>
</ol>
<p>Refer to <span data-cites="wang2017probabilistic" class="citation">(Wang, Catterall, and Mortier 2017)</span> for more detail.</p>
<section class="level2" id="actor-system">
<h2>Actor System</h2>
<p>Introduction: Distributed computing engines etc.</p>
<section class="level3" id="design">
<h3>Design</h3>
<p>(TODO: the design of actor’s functor stack; how network/barrier etc. are implemented as separated as different module. Connection with Mirage etc.)</p>
</section>
<section class="level3" id="actor-engines">
<h3>Actor Engines</h3>
<p>A key choice when designing systems for decentralised machine learning is the organisation of compute nodes. In the simplest case, models are trained in a centralised fashion on a single node leading to use of hardware accelerators such as GPUs and the TPU. For reasons indicated above, such as privacy and latency, decentralised machine learning is becoming more popular where data and model are spread across multiple compute nodes. Nodes compute over the data they hold, iteratively producing model updates for incorporation into the model, which is subsequently disseminated to all nodes. These compute nodes can be organised in various ways.</p>
<p>The Actor system has implemented core APIs in both map-reduce engine and parameter sever engine. Both map-reduce and parameter server engines need a (logical) centralised entity to coordinate all the nodes’ progress. To demonstrate PSP’s capability to transform an existing barrier control method into its fully distributed version, we also extended the parameter server engine to peer-to-peer (p2p) engine. The p2p engine can be used to implement both data and model parallel applications, both data and model parameters can be (although not necessarily) divided into multiple parts then distributed over different nodes.</p>
<p>Each engine has its own set of APIs. E.g., map-reduce engine includes <code>map</code>, <code>reduce</code>, <code>join</code>, <code>collect</code>, and etc.; whilst the peer-to-peer engine provides four major APIs: push, pull, schedule, barrier. It is worth noting there is one function shared by all the engines, i.e.&nbsp;barrier function which implements various barrier control mechanisms.</p>
<p>Next we will introduce these three different kinds of engines of Actor.</p>
</section>
<section class="level3" id="map-reduce-engine">
<h3>Map-Reduce Engine</h3>
<p>Following MapReduce <span data-cites="dean2008mapreduce" class="citation">(Dean and Ghemawat 2008)</span> programming model, nodes can be divided by tasks: either <em>map</em> or <em>reduce</em>. A map function processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function aggregates all the intermediate key/value paris with the same key. Execution of this model can automatically be paralleled. Mappers compute in parallel while reducers receive the output from all mappers and combine to produce the accumulated result. This parameter update is then broadcast to all nodes. Details such as distributed scheduling, data divide, and communication in the cluster are mostly transparent to the programmers so that they can focus on the logic of mappers and reducers in solving a problem within a large distributed system.</p>
<p>We can use a simple example to demonstrate this point. (with illustration, not code)</p>
<p>This simple functional style can be applied to a surprisingly wide range of applications.</p>
<p>Interfaces in Actor:</p>
<div class="highlight">
<pre><code class="language-clike">val map : ('a -&gt; 'b) -&gt; string -&gt; string

val reduce : ('a -&gt; 'a -&gt; 'a) -&gt; string -&gt; 'a option

val fold : ('a -&gt; 'b -&gt; 'a) -&gt; 'a -&gt; string -&gt; 'a

val filter : ('a -&gt; bool) -&gt; string -&gt; string

val shuffle : string -&gt; string

val union : string -&gt; string -&gt; string

val join : string -&gt; string -&gt; string

val collect : string -&gt; 'a list</code></pre>
</div>
<p>Example of using Map-Reduce in Actor: we use the classic wordcount example.</p>
<div class="highlight">
<pre><code class="language-clike">module Ctx = Actor.Mapre

let print_result x = List.iter (fun (k,v) -&gt; Printf.printf "%s : %i\n" k v) x

let stop_words = ["a";"are";"is";"in";"it";"that";"this";"and";"to";"of";"so";
  "will";"can";"which";"for";"on";"in";"an";"with";"the";"-"]

let wordcount () =
  Ctx.init Sys.argv.(1) "tcp://localhost:5555";
  Ctx.load "unix://data/wordcount.data"
  |&gt; Ctx.flatmap Str.(split (regexp "[ \t\n]"))
  |&gt; Ctx.map String.lowercase_ascii
  |&gt; Ctx.filter (fun x -&gt; (String.length x) &gt; 0)
  |&gt; Ctx.filter (fun x -&gt; not (List.mem x stop_words))
  |&gt; Ctx.map (fun k -&gt; (k,1))
  |&gt; Ctx.reduce_by_key (+)
  |&gt; Ctx.collect
  |&gt; List.flatten |&gt; print_result;
  Ctx.terminate ()

let _ = wordcount ()</code></pre>
</div>
</section>
<section class="level3" id="parameter-server-engine">
<h3>Parameter Server Engine</h3>
<p>The Parameter Server topology proposed by <span data-cites="li2014scaling" class="citation">(Li et al. 2014)</span> is similar: nodes are divided into servers holding the shared global view of the up-to-date model parameters, and workers, each holding its own view of the model and executing training. The workers and servers communicate in the format of key-value pairs. It is proposed to address of challenge of sharing large amount of parameters within a cluster. The parameter server paradigm applies an asynchronous task model to educe the overall network bandwidth, and also allows for flexible consistency, resource management, and fault tolerance.</p>
<p>Simple Example (distributed training) with illustration: <a href="https://miro.medium.com/max/371/1*6VRMmXkY3On-PJh8vNRHww.png">IMAGE</a>: Distributed training with Parameter Server (Src:<span data-cites="li2014scaling" class="citation">(Li et al. 2014)</span>)</p>
<p>According to this example, we can see that the Parameter Server paradigm mainly consists of four APIs for the users.</p>
<ul>
<li><p><code>schedule</code>: decide what model parameters should be computed to update in this step. It can be either a local decision or a central decision.</p></li>
<li><p><code>pull</code>: retrieve the updates of model parameters from somewhere then applies them to the local model. Furthermore, the local updates will be computed based on the scheduled model parameter.</p></li>
<li><p><code>push</code>: send the updates to the model plane. The updates can be sent to either a central server or to individual nodes depending on which engine is used(e.g., map-reduce, parameter server, or peer-to-peer).</p></li>
<li><p><code>barrier</code>: decide whether to advance the local step. Various synchronisation methods can be implemented. Besides the classic BSP, SSP, and ASP, we also implement the proposed PSP within this interface.</p></li>
</ul>
<p>The interfaces in Actor:</p>
<div class="highlight">
<pre><code class="language-clike">val start : ?barrier:barrier -&gt; string -&gt; string -&gt; unit
(** start running the model loop *)

val register_barrier : ps_barrier_typ -&gt; unit
(** register user-defined barrier function at p2p server *)

val register_schedule : ('a, 'b, 'c) ps_schedule_typ -&gt; unit
(** register user-defined scheduler *)

val register_pull : ('a, 'b, 'c) ps_pull_typ -&gt; unit
(** register user-defined pull function executed at master *)

val register_push : ('a, 'b, 'c) ps_push_typ -&gt; unit
(** register user-defined push function executed at worker *)

val register_stop : ps_stop_typ -&gt; unit
(** register stopping criterion function *)

val get : 'a -&gt; 'b * int
(** given a key, get its value and timestamp *)

val set : 'a -&gt; 'b -&gt; unit
(** given a key, set its value at master *)

val keys : unit -&gt; 'a list
(** return all the keys in a parameter server *)

val worker_num : unit -&gt; int
(** return the number of workers, only work at server side *)</code></pre>
</div>
<p>EXPLAIN</p>
<p>Example of using PS in Actor :</p>
<div class="highlight">
<pre><code class="language-clike">module PS = Actor_param

let schedule workers =
  let tasks = List.map (fun x -&gt;
    let k, v = Random.int 100, Random.int 1000 in (x, [(k,v)])
  ) workers in tasks

let push id vars =
  let updates = List.map (fun (k,v) -&gt;
    Owl_log.info "working on %i" v;
    (k,v) ) vars in
  updates

let test_context () =
  PS.register_schedule schedule;
  PS.register_push push;
  PS.start Sys.argv.(1) Actor_config.manager_addr;
  Owl_log.info "do some work at master node"

let _ = test_context ()</code></pre>
</div>
</section>
<section class="level3" id="peer-to-peer-engine">
<h3>Peer-to-Peer Engine</h3>
<p>In the above approaches the model parameter storage is managed by a set of centralised servers. In contrast, Peer-to-Peer (P2P) is a fully distributed structure, where each node contains its own copy of the model and nodes communicate directly with each other. The benefit of this approach.</p>
<p>Illustrate how distributed computing can be finished with P2P model, using a figure.</p>
<p>To obtain the aforementioned two pieces of information, we can organise the nodes into a structured overlay (e.g., chord or kademlia), the total number of nodes can be estimated by the density of each zone (i.e., a chunk of the name space with well-defined prefixes), given the node identifiers are uniformly distributed in the name space. Using a structured overlay in the design guarantees the following sampling process is correct, i.e., random sampling.</p>
<p>Implementation in Actor:</p>
<div class="highlight">
<pre><code class="language-clike">open Actor_types

(** start running the model loop *)
val start : string -&gt; string -&gt; unit

(** register user-defined barrier function at p2p server *)
val register_barrier : p2p_barrier_typ -&gt; unit

(** register user-defined pull function at p2p server *)
val register_pull : ('a, 'b) p2p_pull_typ -&gt; unit

(** register user-defined scheduler at p2p client *)
val register_schedule : 'a p2p_schedule_typ -&gt; unit

(** register user-defined push function at p2p client *)
val register_push : ('a, 'b) p2p_push_typ -&gt; unit

(** register stopping criterion function at p2p client *)
val register_stop : p2p_stop_typ -&gt; unit

(** given a key, get its value and timestamp *)
val get : 'a -&gt; 'b * int

(** given a key, set its value at master *)
val set : 'a -&gt; 'b -&gt; unit</code></pre>
</div>
<p>EXPLAIN</p>
<p>Example of using P2P in Actor (SGD):</p>
<div class="highlight">
<pre><code class="language-clike">open Owl
open Actor_types


module MX = Mat
module P2P = Actor_peer

...

let schedule _context =
  Owl_log.debug "%s: scheduling ..." !_context.master_addr;
  let n = MX.col_num !_model in
  let k = Stats.Rnd.uniform_int ~a:0 ~b:(n - 1) () in
  [ k ]

let push _context params =
  List.map (fun (k,v) -&gt;
    Owl_log.debug "%s: working on %i ..." !_context.master_addr k;
    let y = MX.col !data_y k in
    let d = calculate_gradient 10 !data_x y v !gradfn !lossfn in
    let d = MX.(d *$ !step_t) in
    (k, d)
  ) params

let barrier _context =
  Owl_log.debug "checking barrier ...";
  true

let pull _context updates =
  Owl_log.debug "pulling updates ...";
  List.map (fun (k,v,t) -&gt;
    let v0, _ = P2P.get k in
    let v1 = MX.(v0 - v) in
    k, v1, t
  ) updates

let stop _context = false

let start jid =
  P2P.register_barrier barrier;
  P2P.register_schedule schedule;
  P2P.register_push push;
  P2P.register_pull pull;
  P2P.register_stop stop;
  Owl_log.info "P2P: sdg algorithm starts running ...";
  P2P.start jid Actor_config.manager_addr</code></pre>
</div>
<p>EXPLAIN</p>
</section>
</section>
<section class="level2" id="classic-synchronise-parallel">
<h2>Classic Synchronise Parallel</h2>
<p>To ensure the correctness of computation, normally we need to make sure a correct order of updates. For example, one worker can only proceed when the model has been updated with all the workers’ updates from previous round. However, the iterative-convergent nature of ML programmes means that they are error-prone to a certain degree.</p>
<p>Most consistent system often leads to less than ideal system throughput. This error-proneness means that, the consistency can be relaxed a bit without sacrificing accuracy, and gains system performance at the same time. This trade-off is decided by the ``barrier’’ in distributed ML. A lot of research on it, both theoretically and practically.</p>
<p>Existing distributed processing systems operate at various points in the space of consistency/speed trade-offs. However, all effectively use one of three different synchronisation mechanisms: Bulk Synchronise Parallel (BSP), Stale Synchronise Parallel (SSP), and Asynchronous Parallel (ASP). These are depicted in fig.&nbsp;1.</p>
<figure>
<img alt="" id="fig:distributed:barriers" src="images/distributed/barriers.png"><figcaption>Figure 1: Barrier control methods used for synchronisation</figcaption>
</figure>
<section class="level3" id="bulk-synchronous-parallel">
<h3>Bulk Synchronous Parallel</h3>
<p>Bulk Synchronous Parallel (BSP) is a deterministic scheme where workers perform a computation phase followed by a synchronisation/communication phase where they exchange updates. The method ensures that all workers are on the same iteration of a computation by preventing any worker from proceeding to the next step until all can. Furthermore, the effects of the current computation are not made visible to other workers until the barrier has been passed. Provided the data and model of a distributed algorithm have been suitably scheduled, BSP programs are often serialisable – that is, they are equivalent to sequential computations. This means that the correctness guarantees of the serial program are often realisable making BSP the strongest barrier control method. Unfortunately, BSP does have a disadvantage. As workers must wait for others to finish, the presence of <em>stragglers</em>, workers which require more time to complete a step due to random and unpredictable factors, limit the computation efficiency to that of the slowest machine. This leads to a dramatic reduction in performance. Overall, BSP tends to offer high computation accuracy but suffers from poor efficiency in unfavourable environments.</p>
<p>BSP is the most strict lockstep synchronisation; all the nodes are coordinated by a central server. BSP is sensitive to stragglers so is very slow. But it is simple due to its deterministic nature, easy to write application on top of it.</p>
</section>
<section class="level3" id="asynchronous-parallel">
<h3>Asynchronous Parallel</h3>
<p>Asynchronous Parallel (ASP)} takes the opposite approach to BSP, allowing computations to execute as fast as possible by running workers completely asynchronously. In homogeneous environments (e.g.&nbsp;data centers), wherein the workers have similar configurations, ASP enables fast convergence because it permits the highest iteration throughputs. Typically, <span class="math inline">\(P\)</span>-fold speed-ups can be achieved by adding more computation/storage/bandwidth resources. However, such asynchrony causes delayed updates: updates calculated on an old model state which should have been applied earlier but were not. Applying them introduces noise and error into the computation. Consequently, ASP suffers from decreased iteration quality and may even diverge in unfavourable environments. Overall, ASP offers excellent speed-ups in convergence but has a greater risk of diverging especially in a heterogeneous context.</p>
<p>ASP is the Least strict synchronisation, no communication among workers for barrier synchronisation all all. Every computer can progress as fast as it can. It is fast and scalable, but often produces noisy updates. No theoretical guarantees on consistency and algorithm’s convergence.</p>
</section>
<section class="level3" id="stale-synchronous-parallel">
<h3>Stale Synchronous Parallel</h3>
<p>Stale Synchronous Parallel (SSP) is a bounded asynchronous model which can be viewed as a relaxation of BSP. Rather than requiring all workers to be on the same iteration, the system decides if a worker may proceed based on how far behind the slowest worker is, i.e.&nbsp;a pre-defined bounded staleness. Specifically, a worker which is more than <span class="math inline">\(s\)</span> iterations behind the fastest worker is considered too slow. If such a worker is present, the system pauses faster workers until the straggler catches up. This <span class="math inline">\(s\)</span> is known as the <em>staleness</em> parameter. More formally, each machine keeps an iteration counter, <span class="math inline">\(c\)</span>, which it updates whenever it completes an iteration. Each worker also maintains a local view of the model state. After each iteration, a worker commits updates, i.e., <span class="math inline">\(\Delta\)</span>, which the system then sends to other workers, along with the worker’s updated counter. The bounding of clock differences through the staleness parameter means that the local model cannot contain updates older than <span class="math inline">\(c -s - 1\)</span> iterations. This limits the potential error. Note that systems typically enforce a <em>read-my-writes</em> consistency model. The staleness parameter allows SSP to provide deterministic convergence guarantees. Note that SSP is a generalisation of BSP: setting <span class="math inline">\(s = 0\)</span> yields the BSP method, whilst setting <span class="math inline">\(s = \infty\)</span> produces ASP. Overall, SSP offers a good compromise between fully deterministic BSP and fully asynchronous ASP~, despite the fact that the central server still needs to maintain the global state to guarantee its determinism nature.</p>
<p>SSP relaxes consistency by allowing difference in iteration rate. The difference is controlled by the bounded staleness. SSP is supposed to mitigate the negative effects of stragglers. But the server still requires global state.</p>
</section>
</section>
<section class="level2" id="probabilistic-synchronise-parallel">
<h2>Probabilistic Synchronise Parallel</h2>
<p>Existing barrier methods allow us to balance consistency against iteration rate in attempting to achieve a high rate of convergence. In particular, SSP parameterises the spectrum between ASP and BSP by introducing a staleness parameter, allowing some degree of asynchrony between nodes so long as no node lags too far behind. Figure~ depicts this trade-off.</p>
<p>However, in contrast to a highly reliable and homogeneous datacentre context, let’s assume a distributed system consisting of a larger amount of heterogeneous nodes that are distributed at a much larger geographical areas. The network is unreliable since links can break down, and the bandwidth is heterogeneous. The nodes are not static, they can join and leave the system at any time. We observe that BSP and SSP are not suitable for this scenario, since both are centralised mechanisms: a single server is responsible for receiving updates from each node and declaring when the next iteration can proceed.</p>
<p>In contrast, ASP is fully distributed and no single node is responsible for the system making progress. We examine this trade-off in more detail, and suggest that the addition of a new <em>sampling</em> primitive exposes a second axis on which solutions can be placed, leading to greater flexibility in how barrier controls can be implemented. We call the resulting barrier control method (or, more precisely, family of barrier control methods) <em>Probabilistic Synchronous Parallel</em>, or PSP.</p>
<section class="level3" id="basic-idea-sampling">
<h3>Basic idea: sampling</h3>
<p>The idea of PSP is simple: in a unreliable environment, we can minimise the impact of outliers and stragglers by guaranteeing the majority of the system have synchronised boundary. The reason we can drop the results from certain portion of workers is that, practically many iterative learning algorithms can tolerate certain level of errors in the process of converging to final solutions. Given a well-defined boundary, if most of the nodes have passed it, the impact of those lagged nodes should be minimised.</p>
<p>Therefore, in PSP, all we need to do is to estimate what percent of nodes have passed a given synchronisation barrier. Two pieces of information is required to answer this question: - an estimate on the total number of nodes in the system; - an estimate of the distribution of current steps of the nodes.</p>
<p>In PSP, either a central oracle tracks the progress of each worker or the workers each hold their own local view. In a centralised system, without considering the difficulty of monitoring state of all the nodes as system grows bigger, these two pieces of information is apparently trivial to get at a central server. However, in a distributed system, where each node does not have global knowledge of other nodes, how can it get these information? In that case, a node randomly selects a subset of nodes in the system and query their individual current local step. By so doing, it obtains a sample of the current nodes’ steps in the whole system. By investigating the distribution of these observed steps, it can derive an estimate of the percentage of nodes which have passed a given step. After deriving the estimate on the step distribution, a node can choose to either pass the barrier by advancing its local step if a given threshold has been reached (with certain probability) or simply holds until certain condition is satisfied. Each node only depends on several other nodes to decide its own barrier.</p>
</section>
<section class="level3" id="compatibility">
<h3>Compatibility</h3>
<figure>
<img alt="" id="fig:distributed:psp_00" src="images/distributed/psp_00.png"><figcaption>Figure 2: Probabilistic Synchronous Parallel example</figcaption>
</figure>
<p>One great advantage of PSP is its compatibility with existing synchronisation methods. For classic BSP and SSP, their barrier functions are called by the centralised server to check the synchronisation condition with the given inputs. The output of the function is a boolean decision variable on whether or not to cross the synchronisation barrier, depending on whether the criterion specified in the algorithm is met. With the proposed sampling primitive, almost nothing needs to be changed in aforementioned algorithms except that only the sampled states instead of the global states are passed into the barrier function. Therefore, we can easily derive the probabilistic version of BSP and SSP, namely <em>pBSP</em> and <em>pSSP</em>.</p>
<p>PSP improves on ASP by providing probabilistic guarantees about convergence with tighter bounds and less restrictive assumptions. pSSP relaxes SSP’s inflexible staleness parameter by permitting some workers to fall further behind. pBSP relaxes BSP by allowing some workers to lag slightly, yielding a BSP-like method which is more resistant to stragglers but no longer deterministic.</p>
<p>fig.&nbsp;2(a) depicts PSP showing that different subsets of the population of nodes operate in (possibly overlapping) groups, applying one or other barrier control method within their group. In this case, fig.&nbsp;2(b) shows PSP using BSP within group (or pBSP), and fig.&nbsp;2(c) shows PSP using SSP within group (or pSSP).</p>
<p>Formally, at the barrier control point, a worker samples <span class="math inline">\(\beta\)</span> out of <span class="math inline">\(P\)</span> workers without replacement. If a single one of these lags more than <span class="math inline">\(s\)</span> steps behind the current worker then it waits. This process is pBSP (based on BSP) if <span class="math inline">\(s = 0\)</span> and pSSP (based on SSP) if <span class="math inline">\(s &gt; 0\)</span>. However, if <span class="math inline">\(s = \infty\)</span> then PSP reduces to ASP.</p>
</section>
<section class="level3" id="barrier-trade-off-dimensions">
<h3>Barrier Trade-off Dimensions</h3>
<figure>
<img alt="" id="fig:distributed:psp_01" src="images/distributed/psp_01.png"><figcaption>Figure 3: Extra trade-off exposed through PSP</figcaption>
</figure>
<p>This allows us to decouple the degree of synchronisation from the degree of distribution, introducing <em>completeness</em> as a second axis by having each node sample from the population. Within each sampled subgroup, traditional mechanisms can be applied allowing overall progress to be robust against the effect of stragglers while also ensuring a degree of consistency between iterations as the algorithm progresses. As fig.&nbsp;3(a-b) depicts, the result is a larger design space for synchronisation methods when operating distributed data processing at scale.</p>
<p>As fig.&nbsp;3(c) summarises, probabilistic sampling allows us greater flexibility in designing synchronisation mechanisms in distributed processing systems at scale. When compared with BSP and SSP, we can obtain faster convergence through faster iterations and with no dependence on a single centralised server. When compared with ASP, we can obtain faster convergence with stronger guarantees by providing greater consistency between updates.</p>
<p>Besides its compatibility with existing synchronisation methods, it is also worth emphasising that applying sampling leads to the biggest difference between the classic synchronisation control and probabilistic control: namely the original synchronisation control requires a centralised node to hold the global state whereas the derived probabilistic ones no longer require such information thus can be executed independently on each individual node, further leading to a fully distributed solution.</p>
</section>
<section class="level3" id="convergence">
<h3>Convergence</h3>
<p>At the barrier control point, every worker samples <span class="math inline">\(\beta\)</span> out of <span class="math inline">\(P\)</span> workers without replacement. If a single one of these lags more than <span class="math inline">\(s\)</span> steps behind the current worker then it waits. The probabilities of a node lagging <span class="math inline">\(r\)</span> steps are drawn from a distribution with probability mass function <span class="math inline">\(f(r)\)</span> and cumulative distribution function (CDF) <span class="math inline">\(F(r)\)</span>. Both <span class="math inline">\(r\)</span> and <span class="math inline">\(\beta\)</span> can be thought of as constant value.</p>
<p>In a distributing machine learning process, these <span class="math inline">\(P\)</span> workers keep generating updates, and the model is updated with them continuously. In this sequence of updates, each one is indexed by <span class="math inline">\(t\)</span> (which does not mean clock time), and the total length of this sequence is <span class="math inline">\(T\)</span>. Ideally, in a fully deterministic barrier control system, such as BSP, the ordering of updates in this sequence should be fixed. We call it a <em>true sequence</em>. However, in reality, what we get is often a <em>noisy sequence</em>, where updates are reordered due to sporadic and random network and system delays. The difference, or lag, between the order of these two sequence, is denoted by <span class="math inline">\(\gamma_{t}\)</span>.</p>
<p>Without talking too much about math in detail in this book, to prove the convergence of PSP requires to construct and idea sequence of updates, each generated by workers in the distributed learning, and compare it with the actual sequence after applying PSP. The target of proof is to show that, given sufficient time <span class="math inline">\(t\)</span>, the difference between these two sequences <span class="math inline">\(\gamma_t\)</span> is limited.</p>
<p>The complete proof of convergence is too long to fit into this chapter. Please refer to <span data-cites="wang2017probabilistic" class="citation">(Wang, Catterall, and Mortier 2017)</span> for more detail if you are interested in the math. One key step in the proof is to show that the mean and variance of <span class="math inline">\(\gamma_t\)</span> are bounded. The average of the mean is bounded by:</p>
<p><span id="eq:distributed:bound_mean"><span class="math display">\[\frac{1}{T} \sum_{t=0}^{T} E(\gamma_{t}) \leq  S \left( \frac{r(r+1)}{2} + \frac{a(r + 2)}{(1-a)^{2}} \right).\qquad(1)\]</span></span></p>
<p>The average of the variance has a similar bound:</p>
<p><span id="eq:distributed:bound_var"><span class="math display">\[\frac{1}{T}  \sum_{t=0}^{T} E(\gamma_{t}^{2}) &lt; S \left(\frac{r(r+1)(2r+1)}{6} + \frac{a(r^{2} + 4)}{(1-a)^{3}} \right),\qquad(2)\]</span></span></p>
<p>where <span id="eq:distributed:bound_s"><span class="math display">\[S = \frac{1-a}{F(r)(1-a) + a - a^{T-r+1}}.\qquad(3)\]</span></span></p>
<p>The intuition is that, when applying PSP, the update sequence we get will not be too different from the true sequence. To demonstrate the impact of the sampling primitive on bounds quantitatively, fig.&nbsp;4 shows how increasing the sampling count, <span class="math inline">\(\beta\)</span>, (from 1, 5, to 100, marked with different line colours on the right) yields tighter bounds. The sampling count <span class="math inline">\(\beta\)</span> is varied between 1 and 100 and marked with different line colours on the right. The staleness, <span class="math inline">\(r\)</span>, is set to 4 with <span class="math inline">\(T\)</span> equal to 10000. The bounds on <span class="math inline">\(\gamma_t\)</span> mean that what a true sequence achieves, in time, a noisy sequence can also achieve, regardless of the order of updates. Notably, only a small number of nodes need to be sampled to yield bounds close to the optimal. This result has an important implication to justify using sampling primitive in large distributed learning systems due to its effectiveness.</p>
<figure>
<img alt="" id="fig:distributed:proof_exp" src="images/distributed/proof_exp.png"><figcaption>Figure 4: Plot showing the bound on the average of the means and variances of the sampling distribution.</figcaption>
</figure>
</section>
</section>
<section class="level2" id="a-distributed-training-example">
<h2>A Distributed Training Example</h2>
<p>In this section, we investigate performance of the proposed PSP in experiments. We focus on two common metrics in evaluating barrier strategies: the step progress and accuracy. We use the training of a DNN as an example, using a 9-layer structure used in the Neural Network chapter, and for the training we also use the MNIST handwritten digits dataset. The learning rate has a decay factor of <span class="math inline">\(1e4\)</span>. The network strucuture is shown below:</p>
<div class="highlight">
<pre><code class="language-clike">let make_network () =
  input [|28;28;1|]
  |&gt; normalisation ~decay:0.9
  |&gt; conv2d [|5;5;1;32|] [|1;1|] ~act_typ:Activation.Relu
  |&gt; max_pool2d [|2;2|] [|2;2|]
  |&gt; dropout 0.1
  |&gt; fully_connected 1024 ~act_typ:Activation.Relu
  |&gt; linear 10 ~act_typ:Activation.(Softmax 1)
  |&gt; get_network</code></pre>
</div>
<p>We use both real-world experiments and simulations to evaluate different barrier control methods. The experiments run on 6 nodes using Actor. To extend the scale, we have also built a simulation platform. For both cases, we implement the Parameter Server framework. It consists of one server and many worker nodes. In each step, a worker takes a chunk of training data, calculates the weight value, and then aggregates these updates to the parameter server, thus updating the shared model iteratively. A worker pulls new model from server after it is updated.</p>
<section class="level3" id="step-progress">
<h3>Step Progress</h3>
<p>First, we are going to investigate if PSP achieves faster iteration speed. We use 200 workers, and run the simulation for 200 simulated seconds. fig.&nbsp;5 shows the distribution of all nodes’ step progress when simulation is finished.</p>
<figure>
<img alt="" id="fig:distributed:exp_step_01" src="images/distributed/exp_step_01.png"><figcaption>Figure 5: Progress distribution in steps</figcaption>
</figure>
<p>As expected, the most strict BSP leads to a tightly bounded step distribution, but at the same time, using BSP makes all the nodes progress slowly. At the end of simulation, all the nodes only proceed to about 30th step. As a comparison, using ASP leads to a much faster progress of around 100 steps. But the cost is a much loosely spread distribution, which shows no synchronisation at all among nodes. SSP allows certain staleness (4 in our experiment) and sits between BSP and ASP.</p>
<p>PSP shows another dimension of performance tuning. We set sample size <span class="math inline">\(\beta\)</span> to 10, i.e.&nbsp;a sampling ratio of only 5%. The result shows that pBSP is almost as tightly bound as BSP and also much faster than BSP itself. The same is also true when comparing pSSP and SSP. In both cases, PSP improves the iteration efficiency while limiting dispersion.</p>
<figure>
<img alt="" id="fig:distributed:exp_step_02" src="images/distributed/exp_step_02.png"><figcaption>Figure 6: pBSP parameterised by different sample sizes, from 0 to 64.</figcaption>
</figure>
<p>To further investigate the impact of sample size, we focus on BSP, and choose different sample sizes. In fig.&nbsp;6 we vary the sample size from 0 to 64. As we increase the sample size step by step, the curves start shifting from right to left with tighter and tighter spread, indicating less variance in nodes’ progress. With sample size 0, the pBSP exhibits exactly the same behaviour as that of ASP; with increased sample size, pBSP starts becoming more similar to SSP and BSP with tighter requirements on synchronisation. pBSP of sample size 4 behaves very close to SSP.</p>
<p>Another interesting thing we notice in the experiment is that, with a very small sample size of one or two (i.e., very small communication cost on each individual node), pBSP can already effectively synchronise most of the nodes comparing to ASP. The tail caused by stragglers can be further trimmed by using larger sample size. This observation confirms our convergence analysis, which explicitly shows that a small sample size can effectively push the probabilistic convergence guarantee to its optimum even for a large system size, which further indicates the superior scalability of the proposed solution.</p>
</section>
<section class="level3" id="accuracy">
<h3>Accuracy</h3>
<p>Next, we evaluate barrier control methods in training a deep neural network using MNIST dataset. We use inference accuracy on test dataset as measurement of performance.</p>
<p>To run the code, we mainly implement the interfaces we mentioned before: <code>sche</code>, <code>push</code>, and <code>pull</code>. <code>sche</code> and <code>pull</code> are performed on server, and <code>push</code> is on worker.</p>
<div class="highlight">
<pre><code class="language-clike">  let schd nodes =
    Actor_log.info "#nodes: %d" (Array.length nodes);
    if (Array.length nodes &gt; 0) then (
      eval_server_model ()
    );

    let server_value = (get [|key_name|]).(0) in
    Array.map (fun node -&gt;
      Actor_log.info "node: %s schd" node;
      let wid = int_of_uuid node in
      let v = make_task server_value.weight wid in
      let tasks = [|(key_name, v)|] in
      (node, tasks)
    ) nodes

let pull kv_pairs =
    Gc.compact ();
    (* knowing that there is only one key...*)
    let key = key_name in
    Actor_log.info "pull: %s (length: %d) " key (Array.length kv_pairs);

    let s_value = (get [|key|]).(0) in
    let s_weight = ref s_value.weight in
    Array.iter (fun (_, v) -&gt;
      s_weight := add_weight !s_weight v.weight;
      model.clocks.(v.wid) &lt;- model.clocks.(v.wid) + 1
    ) kv_pairs;
    let value = make_task !s_weight 0 in (* wid here is meaningless *)
    [|(key, value)|]
</code></pre>
</div>
<p>And on worker:</p>
<div class="highlight">
<pre><code class="language-clike">let push kv_pairs =
    Gc.compact ();
    Array.map (fun (k, v) -&gt;
      Actor_log.info "push: %s" k;
      (* simmulated communication delay *)
      
      (* let t = delay.(v.wid) in *)
      let t = Owl_stats.gamma_rvs ~shape:1. ~scale:1. in
      Unix.sleepf t;

      let nn = make_network () in
      Graph.init nn;
      Graph.update nn v.weight;
      let x, y = get_next_batch v.wid in
      let x = pack x in
      let y = pack y in
      let _ = CGCompiler.train ~params ~init_model:false nn x y in
      let delta = delta_nn nn v.weight in
      let value = make_task delta v.wid in
      (k, value)
    ) kv_pairs</code></pre>
</div>
<p>TODO: Explain the code</p>
<figure>
<img alt="" id="fig:distributed:exp_accuracy_01" src="images/distributed/exp_accuracy_01.png"><figcaption>Figure 7: MNIST training using Actor</figcaption>
</figure>
<p>In fig.&nbsp;7, we conduct the real-world experiments using 6 worker nodes with the Parameter Server framework we have implemented. We run the training process for a fixed amount of time, and observe the performance of barrier methods given the same number of updates. It shows that BSP achieves the highest model accuracy with the least of number of updates, while SSP and ASP achieve lower efficiency. With training progressing, both methods show a tendency to diverge. By applying sampling, pBSP and pSSP achieve smoother accuracy progress.</p>
</section>
</section>
<section class="level2 unnumbered" id="references">
<h2 class="unnumbered">References</h2>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-dean2008mapreduce">
<p>Dean, Jeffrey, and Sanjay Ghemawat. 2008. “MapReduce: Simplified Data Processing on Large Clusters.” <em>Communications of the ACM</em> 51 (1): 107–13.</p>
</div>
<div id="ref-li2014scaling">
<p>Li, Mu, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J Shekita, and Bor-Yiing Su. 2014. “Scaling Distributed Machine Learning with the Parameter Server.” In <em>11th <span class="math inline">\(\{\)</span>Usenix<span class="math inline">\(\}\)</span> Symposium on Operating Systems Design and Implementation (<span class="math inline">\(\{\)</span>Osdi<span class="math inline">\(\}\)</span> 14)</em>, 583–98.</p>
</div>
<div id="ref-wang2017probabilistic">
<p>Wang, Liang, Ben Catterall, and Richard Mortier. 2017. “Probabilistic Synchronous Parallel.” <em>arXiv Preprint arXiv:1709.07772</em>.</p>
</div>
</div>
</section>
</section>
</article></div><a href="testing.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 26</small>Testing Framework</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>