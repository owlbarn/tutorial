<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><meta content="OCaml Scientific and Engineering Computing - Tutorial Book" name="description"><meta content="OCaml, Data Science, Data Analytics, Analytics, Functional Programming, Machine Learning, Deep Neural Network, Scientific Computing, Numerical Algorithm, Tutorial, Linear Algebra, Matrix" name="keywords"><meta content="Liang Wang" name="author"><title>Natural Language Processing - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script><script data-ad-client="ca-pub-1868946892712371" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-123353217-1"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-123353217-1');</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="natural-language-processing">
<h1>Natural Language Processing</h1>
<p>Text is a dominant media type on the Internet along with images, videos, and audios. Many of our day-to-day tasks involve text analysis. Natural language processing (NLP) is a powerful tool to extract insights from text corpora.</p>
<p>NLP is a large topic that covers many different advanced problems, such as speech tagging, named entity recognition, machine translation, speech recognition, etc. We surely cannot cover all of them in this one single chapter, perhaps not even a whole book. To this end, in this chapter we mainly focus on the vector space models and topic modelling.</p>
<p>TODO: Explain Topic modelling briefly</p>
<p>TODO: this chapter now mainly lacks general text introduction of NLP.</p>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<p>Survey the literature, give a high-level picture of NLP. Talk about classic NLP … structured and unstructured text …</p>
<p>In this chapter, we will use a <a href="https://github.com/ryanrhymes/owl_dataset/raw/master/news.txt.gz">news dataset</a> crawled from the Internet. It contains 130000 pieces of news from various sources, each line in the file represents one entry. For example we the first line/document is:</p>
<div class="highlight">
<pre><code class="language-text">a brazilian man who earns a living by selling space for tattoo adverts on his body is now looking for a customer for his forehead , it appears ... borim says when clients do n't pay or cancel an ad , he crosses them out . " skinvertising " caused a stir in the mid-2000s , when many dot.com companies experimented with it...</code></pre>
</div>
</section>
<section class="level2" id="text-corpus">
<h2>Text Corpus</h2>
<p>Normally we call a collection of documents a <em>text corpus</em>, which contains a large and structured set of texts. For example, for the English language there are the <a href="https://www.english-corpora.org/coca/">Corpus of Contemporary American English</a>, <a href="https://corpling.uis.georgetown.edu/gum">Georgetown University Multilayer Corpus</a>, etc. Our news collection is also one such example. To perform NLP tasks such as topic modelling, the first and perhaps the most important thing is to represent a text corpus as format that the models can process, instead of directly using natural language.</p>
<p>TODO: A survey of annotation methods.</p>
<p>For the task of topic modelling, we perform the tokenisation on the input English text. The target is to represent each word as an integer index so that we can further process the numbers instead of words. This is called the <em>tokenisation</em> of the text. Of course we also need to have a mapping function that from index to word.</p>
<section class="level3" id="step-by-step-operation">
<h3>Step-by-step Operation</h3>
<p>The NLP module in Owl supports building a proper text corpus from given text dataset. In this section we will show how we can build a corpus from a collection of documents, in a step step way.</p>
<p>In the first step, remove the special characters. We define a regular expression <code>regexp_split</code> for special characters such as <code>,</code>, <code>?</code>, <code>\t</code> etc. First remove them, and then convert all the text into lower-case. The code below define such a process function, and the <code>Nlp.Corpus.preprocess</code> apply it to all the text. Note this function will not change the number of lines in a corpus.</p>
<div class="highlight">
<pre><code class="language-ocaml">let simple_process s =
  Str.split Owl_nlp_utils.regexp_split s
  |&gt; List.filter (fun x -&gt; String.length x &gt; 1)
  |&gt; String.concat " "
  |&gt; String.lowercase_ascii
  |&gt; Bytes.of_string

let preprocess input_file =
  let output_file = input_file ^ ".output" in
  Nlp.Corpus.preprocess simple_process input_file output_file</code></pre>
</div>
<p>Based on the processed text corpus, we can build the <em>vocabulary</em>. Each word is assigned a number id, or index, and we have the dictionary to map word to index, and index to word. This is achieved by using the <code>Nlp.Vocabulary.build</code> function.</p>
<div class="highlight">
<pre><code class="language-ocaml">let build_vocabulary input_file =
  let vocab = Nlp.Vocabulary.build input_file in
  let output_file = input_file ^ ".vocab" in
  Nlp.Vocabulary.save vocab output_file</code></pre>
</div>
<p>The <code>build</code> function returns a vocabulary. It contains three <code>Hasthtbl</code>s. The first maps a word to an index, and the second index to word. The last hash table is a map between index and its frequency, i.e.&nbsp;number of occurrence in the whole text body. We can check out the words of highest frequency with:</p>
<div class="highlight">
<pre><code class="language-ocaml">let print_freq vocab =
  Nlp.Vocabulary.top vocab 10 |&gt;
  Owl.Utils.Array.to_string ~sep:", " fst</code></pre>
</div>
<p>Unsurprisingly, the “the”’s and “a”’s are most frequently used:</p>
<div class="highlight">
<pre><code class="language-text">- : string =
"the, to, of, a, and, in, \", s, that, on"</code></pre>
</div>
<p>Change <code>Nlp.Vocabulary.top</code> to <code>Nlp.Vocabulary.bottom</code> can shows the words of lowest frequency:</p>
<div class="highlight">
<pre><code class="language-text">"eichorst, gcs, freeross, depoliticisation, humping, shopable, appurify, intersperse, vyaecheslav, raphaelle"</code></pre>
</div>
<p>However, in a topic modelling task, we don’t want these too frequent but meaningless words and perhaps also the least frequent words that are not about the topic of this document. Now let’s trim off some most and least frequency words. You can trim either by absolute number or by percent. We use percent here, namely trimming off top and bottom 1% of the words.</p>
<div class="highlight">
<pre><code class="language-ocaml">let trim_vocabulary vocab =
  Nlp.Vocabulary.trim_percent ~lo:0.01 ~hi:0.01 vocab</code></pre>
</div>
<p>With a proper vocabulary at hands, now we are ready to tokenise a piece of text.</p>
<div class="highlight">
<pre><code class="language-ocaml">let tokenise vocab text =
  String.split_on_char ' ' text |&gt;
  List.map (Nlp.Vocabulary.word2index vocab)</code></pre>
</div>
<p>For example, if we tokenise “this is owl book”, you will get the following output.</p>
<div class="highlight">
<pre><code class="language-text">tokenise vocab "this is an owl book";;
- : int list = [55756; 18322; 109456; 90661; 22362]</code></pre>
</div>
<p>Furthermore, we can now tokenise the whole news collection.</p>
<div class="highlight">
<pre><code class="language-ocaml">let tokenise_all vocab input_file =
  let doc_s = Owl_utils.Stack.make () in
  Owl_io.iteri_lines_of_file
    (fun i s -&gt;
      let t =
        Str.split Owl_nlp_utils.regexp_split s
        |&gt; List.filter (Owl_nlp_vocabulary.exits_w vocab)
        |&gt; List.map (Owl_nlp_vocabulary.word2index vocab)
        |&gt; Array.of_list
      in
      Owl_utils.Stack.push doc_s i)
    input_file;
  doc_s</code></pre>
</div>
<p>The process is simple: in the text corpus each line is a document and we iterate through the text line by line. For each line/document, we remove the special characters, filter out the words that exist in the vocabulary, and map each word to an integer index accordingly. Even though this is a simplified case, it well illustrates the typical starting point of text analysis before delving into any topic modelling.</p>
</section>
<section class="level3" id="use-the-corpus-module">
<h3>Use the Corpus Module</h3>
<p>But we don’t have to build a text corpus step by step. We provide the <code>NLP.Corpus</code> module for convenience. By using the <code>Nlp.Corpus.build</code> we perform both tasks we have introduced: building vocabulary, and tokenising the text corpus. With this function we can also specify how to trim off the high-frequency and low-frequency words. Here is an example:</p>
<div class="highlight">
<pre><code class="language-ocaml">let main () =
  let ids = Nlp.Corpus.unique "news.txt" "clean.txt" in
  Printf.printf "removed %i duplicates." (Array.length ids);
  let corpus = Nlp.Corpus.build ~lo:0.01 ~hi:0.01 "clean.txt" in
  Nlp.Corpus.print corpus</code></pre>
</div>
<p>The <code>Nlp.Corpus.unique</code> function is just one more layer of pre-processing. It removes the possible duplicated lines/documents. The output prints out the processing progress, and then a summary of the corpus is printed out.</p>
<div class="highlight">
<pre><code class="language-text">2020-01-28 19:07:05.461 INFO : build up vocabulary ...
2020-01-28 19:07:10.461 INFO : processed 13587, avg. 2717 docs/s
2020-01-28 19:07:15.463 INFO : processed 26447, avg. 2644 docs/s
...
2020-01-28 19:08:09.125 INFO : convert to binary and tokenise ...
2020-01-28 19:08:34.130 INFO : processed 52628, avg. 2104 docs/s
2020-01-28 19:08:39.132 INFO : processed 55727, avg. 1857 docs/s
...
corpus info
  file path  : news.txt
  # of docs  : 129968
  doc minlen : 10
- : unit = ()</code></pre>
</div>
<p>The corpus contains xxx parts: the vocabulary, token, and text string. By calling the <code>build</code> function, we also save them for later use. It creates several files in the current directory. First, there is the vocabulary file <code>news.txt.voc</code> and <code>news.txt.voc.txt</code>. They are the same; only that the latter is in a human-readable format that has each line a word and the corresponding index number. We can get the vocabulary with <code>Corpus.get_vocab</code>.</p>
<p>The tokenised text corpus is marshalled to the <code>news.txt.tok</code> file, and the string format content is saved as binary file to <code>news.txt.bin</code>. We choose to save the content as binary format to save file size. To get the i-th document, we can use <code>Corpus.get corpus i</code> to get the text string, or <code>Corpus.get_tok corpus i</code> to get an integer array that is tokenised version of this document.</p>
<p>To efficiently access different documents by the document index (line number), we keep track of the accumulated length of text corpus and token array after processing each document. These two type of indexes are saved in the <code>news.txt.mdl</code> file. This file also contains the document id. We have seen the <code>minlen</code> value in the output of corpus information. Each document with less than 10 words will not be included in the corpus. The document id is an int array that shows the index (line number) of each document in the original text corpus so that it can be traced back. The document id can be retrieved by <code>Corpus.get_docid corpus</code></p>
<p>In the <code>Corpus</code> module, we provide three mechanisms to iterate through the text corpus: <code>next</code>, <code>iteri</code>, <code>mapi</code>. The <code>next</code> function is a generator that yields the next line of text document string in the text corpus until it hits the end of file. The <code>iteri</code> and <code>mapi</code> functions work exactly like in the normal Array module. The first function iterates all the documents one by one in the corpus, and the second maps all the documents in a corpus into another array. The <code>iteri_tok</code> and <code>mapi_tok</code> work the same, except that the function should work on integer array instead of string. Their signatures is shown below:</p>
<div class="highlight">
<pre><code class="language-text">val iteri : (int -&gt; string -&gt; unit) -&gt; t -&gt; unit

val iteri_tok : (int -&gt; int array -&gt; unit) -&gt; t -&gt; unit

val mapi : (int -&gt; string -&gt; 'a) -&gt; t -&gt; 'a array

val mapi_tok : (int -&gt; 'a -&gt; 'b) -&gt; t -&gt; 'b array</code></pre>
</div>
<p>The <code>Corpus</code> module is designed to support a large number of text corpus. With this tool in hand, we can further proceed with the discussion of topic modelling.</p>
</section>
</section>
<section class="level2" id="vector-space-models">
<h2>Vector Space Models</h2>
<p>Based on the tokenised text corpus, the next thing we need is a mathematical model to express abstract ideas such as “this sentence makes sense and that one does not”, “these two documents are similar”, or “the key word in that paragraph is such and such”. To perform NLP tasks such as text retrieval and topic modelling, we use the <em>Vector Space Model</em> (VSM) to do that.</p>
<p>According to the wikipedia, a VSM is “an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers”. It may sounds tricky but the basic idea is actually very simple. For example, let’s assume we only care about three topics in any news: covid19, economics, and election. Then we can represent any news article with a three-element vector, each representing the weight of this topic in it. For the BBC news <a href="https://www.bbc.co.uk/news/uk-52462928">“Coronavirus: Millions more to be eligible for testing”</a>, we can represent it with vector <code>(100, 2.5, 0)</code>. The specific value does not actually matter here. The point is that now instead of a large chunks of text corpus, we only need to deal with this vector for further processing.</p>
<p>The vector space model proposes a framework that maps a document to a vector <span class="math inline">\(d = (x_1, x_1, \ldots, x_N)\)</span>. This N-dimensional vector space is defined by <span class="math inline">\(N\)</span> basic terms. Under this framework, we mainly have to decide on three factors. The first is to choose the meaning of each dimension, or the <span class="math inline">\(N\)</span> basic concepts in the vector space. The second is to specify the weight of each dimension for a document. In our simple example, why do we assign the first weight to <code>100</code> instead of <code>50</code>? There should be rules about it. That means we need a proper mapping function <span class="math inline">\(f\)</span> defined. Finally, after learning the vector representation, how should we measure their similarity? The similarity of document is a basic idea in text processing. For topic modelling, we can cluster the documents based on their similarity. (TODO: Extend this point)</p>
<p>In this chapter we focusing on mapping a document to a vector space. However, VSM is not limited to only documents. We can also map a word into a vector that represents a point in a certain vector space. This vector is also called <em>word embedding</em>. In a proper representation, the similar words should be cluster together, and can even be used for calculation such as:</p>
<p><span class="math display">\[V_\textrm{king} - V_\textrm{man} + V_\textrm{women} \approx V_\textrm{queen}.\]</span></p>
<p>One of the most widely used method for word embedding is the <code>word2vec</code> proposed in <span data-cites="mikolov2013exploiting" class="citation">(Mikolov, Le, and Sutskever 2013)</span>. It includes different algorithms such as the skip-gram for computing the vector representation of words. For general purpose use, Google has already published a <a href="https://code.google.com/archive/p/word2vec/">pre-trained</a> word2vec-based word embedding vector set based on part of the GoogleNews dataset. This vector set contains 300-dimensional vectors for 3 million words and phrases.</p>
<p>Back to the theme of mapping documents to vector space. In the next chapter, we will start with a simple method that instantiate the VSM: the Bag of Words.</p>
</section>
<section class="level2" id="bag-of-words-bow">
<h2>Bag of Words (BOW)</h2>
<p>The Bag of Words is a simple way to map docs into a vector space. This space uses all the vocabulary as the dimensions. Suppose there are totally <span class="math inline">\(N\)</span> different words in the vocabulary, then the vector space is of <span class="math inline">\(N\)</span> dimension. The mapping function is simply counting how many times each word in the vocabulary appears in a document.</p>
<p>For example, let’s use the five words “news”, “about”, “coronavirus”, “test”, “cases” as the five dimensions in the vector space. Then if a document is <code>"...we heard news a new coronavirus vaccine is being developed which is expected to be tested about September..."</code> will be represented as <code>[1, 1, 1, 1, 0]</code> and the document <code>"...number of positive coronavirus cases is 100 and cumulative cases are 1000..."</code> will be projected to vector <code>[0, 0, 1, 0, 2]</code>.</p>
<p>This Bag of Words method is easy to implement based on the text corpus. We first define a function that count the term occurrence in a document and return a hash table:</p>
<div class="highlight">
<pre><code class="language-ocaml">let term_count htbl doc =
  Array.iter
    (fun w -&gt;
      match Hashtbl.mem htbl w with
      | true  -&gt;
        let a = Hashtbl.find htbl w in
        Hashtbl.replace htbl w (a +. 1.)
      | false -&gt; Hashtbl.add htbl w 1.)
    doc</code></pre>
</div>
<p>The hash table contains all the counts of words in this document. Of course, we can also represent the returned results as an array of integers, though the array would likely be sparse. Then we can apply this function to all the documents in the corpus using the map function:</p>
<div class="highlight">
<pre><code class="language-ocaml">let build_bow corpus =
  Nlp.Corpus.mapi_tok
    (fun i doc -&gt;
      let htbl = Hashtbl.create 128 in
      term_count htbl doc;
      htbl)
    corpus</code></pre>
</div>
<p>Based on this bag of words, the similarity between two vectors can be measured using different methods, e.g.&nbsp;with a simple dot product.</p>
<p>This method is easy to implement and the computation is inexpensive. It maybe simple, but for some tasks, especially those that has no strict requirement for context or position of words, this method proves to work well. For example, to cluster spam email, we only need to specify proper keywords as dimensions, such as “income”, “bonus”, “extra”, “cash”, “free”, “refund”, “promise” etc. We can expect that the spam email texts will be clustered closely and easy to recognise in this vector space using the bag of words.</p>
<p>Actually, one even simpler method is called Boolean model. Instead of term frequency (count of word), the table only contains 1 or 0 to indicate if a word is present in a document. This approach might also benefit from its simplicity and proved to be useful in certain tasks, but it loses the information about the importance of the word. One can easily construct a document that is close to everyone else, by putting all the vocabulary together. The bag of word method fixes this problem.</p>
<p>On the other hand, this simple approach does have its own problems. Back to the previous example, if we want to get how close the a document is to <code>"news about coronavirus test cases"</code>, then the doc <code>"...number of positive coronavirus cases is 100 and cumulative cases are 1000..."</code> is scored the same as <code>"hey, I got some good news about your math test result..."</code>. This is not what we expected. Intuitively, words like “coronavirus” should matter more than the more normal words like “test” and “about”. That’s why we are going to introduce an improved method in the next section.</p>
</section>
<section class="level2" id="term-frequencyinverse-document-frequency-tf-idf">
<h2>Term Frequency–Inverse Document Frequency (TF-IDF)</h2>
<p>In this previous section, we use the count of each term in representing document as vector. It is a way to represent the frequency the term in the document, and we can call it <em>term frequency</em>. In the previous section we have seen the intuition that the meaning of different word should be different. This cannot be fixed by simply using term count. In this section we introduce the idea of <em>Inverse Document Frequency</em> (IDF) to address this problem.</p>
<p>The basic idea is simple. The IDF is used to represent how common a word is across all the documents. You can imagine that if a word is used throughout all the documents, then it must be of less importance in determining a feature of a document. On the other hand, if a word exists in only 1-2 documents, and where it exists, this word must be of crucial importance to determine its topic. Therefore, the IDF factor can be multiplied with the term frequency to present a more accurate metric for representing a document as vector. This approach is called TF-IDF.</p>
<p>Actually, the two parts TF and IDF are just a framework for different computation methods. To compute the term frequency, we can use the count of words <span class="math inline">\(c\)</span>, or the percentage of word in the current document <span class="math inline">\(\frac{c}{N}\)</span> where <span class="math inline">\(N\)</span> is the total number of words in the document. Another computation method is logarithm normalisation which use <span class="math inline">\(\textrm{log}(c + 1)\)</span>. We can even use the boolean count that take the frequency of word that exists to be 1 that the ones that are not to be 0. These methods are all defined in the <code>Owl_nlp.Tfidf</code> module.</p>
<div class="highlight">
<pre><code class="language-ocaml">type tf_typ =
  | Binary
  | Count
  | Frequency
  | Log_norm</code></pre>
</div>
<p>The same goes for the IDF. To measure how common a word <span class="math inline">\(w\)</span> is across all the document, a common way to compute is to do: <span class="math inline">\(log(\frac{N_D}{n_w})\)</span>, where <span class="math inline">\(N_D\)</span> is the total number of documents and <span class="math inline">\(n_w\)</span> is the number of documents with term <span class="math inline">\(w\)</span> in it. This metric is within the range of <span class="math inline">\([0, \infty]\)</span>. It increases with larger total document number or smaller number of documents that contain a specific word. An improved version is called <code>Idf_Smooth</code>. It is calculated as <span class="math inline">\(log(\frac{N_D}{n_w + 1})\)</span>. This method avoid the <span class="math inline">\(n_w\)</span> to be zero to cause divide error, and also avoid getting a <code>0</code> for a word just because it used across all the documents. In Owl they are included in the type <code>df_typ</code>. Here the <code>Unary</code> method implies not using IDF, only term frequency.</p>
<div class="highlight">
<pre><code class="language-ocaml">type df_typ =
  | Unary
  | Idf
  | Idf_Smooth</code></pre>
</div>
<p>We provide the <code>Owl_nlp.Tfidf</code> module to perform the TF-IDF method. The corpus we have built in the previous section is used as input to it. Specifically, we use the <code>Nlp.Tfidf.build</code> function to build the TFIDF model:</p>
<div class="highlight">
<pre><code class="language-ocaml">let build_tfidf corpus =
  let tf = Nlp.Tfidf.Count in
  let df = Nlp.Tfidf.Idf in
  let model = Nlp.Tfidf.build ~tf ~df corpus in
  Nlp.Tfidf.save model "news.tfidf";
  model</code></pre>
</div>
<p>In this code, we configure to use the bag-of-words style word count method to calculate term frequency, and use the normal logarithm method to compute inverse document frequency. The model can be saved for later use. After the model is build, we can search similar documents according to a given string. As a random example, let’s just use the first sentence in our first piece of news in the dataset as search target: <code>"a brazilian man who earns a living by selling space for tattoo adverts on his body is now looking for a customer for his forehead"</code>.</p>
<div class="highlight">
<pre><code class="language-ocaml">let query model doc k =
  let typ = Owl_nlp_similarity.Cosine in
  let vec = Nlp.Tfidf.apply model doc in
  let knn = Nlp.Tfidf.nearest ~typ model vec k in
  knn</code></pre>
</div>
<p>Recall the three gradients in vector space model: choosing dimension topic words, mapping document to vector, and the measurement of similarity. Here we use the <em>consine similarity</em> as a way to measure how aligned two vectors <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are. We will talk about the similarity measurement in detail later.</p>
<p>Next, the <code>vec</code> returned by the <code>apply</code> functions return an array of <code>(int * float)</code> tuples. For each item, the integer is the tokenised index of a word in the input document <code>doc</code>, and the float number is the corresponding TF-IDF value, based on the <code>model</code> we get from previous step. Finally, the <code>nearest</code> function search all the documents and find the vectors that has the largest similarity with the target document. Let’s show the top-10 result by setting <code>k</code> to 10:</p>
<div class="highlight">
<pre><code class="language-text">val knn : (int * float) array =
  [|(11473, -783.546068863270875); (87636, -669.76533603535529);
    (121966, -633.92555577720907); (57239, -554.838541799660675);
    (15810, -550.95468134048258); (15817, -550.775276912183131);
    (15815, -550.775276912183131); (83282, -547.322385552312426);
    (44647, -526.074567425088844); (0, -496.924176137374445)|]</code></pre>
</div>
<p>The returned result shows the id of the matched documents. We can retrieve each document by running e.g.&nbsp;<code>Owl_nlp.Corpus.get corpus 11473</code>. (TODO: what is the second number?) To save you some effort to do that, here we list link to some of the original news that are matched to be similar to the target document:</p>
<ol type="1">
<li><em>Every tatto tells a story</em>, doc id: 11473. [<a href="https://www.bbc.co.uk/news/magazine-27831231">Link</a>]</li>
<li><em>The Complete Guide to Using Social Media for Customer Service</em>, doc id: 87636. [<a href="https://buffer.com/resources/social-media-for-customer-service-guide">Link</a>]</li>
<li><em>Murder ink? Tattoos can be tricky as evidence</em>, doc id: 57239. [<a href="https://www.gazettenet.com/Archives/2014/06/tattoos-hg-060514">Link</a>]</li>
<li><em>Scottish independence: Cinemas pull referendum adverts</em>, doc id: 15810. [<a href="https://www.bbc.co.uk/news/uk-scotland-scotland-politics-27602601">Link</a>]</li>
<li><em>The profusion of temporarily Brazilian-themed products</em>, doc id:44647. [<a href="https://www.bbc.co.uk/news/magazine-27879430">Link</a>]</li>
</ol>
<p>If you are interested, the input document comes from <a href="https://www.bbc.co.uk/news/blogs-news-from-elsewhere-27051009">this</a> BBC news: <em>Brazil: Man ‘earns a living’ from tattoo ads</em>. Then you can see that, the searched result is actually quite related to the input document, especially the first one, which is exactly the same story written in another piece of news. The second result is somewhat distant. The word “customer” is heavily used in this document, and we can guess that it is also not frequently seen throughout the text corpus. The fourth news is not about the tattoo guy, but this news features the topic of “customer” and “adverts”. The fifth news is chosen apparently because of the non-frequent word “brazilian” carries a lot of weight in TF-IDF. The interesting thing is that the same document, the first document, is ranked only 10th closest. Note that we just simply take a random sentence without any preprocessing or keyword design, also we use the un-trimmed version of text corpus. Even so, we can still achieves a somewhat satisfactory matching result, and the result fits nicely with the working mechanisms of the TF-IDF method.</p>
</section>
<section class="level2" id="latent-dirichlet-allocation-lda">
<h2>Latent Dirichlet Allocation (LDA)</h2>
<p>In the previous section, we have seen that by specifying a document and using it as a query, we can find out the similar documents as the query. The query document itself is actually seen as a collection of words.<br>
However, the real world text, article or news, are rarely as simple as collections of words. More often than not, an article contains one or more <em>topics</em>. For example, it can involves the responsibility of government, the protection of environment, and a recent protest in the city, etc. Moreover, each of these topics can hardly be totally covered by just one single word. To this end we introduce the problem <em>topic modelling</em>: instead of proposing a search query to find similar content in text corpus, we hope to automatically cluster the documents according to several topics, and each topic is represented by several words.</p>
<p>One of such method to do topic modelling is called <em>Latent Dirichlet Allocation</em> (LDA). The trained model of LDA containS two matrices. The first is called the “document-topic”, which contains the number of tokens assigned to each topic in each doc. What do these topics look like then? This concerns the other trained matrix in the model: the “word-topic table”. It contains the number of tokens assigned to each topic for each word. We will see how they work in a latter example. But first, some theory background.</p>
<section class="level3" id="models">
<h3>Models</h3>
<p>let’s take a look at the model of LDA that is proposed in <span data-cites="blei2003latent" class="citation">(Blei, Ng, and Jordan 2003)</span>. That is to say, how the LDA thinks about the way a document is composed. The model is expressed in fig.&nbsp;1.</p>
<figure>
<img alt="" style="width:60.0%" id="fig:nlp:lda" title="lda" src="images/nlp/lda.png"><figcaption>Figure 1: Plate notation for LDA with Dirichlet-distributed topic-word distributions</figcaption>
</figure>
<p>This model uses the <a href="https://en.wikipedia.org/wiki/Plate_notation">plate notation</a>, the notation for describing probabilistic graphical models, to capture the dependencies among variables. And in tbl.&nbsp;1 we list the definition of the math notations used here and latter in this section.</p>
<div id="tbl:nlp:lda">
<table>
<caption>Table 1: Variable notations in the LDA model</caption>
<colgroup>
<col style="width: 21%">
<col style="width: 78%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Variable</th>
<th style="text-align: left;">Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(K\)</span></td>
<td style="text-align: left;">number of topics</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(D\)</span></td>
<td style="text-align: left;">number of documents in text corpus</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(V\)</span></td>
<td style="text-align: left;">number of words in the vocabulary</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(N\)</span></td>
<td style="text-align: left;">total number or words in all document</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\alpha\)</span></td>
<td style="text-align: left;">vector of length <span class="math inline">\(K\)</span>, prior weight of the <span class="math inline">\(K\)</span> topics in a document</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\beta\)</span></td>
<td style="text-align: left;">vector of length <span class="math inline">\(V\)</span>, prior weight of the <span class="math inline">\(V\)</span> words in a topic</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(\Theta\)</span></td>
<td style="text-align: left;">vector of length <span class="math inline">\(K\)</span>, distribution of topics in a document</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(\phi\)</span></td>
<td style="text-align: left;">vector of length <span class="math inline">\(V\)</span>, distribution of words in a topic</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(Z\)</span></td>
<td style="text-align: left;">matrix of shape <span class="math inline">\(D\times~V\)</span>, topic assignment of all words in all documents</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(W\)</span></td>
<td style="text-align: left;">matrix of shape <span class="math inline">\(D\times~V\)</span>, token of words in all documents</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(n_{d,k}\)</span></td>
<td style="text-align: left;">how many times the document <span class="math inline">\(d\)</span> uses topic <span class="math inline">\(k\)</span> in the document-topic table</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(m_{k,w}\)</span></td>
<td style="text-align: left;">the number of times topic <span class="math inline">\(k\)</span> uses word <span class="math inline">\(w\)</span> in the topic-word table</td>
</tr>
</tbody>
</table>
</div>
<p>In this model, to infer the topics in a corpus, we imagine a <strong>generative process</strong> to create a document. The core idea here is that each document can be described by the distribution of topics, and each topic can be described by distribution of words. This makes sense, since we don’t need the text in order to find the topics in an article. The process is as follows:</p>
<ol type="1">
<li>Initialise the distribution of topics <span class="math inline">\(\theta_d \sim \textrm{Dirichlet}(\alpha)\)</span> in document <span class="math inline">\(d\)</span>. Here <span class="math inline">\(\textrm{Dirichlet}(\alpha)\)</span> is a Dirichlet distribution parameterised by <span class="math inline">\(\alpha\)</span>. We will talk about it in detail later.</li>
<li>Initialise the distribution of words <span class="math inline">\(\phi_k \sim \textrm{Dirichlet}(\beta)\)</span> for topic <span class="math inline">\(k\)</span>.</li>
<li>We iterate each document <span class="math inline">\(d\)</span>, and each word position <span class="math inline">\(w\)</span>, and then performs the steps below:
<ol type="1">
<li>first, picks one of these topics randomly (one of the elements in <span class="math inline">\(Z\)</span>). Specifically, the choice of topic is actually taken according to a <a href="https://en.wikipedia.org/wiki/Categorical_distribution">categorical distribution</a>, parameterised by <span class="math inline">\(\theta\)</span>. Formally, this step is represented as <span class="math inline">\(Z_{d,w} \sim \textrm{Categorical}(\theta_d)\)</span>;</li>
<li>second, according to the words this topic contains, we can pick a word randomly according to <span class="math inline">\(\phi\)</span>. The picking process also follows categorical distribution: <span class="math inline">\(W_{d,w} \sim \textrm{Categorical}(\phi_{Z_{d,w}})\)</span>.</li>
</ol></li>
</ol>
<p>After finishing this generative process, We now have a “fake” document. The total probability of the model is:</p>
<p><span id="eq:nlp:lda0"><span class="math display">\[P(W, Z, \theta, \phi; \alpha, \beta) = \prod_{i=1}^K~P(\phi_i; \beta)~\prod_{j=1}^D~P(\theta_j; \alpha)~\prod_{t=1}^N~P(Z_{j,t}| \theta_j)~P(W_{j,t}| \phi_{Z_{j,t}}).\qquad(1)\]</span></span></p>
<p>eq.&nbsp;1 corresponds to the above process and model in fig.&nbsp;1 step by step. It is multiplication of three parts: the probability of <span class="math inline">\(\theta\)</span> across all the documents, the probability of <span class="math inline">\(\phi\)</span> across all the topics, and that of the generated words across all documents. The LDA hopes to make this generated document to be close to a real document as much as possible. In another word, when we are looking at real document, LDA tries to maximise the possibility eq.&nbsp;1 that this document can be generated from a set of topics.</p>
</section>
<section class="level3" id="dirichlet-distribution">
<h3>Dirichlet Distribution</h3>
<p>There are something we need to add to the generative process in the previous section. How <span class="math inline">\(theta\)</span> and <span class="math inline">\(phi\)</span> are generated? Randomly? No, that would not be a proper way. Think about what would happen if we randomly initialise the document-topic table: each document will be equally likely to contain any topic. But that’s rarely the case. An article cannot talk about all the topics at the same time. What we really hope however, is that a single document belongs to a single topic, which is a more real-world scenario.<br>
The same goes for the word-topic table.</p>
<p>To that end, LDA uses the <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet Distribution</a> to perform this task. It is a family of continuous multivariate probability distribution parameterised by a vector <span class="math inline">\(\alpha\)</span>. For example, suppose we have only two topics in the whole world. The tuple <code>(0, 1)</code> means it’s totally about one topic, and <code>(1,0)</code> means its totally about the other. We can run the <code>Stats.dirichlet_rvs</code> function to generate such a pair of float numbers. The results are shown in fig.&nbsp;2. Both figures have the same number of dots. It shows that with smaller <span class="math inline">\(\alpha\)</span> value, the distribution are pushed to the corners, where it is obviously about one topic or the other. A larger <span class="math inline">\(\alpha\)</span> value, however, makes the topic concentrate around the middle where it’s a mixture of both topics.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:nlp:dirichlet" title="dirichlet" src="images/nlp/dirichlet.png"><figcaption>Figure 2: Two dimensional dirichlet distribution with different alpha parameters</figcaption>
</figure>
<p>Therefore, in the model in fig.&nbsp;1, we have two parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> as prior weights to initialise <span class="math inline">\(\Theta\)</span> and <span class="math inline">\(\phi\)</span> respectively. We use a reasonably small parameters to have skewed probability distributions where only a small set of topics or words have high probability.</p>
</section>
<section class="level3" id="gibbs-sampling">
<h3>Gibbs Sampling</h3>
<p>Next, we will briefly introduce how the training algorithm works to get the topics using LDA. The basic idea is that we go through the documents one by one. Each word is initially assigned a random topic from the Dirichlet distribution. After that, we iterate over all the documents again and again. In each iterate, we look at each word, and try to find a hopefully a bit more proper topic for this word. In this process, we assume that all the other topic assignments in the whole text corpus are correct except for the current word we are looking at. Then we move forward to the next word in this document. In one iteration, we process all the words in all the documents in the same way. After enough iterations, we can get a quite accurate assignment for each word. And then of course the topics of each document would be clear.</p>
<p>We need to further explain some details in this general description. The most important question is, in the sampling of a document, how exactly do we update the topic assignment of a word? We use the <em>Gibbs Sampling</em> algorithm to approximate the distribution of <span class="math inline">\(P(Z | W; \alpha, \beta)\)</span>. For this word, we expect to get a vector of length <span class="math inline">\(k\)</span> where <span class="math inline">\(k\)</span> is the number of topics. It represents a conditional probability distribution of a one word topic assignment conditioned on the rest of the model. Based on eq.&nbsp;1, it can be derived that, in this distribution vector, the k-th element is:</p>
<p><span class="math display">\[p(Z_{d,n}=k | Z_{-d,n}, W, \alpha, \beta) = \frac{n_{d,k} + \alpha_k}{\sum_{i=1}^K~(n_{d,i} + \alpha_i)}~\frac{m_{k,w_{d.n}} + \beta_{w_{d,n}}}{\sum_i~m_{k, i} + \beta_i}.\]</span></p>
<p>Here <span class="math inline">\(w_{d,n}\)</span> is the current word we are looking at. To perform the sampling, we assume that only the current topic assignment to <span class="math inline">\(w_{d,n}\)</span> is wrong, so we remove the current assignment from the model before this round of iteration. <span class="math inline">\(Z\)</span> is the topic assignment of all words in all documents, and <span class="math inline">\(W\)</span> is the text corpus.</p>
<p>This computations is multiplication of two parts. As shown in tbl.&nbsp;1, in the first part, <span class="math inline">\(n_{d,k}\)</span> show how many times the document <span class="math inline">\(d\)</span> uses topic <span class="math inline">\(k\)</span>, and <span class="math inline">\(\alpha_k\)</span> is the prior weight of topic <span class="math inline">\(k\)</span> in document. Therefore, this item means the percentage of words that are also assigned the same topic in the whole document To put it more simply, it shows how much this document likes topic <span class="math inline">\(k\)</span>. The larger it is, the more likely we will assign the current word to topic <span class="math inline">\(k\)</span> again. Similarly, the second part is the percentage of words that are also assigned the same topic in the whole document. Therefore, this item indicate how a topic likes the word <span class="math inline">\(w\)</span>. Larger number means <span class="math inline">\(w\)</span> will continue be assigned this topic <span class="math inline">\(k\)</span> again.</p>
<p>Finally, we multiply these two items to get the final distribution of probability for the word <span class="math inline">\(w_{d,n}\)</span>, in the form of a vector of length <span class="math inline">\(K\)</span>. Then we can uniformly draw a topic from this vector. As we have said, we iterate this sampling process again and again until the model is good enough.</p>
</section>
<section class="level3" id="topic-modelling-example">
<h3>Topic Modelling Example</h3>
<p>We have implemented the <code>Owl_nlp.Lda</code> module to perform LDA method. Let’s first use an example to demonstrate how LDA works.</p>
<div class="highlight">
<pre><code class="language-ocaml">let build_lda corpus topics =
  let model = Nlp.Lda.init ~iter:1000 topics corpus in
  Nlp.Lda.(train SimpleLDA model);
  model</code></pre>
</div>
<p>The input to LDA is still the text corpus we have built. We also need to specify how many topics we want the text corpus to be divided into. Let’s say we set the number of topics to 8. The process is simple, we first initialise the model using the <code>init</code> function and then we can train the model. Let’s take a look at the document-topic table in this model, as shown below.</p>
<div class="highlight">
<pre><code class="language-text">val dk : Arr.arr =
    C0  C1  C2  C3  C4  C5  C6  C7
R0  13  13   4   7  11  12  14  16
R1  35 196  15  42  31  23 122   4
R2   7   9   3   1   3 163   2   4
R3  10  22  23 140  18  11  17 143
...</code></pre>
</div>
<p>This matrix shows the distribution of topics in each document, represented by a row. Each column represents a topic. For example, you can see that the fifth column of in the third document (R2) is obviously larger than the others. It means that dominantly talks about only the topic 6. Similarly, in the fourth document, the topic 4 and topic 8 are of equal coverage.</p>
<p>We can then check the topic-word table in this model:</p>
<div class="highlight">
<pre><code class="language-text">val wk : Arr.arr =
    C0  C1  C2  C3  C4  C5  C6  C7
R0   1   0   0   0   0   0   0   0
R1   0   0   0   1   0   0   0   0
R2   0   0   0   0   3   0   0   0
R3   0   0   0   0   0   0   0   3
...</code></pre>
</div>
<p>This is sparse matrix. Each row represents a word from the vocabulary. A topic in a column can thus be represented as the words that have the largest numbers in that column. For example, we can set that a topic be represented by 10 words. The translation from the word-topic table to text representation is straightforward:</p>
<div class="highlight">
<pre><code class="language-ocaml">let get_topics vocab wt =
  Mat.map_cols (fun col -&gt;
    Mat.top col 10
    |&gt; Array.map (fun ws -&gt;
      Owl_nlp.Vocabulary.index2word vocab ws.(0))
  ) wt</code></pre>
</div>
<p>As an example, we can take a look at the topics generated by the “A Million News Headlines” <a href="https://www.kaggle.com/therohk/million-headlines">dataset</a>. [<a href="https://www.kaggle.com/therohk/million-headlines">Link</a>].</p>
<div class="highlight">
<pre><code class="language-text">Topic 1:  police child calls day court says abuse dead change market
Topic 2:  council court coast murder gold government face says national police
Topic 3:  man charged police nsw sydney home road hit crash guilty
Topic 4:  says wa death sa abc australian report open sex final
Topic 5:  new qld election ban country future trial end industry hour
Topic 6:  interview australia world cup china south accused pm hill work
Topic 7:  police health govt hospital plan boost car minister school house
Topic 8:  new water killed high attack public farmers funding police urged</code></pre>
</div>
<p>Here each topic is represented by ten of its highest ranked words in the vocabulary. but you can “feel” a common theme by connecting these dots together, even though some words may stride away a bit far away from this theme. We cannot directly observe the topic, only documents and words. Therefore the topics are latent. The word-topic matrix shows that. each word have different weight in the topic and the words in a topic is ranked according to the weight. Now that we know what each topic talks about, we can cluster the documents by their most prominent topic, or just discover what topics are covered in a document, with about how much percentage each.</p>
<p>We have introduced the basic mechanism of LDA. There are many work that extend based on it, such as the SparseLDA in <span data-cites="yao2009efficient" class="citation">(Yao, Mimno, and McCallum 2009)</span>, and LightLDA in <span data-cites="yuan2015lightlda" class="citation">(Yuan et al. 2015)</span>. They may differ in details but share similar basic theory.</p>
</section>
</section>
<section class="level2" id="latent-semantic-analysis-lsa">
<h2>Latent Semantic Analysis (LSA)</h2>
<p>Besides LDA, another common technique in performing topic modelling is the Latent Semantic Analysis (LSA). Its purpose is the same as LDA, which is to get two matrices: the document-topic table, and the word-topic table to show the probability distribution of topics in documents and words in topics. The difference is that, instead of using an iterative update approach, LSA explicitly builds the <em>document-word matrix</em> and then performs the singular value decomposition (SVD) on it to get the two aforementioned matrices.</p>
<p>Assume the text corpus contains <span class="math inline">\(n\)</span> documents, and the vocabulary contains <span class="math inline">\(m\)</span> words, then the document-word matrix is of size <span class="math inline">\(n\times~m\)</span>. We can use the simple word count as the element in this matrix. But as we have discussed in previous section, the count of words does not reflect the significance of a word, so a better way to fill in the document-word matrix is to use the TF-IDF approach for each word in a document.</p>
<p>Apparently, this matrix would be quite sparse. Also its row vectors are in a very high dimension. There are surely redundant information here. For example, if two documents talk about the same topic(s), then the words they contain will largely overlap. To this end, the SVD is then used to reduce the dimension and redundancy in this matrix.</p>
<p>We have seen the SVD in the linear algebra chapter. It is widely used for reducing the dimension of information, by rotating and scaling the coordinating system to find suitable dimensions. SVD decomposes a matrix <span class="math inline">\(A\)</span> into <span class="math inline">\(A = USV^T\)</span>. In this specific context of semantic analysis, <span class="math inline">\(A\)</span> is the document-word composition. We can think of the <span class="math inline">\(U\)</span> as representing the relationship between document and the topics, and <span class="math inline">\(V^T\)</span> as the relationship between topics and words.</p>
<p>The columns of <span class="math inline">\(U\)</span> and rows of <span class="math inline">\(V^T\)</span> are both orthonormal bases, and the diagonal matrix <span class="math inline">\(S\)</span> has eigenvalues along its diagonal, each representing the weight of a group of corresponding bases from <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>. Therefore, we can throw away the bases with less weight, truncating only <span class="math inline">\(K\)</span> columns (rows) from each matrix. In that way, we can preserve a large part of the information from the original document-word table by choosing only a small number of topics. This process is shown in fig.&nbsp;3. Once we have the document-topic table <span class="math inline">\(U\)</span> and the topic-word table <span class="math inline">\(V\)</span>, using the model will be the same as in LDA example.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:nlp:svd" title="svd" src="images/nlp/svd.png"><figcaption>Figure 3: Applying SVD and then truncation on document-word matrix to retrieve topic model</figcaption>
</figure>
<p>Compare to LDA, this process is easy to understand and implement. However, SVD is computationally intensive and hard to iterate with new data. The result is decent, but as <a href="https://www.kaggle.com/rcushen/topic-modelling-with-lsa-and-lda">this blog</a> shows, it may not be as good as LDA in separating out the topic categories.</p>
<p>Application of topic modelling is wide. For example, it can be used for summarising the large corpus of text data, text categorisation, spam filter, the recommender system, or automatic tagging of articles, etc. It can even be used to effectively discover useful structure in large collection of biological data.</p>
</section>
<section class="level2" id="search-relevant-documents">
<h2>Search Relevant Documents</h2>
<p>Topic models are effective tools for clustering documents based on their similarity or relevance. We can further use this tool to query relevant document given an input one. In this section, we will go through some techniques on how to query models built using the previous topic modelling method.</p>
<section class="level3" id="euclidean-and-cosine-similarity">
<h3>Euclidean and Cosine Similarity</h3>
<p>Define what is euclidean and cosine similarity. Emphasise both are correlated on a high-dimensional ball model.</p>
<p>In the previous sections, we see that the topic modelling techniques maps documents to a vector space of topics. We can use different metrics to compare the similarity between two vectors. Two of the commonly used are the <em>Euclidean</em> and <em>Cosine</em> distances. Suppose we have two vectors <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, both of length of <span class="math inline">\(n\)</span>. Then the Euclidean distance between these two are:</p>
<p><span id="eq:nlp:euclidean"><span class="math display">\[\sqrt{\sum_{i=1}^n~(a_i - b_i)^2}.\qquad(2)\]</span></span></p>
<p>and the cosine similarity between two vectors <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is defined as:</p>
<p><span id="eq:nlp:cosine"><span class="math display">\[cos(\theta) = \frac{A.B}{\|A\|~\|B\|}.\qquad(3)\]</span></span></p>
<p>It is the dot product of two vectors divided by the product of the length of both vectors.</p>
<figure>
<img alt="" style="width:60.0%" id="fig:nlp:similarity" title="similarity" src="images/nlp/similarity.png"><figcaption>Figure 4: Euclidean distance and cosine similarity ne a two dimensional space</figcaption>
</figure>
<p>We have implemented both methods in the <code>Nlp.Similarity</code> module as similarity metrics for use in NLP. The relationship between the Euclidean distance and Cosine similarity can be expressed in fig.&nbsp;4. There are two points on this two dimensional space. The Euclidean measures the direct distance of these two points, while the cosine similarity is about the degree between these two vectors. Therefore, the cosine similarity is more suitable for cases where the magnitude of the vectors does not matter. For example, in topic modelling, we already have two vectors representing documents. If we multiply all the elements in one of them by a scalar 10, the Euclidean distance between these two would change greatly. However, since the probability distribution in the vector does not change, we don’t expect the similarity between these two vectors to change. That’s why in this case we would prefer to use the cosine similarity as a measurement,</p>
</section>
<section class="level3" id="linear-searching">
<h3>Linear Searching</h3>
<p>First implement linear search, in this case, we do not need index at all, but it is very slow.</p>
<p>Suppose we have <span class="math inline">\(n\)</span> document, each represented by a vector of length <span class="math inline">\(m\)</span>. They are then denoted with variable <code>corpus</code>, an array of arrays, each of which is a document. We provide a vector <code>doc</code> as the query to search for the top-<span class="math inline">\(k\)</span> similar documents to it. First, we need a function to calculate pairwise distance for the whole model, and returns result in the form of array of <code>(id, dist)</code>. Here <code>id</code> is the original index of the document. <code>dist</code> is the distance between a document in corpus and the query document.</p>
<div class="highlight">
<pre><code class="language-ocaml">let all_pairwise_distance typ corpus x =
  let dist_fun = Owl_nlp_similarity.distance typ in
  let l = Array.mapi (fun i y -&gt; i, dist_fun x y) corpus in
  Array.sort (fun a b -&gt; Stdlib.compare (snd a) (snd b)) l;
  l</code></pre>
</div>
<p>The result are sorted according to the distance, whichever distance metric we use. Based on this routine we can find the <span class="math inline">\(k\)</span> most relevant document:</p>
<div class="highlight">
<pre><code class="language-ocaml">let query corpus doc k =
  let typ = Owl_nlp_similarity.Cosine in
  let l = all_pairwise_distance typ corpus doc in
  Array.sub l 0 k</code></pre>
</div>
<p>Here we use the cosine similarity as measurement of distance between vectors. To improve the efficiency of computation, we can instead using matrix multiplication to implement the cosine similarity. Specifically, suppose we have the query document vector <span class="math inline">\(A\)</span>, and the corpus of document vector as before, and this array of arrays has already been converted to a dense matrix <span class="math inline">\(B\)</span>, where each row vector represents a document. Then we can compute the <span class="math inline">\(AB^T\)</span> to directly compute the cosine similarity. Of course, according to eq.&nbsp;3, we also need to make sure that <span class="math inline">\(A\)</span> and each row <span class="math inline">\(r\)</span> in <span class="math inline">\(B\)</span> is normalised by its own l2-norm before computations, so that for any vector <span class="math inline">\(v\)</span> we can have <span class="math inline">\(\|v\| = 1\)</span>.</p>
<div class="highlight">
<pre><code class="language-ocaml">let query corpus doc k =
  let vec = Mat.transpose doc in
  let l = Mat.(corpus *@ vec) in
  Mat.bottom l k</code></pre>
</div>
<p>Compared to the previous direct element-by-element multiplication, the matrix dot multiplication is often implemented with highly optimised linear algebra library routines, such as in OpenBLAS. These methods utilise various techniques such as multi-processing and multi-threading so that the performance is much better than a direct pairwise computation according to definition.</p>
</section>
</section>
<section class="level2" id="summary">
<h2>Summary</h2>
<p>In this chapter, we focus on topic modelling, one important natural language processing task, and introduce the basic idea and how Owl support it. First, we introduce how to tokenise text corpus for further mathematical processing. Then we introduce the basic idea of the vector space, and two different ways: the Bag of words (BOW), and Term Frequency–Inverse Document Frequency (TF-IDF), to project a document into a vector space as as single vector. The BOW is straightforward to understand and implement, and the TF-IDF consider the how special a word is across the whole text corpus, and therefore usually gives more accurate representation.</p>
<p>Next, we present two different methods based on the vector representation to retrieve topics from the documents: the Latent Dirichlet Allocation (LDA), and Latent Semantic Analysis (LSA). The LSA relies on the singular value decomposition technique on a document-word matrix to do that, while LDA relies on a generative model to iteratively to get the topic model. Once we have the topic modelling, we can compare the similarity between documents, or search for similar documents in the text corpus using different measurement of vector distances. The cosine similarity is a common one in text analysis. The computation of search process can be optimised using matrix multiplication.</p>
</section>
<section class="level2 unnumbered" id="references">
<h2 class="unnumbered">References</h2>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-blei2003latent">
<p>Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent Dirichlet Allocation.” <em>Journal of Machine Learning Research</em> 3 (Jan): 993–1022.</p>
</div>
<div id="ref-mikolov2013exploiting">
<p>Mikolov, Tomas, Quoc V Le, and Ilya Sutskever. 2013. “Exploiting Similarities Among Languages for Machine Translation.” <em>arXiv Preprint arXiv:1309.4168</em>.</p>
</div>
<div id="ref-yao2009efficient">
<p>Yao, Limin, David Mimno, and Andrew McCallum. 2009. “Efficient Methods for Topic Model Inference on Streaming Document Collections.” In <em>Proceedings of the 15th Acm Sigkdd International Conference on Knowledge Discovery and Data Mining</em>, 937–46.</p>
</div>
<div id="ref-yuan2015lightlda">
<p>Yuan, Jinhui, Fei Gao, Qirong Ho, Wei Dai, Jinliang Wei, Xun Zheng, Eric Po Xing, Tie-Yan Liu, and Wei-Ying Ma. 2015. “Lightlda: Big Topic Models on Modest Computer Clusters.” In <em>Proceedings of the 24th International Conference on World Wide Web</em>, 1351–61.</p>
</div>
</div>
</section>
</section>
</article></div><a href="dataframe.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 16</small>Dataframe for Tabular Data</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>