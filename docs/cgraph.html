<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><meta content="OCaml Scientific and Engineering Computing - Tutorial Book" name="description"><meta content="OCaml, Data Science, Data Analytics, Analytics, Functional Programming, Machine Learning, Deep Neural Network, Scientific Computing, Numerical Algorithm, Tutorial, Linear Algebra, Matrix" name="keywords"><meta content="Liang Wang" name="author"><title>Computation Graph - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script><script data-ad-client="ca-pub-1868946892712371" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-123353217-1"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-123353217-1');</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="computation-graph">
<h1>Computation Graph</h1>
<p>This chapter first gives a bird’s-eye-view on the computation graph in Owl. Then we will continue to cover the design and implementation details of the computation graph and how it is fitted into Owl’s functor stack, and its implications on the architecture of numerical systems.</p>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<section class="level3" id="what-is-a-computation-graph">
<h3>What is a Computation Graph?</h3>
<p>As a functional programmer, it is basic knowledge that a function takes an input then produces an output. The input of a function can be the output of another function which then creates dependency. If we view a function as one node in a graph, and its input and output as incoming and outgoing links respectively, as the computation continues, these functions are chained together to form a directed acyclic graph (DAG). Such a DAG is often referred to as a computation graph.</p>
<figure>
<img style="width:30.0%" id="fig:cgraph:plot_01" alt="Computation graph of a simple function: sin(x*y)" title="plot_cgraph_01.png" src="images/cgraph/plot_cgraph_01.png"><figcaption>Computation graph of a simple function: sin(x*y)</figcaption>
</figure>
<p>The <span data-cites="fig:cgraph:plot_01" class="citation">[@fig:cgraph:plot_01]</span> shows an example graph for calculating function <code>sin (x * y)</code>. The generated computation graph contains several pieces of information which are essential for debugging the applications. These information include node index, operation type, reference counter, and shapes of data. In the figure above, we can see the row vector <code>y</code> of shape [1; 4] is broadcast on the matrix <code>x</code> of shape [8; 4] in <code>Mul</code> operation.</p>
</section>
<section class="level3" id="from-dynamic-to-static">
<h3>From Dynamic to Static</h3>
<p>The computation graph can be either implicitly constructed or explicitly declared in the code. Often, implicit construction is done by operator overloading while explicit declaration uses domain specific languages (DSL). The two methods lead to two different kinds of computation graphs – <em>dynamic graph</em> and <em>static graph</em>, each has its own pros and cons.</p>
<p>A dynamic graph is constructed during the runtime. Due to operator overloading, its construction can be naturally blended with a language’s native constructs such as <code>if ... else ...</code> and <code>for</code> loops. This renders greatest flexibility and expressiveness. On the other hand, a static graph needs to be declared using a specific DSL (which has a steeper learning curve). Because the structure of a graph is already known during the compilation phase, there is a great space for optimisation. However, it is sometimes very difficult to use static graphs to express conditions and loops when using with native code together.</p>
<p>As we can see, the flexibility of a dynamic graph comes at the price of lower performance. Facebook’s PyTorch and Google’s TensorFlow are the typical examples of dynamic and static graph respectively. Many programmers need to make a choice between these two different types. A common practice is “using PyTorch at home and using TensorFlow in the company”, In other words, PyTorch is preferred for prototyping and TensorFlow is ideal for production use. (The TensorFlow 2.0 uses eager execution by default, which is easier for users to get start with.)</p>
<p>Owl does something slightly different from these two in order to get the best parts of both worlds. Owl achieves this by converting a dynamic graph into static one in the runtime. The motivation is based on a very important observation: in many cases, a computation graph is continuously re-evaluated after its construction. This is especially true for those iterative optimisation algorithms. We only update some inputs of the graph in each iteration.</p>
<p>If we know that the graph structure remains the same in every iteration, rather than re-constructing it all the time, we can convert it into a static graph before the iterative evaluation. This is exactly what Owl does. By so doing, the programmer can enjoy the flexibility offered by the dynamic graph construction with operator overloading, but still achieve the best performance from static graph.</p>
<p>Comparing to TensorFlow, the time overhead (for graph conversion and optimisation) is shifted to the runtime in Owl. You may worry about the performance: “Is it going to slow down my fancy DNN application?” The fact is, even for large and complex graphs, this Just-in-Time compilation (JIT) and optimisation are often quite fast. In this <a href="https://github.com/owlbarn/owl/blob/master/examples/mnist_cnn.ml">lazy_lstm.ml</a> example, there are 15,105 nodes and 21,335 edges. Owl is able to compile the graph within 230ms then optimise it within 210ms. The optimised graph contains only 8,224 nodes, 14,444 edges and runs much faster. Remember that you only need to do it once before training. For smaller networks, it often just takes several milliseconds.</p>
<p>Technically, JIT is very straightforward to implement in Owl’s architecture. Given a deep neural network, Owl first runs both forward pass and backward pass. Because of the computation graph, the calculation becomes symbolic and we can obtain the complete computation graph to calculate the loss and gradients of a neural network. We can then pass this static graph to the optimisation engine to optimise. The <a href="https://github.com/owlbarn/owl/blob/master/src/base/neural/owl_neural_compiler.ml">Neural Compiler</a> functor is parameterised by a computation engine then compiles a DNN definition and training configuration into a device-dependent static graph.</p>
</section>
<section class="level3" id="significance-in-computing">
<h3>Significance in Computing</h3>
<p>Now that you know the basic ideas of computation graph, you may ask why it matters? Well, the computation graph makes many things a lot easier. Here is an incomplete list of potential benefits:</p>
<ul>
<li>simulate lazy evaluation in a language with eager evaluation;</li>
<li>incremental computation (a.k.a Self-Adjusted Computation);</li>
<li>reduce computation complexity by optimising the structure of a graph;</li>
<li>reduce memory management overhead by pre-allocating the space;</li>
<li>reduce memory footprint by reusing allocated memory space;</li>
<li>natural support for parallel and distributed computing;</li>
<li>natural support for heterogeneous computing;</li>
<li>natural support for symbolic maths.</li>
</ul>
<p>Some of the benefits are very obvious. Memory usage can certainly be optimised if the graph structure is fixed and the input shapes are known. One optimisation is reusing previously allocated memory, which is especially useful for those applications involving large ndarray calculations. In fact, this optimisation can also be performed by a compiler by tracking the reference number of allocated memory, a technique referred to as linear types.</p>
<p>Some may appear less obvious at the first glance. For example, we can decompose a computation graph into multiple independent subgraphs and each can be evaluated in parallel on different cores or even computers. Maintaining the graph structure also improves fault-tolerance, by providing natural support for rollback mechanisms.</p>
<p>The computation graph provides a way to abstract the flow of computations, therefore it is able to bridge the high-level applications and low-level machinery of various hardware devices. This is why we say it has natural support for heterogeneous computing.</p>
<p>The computation graph has more profound implications. Because the memory allocated for each node is mutable, Algodiff becomes more scalable when evaluating large and complex graphs. At the same time, mutable transformation is handled by Owl so programmers can still write safe functional code.</p>
</section>
</section>
<section class="level2" id="examples">
<h2>Examples</h2>
<p>Before diving into the details of the design of the computation graph module, let’s first shows some examples of using the CGraph modules and how the computation can be transformed into lazy evaluation.</p>
<section class="level3" id="example-01-basic-cgraph">
<h3>Example 01: Basic CGraph</h3>
<p>Let’s start with a simple operation that adds up one ndarray and one scalar. Normally with Ndarray module what we do is:</p>
<div class="highlight">
<pre><code class="language-ocaml">module N = Dense.Ndarray.D
let x = N.ones [|2;2|]
let y = 2.
let g = N.add_scalar x y</code></pre>
</div>
<p>Now, let’s make it into a lazy evaluation calculation with CGraph:</p>
<div class="highlight">
<pre><code class="language-ocaml">module N = Owl_computation_cpu_engine.Make (Owl_algodiff_primal_ops.D)</code></pre>
</div>
<p>The computation graph is designed as a functor stack. A CGraph module can be built based on a ndarray module, since in the end a lazy evaluation still requires specific computation at some point.</p>
<div class="highlight">
<pre><code class="language-ocaml">let x = N.var_arr ~shape:[|2;2|] "x"
let y = N.var_elt "y"
let g = N.add_scalar x y</code></pre>
</div>
<p>Next we define two variables, the first <code>x</code> is a ndarray, and <code>y</code> is a scalar. At this stage, we only define these two as placeholders with no real data. Then we use the <code>add_scalar</code> function to get another lazy evaluated array <code>g</code>.</p>
<p>To get the value of the lazy expression <code>g</code>, we need to first assign real values to <code>x</code> and <code>y</code>:</p>
<div class="highlight">
<pre><code class="language-ocaml">let x_val = Dense.Ndarray.D.ones [|2;2|]
let y_val = 2.
let _ = N.assign_arr x x_val
let _ = N.assign_elt y y_val</code></pre>
</div>
<p>The real values are the familiar dense ndarray and float number. Note the two different assignment method for ndarray and scalar. Finally, we can evaluate the ndarray <code>g</code>:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">N.eval_arr [|g|];;
&gt;- : unit = ()
N.unpack_arr g;;
&gt;- : Algodiff.D.A.arr =
&gt;   C0 C1
&gt;R0  3  3
&gt;R1  3  3
</code></pre>
</div>
<p>The <code>eval_arr</code> returns nothing. To get the value, we need to use the <code>unpack_arr</code> or <code>unpack_elt</code> function.</p>
</section>
<section class="level3" id="example-02-cgraph-with-ad">
<h3>Example 02: CGraph with AD</h3>
<p>In real applications, we normally need to deal with CGraphs that are constructed in the Algorithmic Differentiation process. Here is an example of using the dense ndarray module to compute the gradients of a function:</p>
<div class="highlight">
<pre><code class="language-ocaml">include Owl_algodiff_generic.Make (Owl_algodiff_primal_ops.D)

let f x y = Maths.((x * sin (x + x) + ((pack_flt 1.) * sqrt x) / (pack_flt 7.)) * (relu y) |&gt; sum')

let x = Dense.Ndarray.D.ones [|2;2|] |&gt; pack_arr
let y = pack_elt 2.
let z = (grad (f x)) y |&gt; unpack_elt</code></pre>
</div>
<p>Obviously, it is extremely difficult for the users to manually construct the computation graph that computes the gradient of the function <code>f</code>. Instead, we use the computation graph as the base module to build the Algorithmic Differentiation module:</p>
<div class="highlight">
<pre><code class="language-ocaml">module G = Owl_computation_cpu_engine.Make (Owl_algodiff_primal_ops.D)
include Owl_algodiff_generic.Make (G)

let f x y = Maths.((x * sin (x + x) + ((pack_flt 1.) * sqrt x) / (pack_flt 7.)) * (relu y) |&gt; sum')

let x = G.var_arr ~shape:[|2;2|] "x" |&gt; pack_arr
let y = G.var_elt "y" |&gt; pack_elt
let z = (grad (f x)) y</code></pre>
</div>
<p>Note how the CGraph module are treated as equal to the Ndarray module in building the AD module. They decide if the AD module uses normal or lazy evaluation. Now we can evaluate <code>z</code> with the approach as before. Or we can use another approach: building a graph based on the input and output.</p>
<div class="highlight">
<pre><code class="language-ocaml">let inputs  = [| unpack_arr x |&gt; G.arr_to_node; unpack_elt y |&gt; G.elt_to_node |]
let outputs = [| unpack_elt z |&gt; G.elt_to_node |]
let g = G.make_graph inputs outputs "graph"</code></pre>
</div>
<p>To build a graph, we need to specify the input and output <em>nodes</em>. Here it might be a bit confusing, since there are two layers of packing and unpacking. Currently the <code>x</code>, <code>y</code>, and <code>z</code> are both AD values of type <code>AD.t</code>, therefore we need <code>AD.unpack_arr</code> and <code>AD.unpack_elt</code> to make them CGraph lazy array and scalar values. And then, to build the explicit computation graph, we need to use the <code>G.arr_to_node</code> and <code>G.elt_to_node</code> functions to make them into graph nodes first. Finally an explicit computation graph can be built with <code>make_graph</code> function.</p>
<p>You might be wondering why bother to build the graph if we can directly evaluate the value <code>z</code>. The reason is that evaluation is not always the target. For example, we often need to visualise the generated computation graph. Backward mode generates and maintains a computation graph in order to back propagate the error. The computation graph is very helpful in both debugging and understanding the characteristic of your numerical functions.</p>
<p>Owl provides the <code>graph_to_dot</code> function to facilitate you in generating computation graphs. It converts the computation graph into a <a href="https://www.graphviz.org/doc/info/lang.html">dot</a> format string. The dot file can be visualised with professional tools such as <a href="https://www.graphviz.org/">graphviz</a>.</p>
<div class="highlight">
<pre><code class="language-text">let s = G.graph_to_dot g
let _ = Owl_io.write_file "cgraph.dot" s</code></pre>
</div>
<p>The generated computation graph looks like below. The Owl source code contains more examples about visualising a computation graph.</p>
<figure>
<img style="width:60.0%" id="fig:algodiff:plot28" alt="Computation graph of a simple math function" title="plot 028" src="images/algodiff/plot_028.png"><figcaption>Computation graph of a simple math function</figcaption>
</figure>
<p>Come back to the evaluation of graph. After constructing the graph <code>g</code>, we can then assign real data values to the computation graph. The only difference is that, now we need to first unpack the AD value to CGraph value before assignment:</p>
<div class="highlight">
<pre><code class="language-text">let x_val = Dense.Ndarray.D.ones [|2;2|]
let y_val = 2.
let _ = G.assign_arr (unpack_arr x) x_val
let _ = G.assign_elt (unpack_elt y) y_val</code></pre>
</div>
<p>Finally, we can evaluate the whole graph with</p>
<div class="highlight">
<pre><code class="language-text">G.eval_graph g</code></pre>
</div>
<p>Since the whole graph is evaluated, then surely the output ndarray <code>z</code> is also evaluated. We can first unpack it from AD value into normal CGraph ndarray and then get its value by:</p>
<div class="highlight">
<pre><code class="language-text"># unpack_elt z |&gt; G.unpack_elt

- : float = 4.20861827873129801</code></pre>
</div>
</section>
<section class="level3" id="example-03-cgraph-with-dnn">
<h3>Example 03: CGraph with DNN</h3>
<p>Since the optimisation and neural network modules are built on Algorithmic Differentiation module, they can also benefit from the power of CGraph. Suppose we have a network built of CGraph based neural network <code>nn</code>, we can then use the <code>forward</code> and <code>backward</code> function to get the forward inference and backward propagation computation graph from the neural network graph module, with CGraph array variable.</p>
<p>Actually, for ease of access, Owl has provided another functor to build the neural network module based on the CGraph module:</p>
<div class="highlight">
<pre><code class="language-ocaml">module CPU_Engine = Owl_computation_cpu_engine.Make (Owl_algodiff_primal_ops.S)
module CGCompiler = Owl_neural_compiler.Make (CPU_Engine)

open CGCompiler.Neural
open CGCompiler.Neural.Graph
open CGCompiler.Neural.Algodiff

let make_network input_shape =
  input input_shape
  |&gt; lambda (fun x -&gt; Maths.(x / pack_flt 256.))
  |&gt; conv2d [|5;5;1;32|] [|1;1|] ~act_typ:Activation.Relu
  |&gt; max_pool2d [|2;2|] [|2;2|]
  |&gt; dropout 0.1
  |&gt; fully_connected 1024 ~act_typ:Activation.Relu
  |&gt; linear 10 ~act_typ:Activation.(Softmax 1)
  |&gt; get_network ~name:"mnist"</code></pre>
</div>
<p>The CGraph-built neural network module does not require any change of code in building the CNN except for the headers. We can then use the training function in <code>CGCompiler</code> module.</p>
<div class="highlight">
<pre><code class="language-ocaml">let pack x = CGCompiler.Engine.pack_arr x |&gt; Algodiff.pack_arr

let train network =
  let x, _, y = Dataset.load_mnist_train_data_arr () in
  let x = pack x in
  let y = pack y in
  CGCompiler.train network x y</code></pre>
</div>
<p>And similarly the inference can be done with <code>CGCompiler.model</code> function. You can see that to make the existing DNN programme into lazy evaluation version, all you need to do is to update the header and use packing/unpacking properly for the data.</p>
<p>You might be asking: the lazy evaluation version of neural network looks cool and all, but why do I need it? That brings to the large performance improvement the CGraph module can bring about to computation. To motivate you to continue to understand more about the design and optimisation of the CGraph module, you can try to run both <a href="https://github.com/owlbarn/owl/blob/master/examples/mnist_cnn.ml">mnist_cnn.ml</a> and <a href="https://github.com/owlbarn/owl/blob/master/examples/lazy_mnist.ml">lazy_mnist.ml</a> then compare their performance. Both Zoo scripts train the same convolution neural network to recognise the handwritten digits using MNIST datasets in 60 iterations. On a normal laptop, <code>mnist_cnn.ml</code> takes 30s to finish and consumes approximate 4GB memory, whilst <code>lazy_mnist.ml</code> only takes 5s and consumes about 0.75GB. <code>lazy_mnist.ml</code> achieves the state-of-the-art performance which you can obtain by using TensorFlow (with its recent XLA optimisation), actually Owl runs even faster on 3 out of 4 machines we have tested.</p>
<p>If these numbers make you interested in knowing how the magic happens, let’s unveil the underlying mechanism of Owl’s computation graph in the following sections.</p>
</section>
</section>
<section class="level2" id="design-rationale">
<h2>Design Rationale</h2>
<p>How the computation graph is designed? In the older versions, Algodiff module has some partial support of computation graph in order to perform reverse mode algorithmic differentiation (AD). The full support was only introduced in Owl 0.4.0.</p>
<p>Owl implements the computation graph in a very unique and interesting way. Let’s first see several principles which we followed:</p>
<ul>
<li>Non-intrusive, the original functor stack should work as it was;</li>
<li>Transparent to the programmers as much as possible;</li>
<li>Support both eager and lazy evaluation;</li>
<li>Flexible enough for future extension on other devices.</li>
</ul>
<p>The computation graph is implemented in a very self-contained stack. I have devised a good way to “inject” it into Owl’s original functor stack. If it sounds too abstract, please have a look at the final product in the following figure.</p>
<figure>
<img style="width:90.0%" id="fig:cgraph:functor" alt="Computation graph functor stack in Owl" title="owl_cgraph_functor_stack" src="images/cgraph/owl_cgraph_functor_stack.png"><figcaption>Computation graph functor stack in Owl</figcaption>
</figure>
<p>The left figure shows part of Owl’s original functor stack, and the right one shows how the current one looks like after injection. We know the functor stack plays a central role in Owl’s architecture. In the old design, Ndarray implements a set of fundamental n-dimensional array operations, then Algodiff defines abstract mathematical operations for differentiation, finally Optimise engine glues low-level maths with high-level deep neural network applications. The whole stack is parameterised by the number type abstraction in Ndarray.</p>
<ul>
<li><code>Ndarray</code>: provides number type abstraction and implements the fundamental numerical operations.</li>
<li><code>Algodiff</code>: implements algorithmic differentiation.</li>
<li><code>Optimise</code>: uses the derivative information to build an optimisation engine.</li>
<li><code>Neural_Neuron</code>: implements various kinds of neuron functions which can be optimised.</li>
<li><code>Neural_Graph</code>: connects neurons together to form a network so that we can train a useful model.</li>
</ul>
<p>The functor stack of computation graph is injected between <code>Ndarray</code> and <code>Algodiff</code>. <em>The design principle is that the functor stack of a numerical system should be parameterised by both number type and device type.</em> Number type provides data representation (real or complex, single or double, row-based or column-based layout, etc.) which decides how a maths construct should be built and operated. Device type provides hardware representation (CPU, GPU, FPGA, etc.) which decides how the computation should be performed on a specific device.</p>
<p>The list below summarises the functionality of each functor. The order and naming of these functors can give you a rough understanding about how it is designed.</p>
<ul>
<li><code>Device</code>: device abstraction contains device-dependent types and functions.</li>
<li><code>Type</code>: type definition of various (mathematical) operations.</li>
<li><code>Shape</code>: provides the shape inference function in the graph.</li>
<li><code>Symbol</code>: provides various functions to manipulate symbols.</li>
<li><code>Operator</code>: implements maths operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, etc.) which decide how the symbols should be connected to form a graph.</li>
<li><code>Optimiser</code>: optimises the structure of a given graph by searching and optimising various patterns.</li>
<li><code>Graph</code>: manipulates computation graphs at high level, e.g.&nbsp;visualisation, connecting inputs and outputs.</li>
<li><code>Engine</code>: evaluates a computation graph on a specific device.</li>
</ul>
<p>Why the magic can happen? Simply put, the injected computation graph stack provides an abstraction layer similar to symbolic maths. The original eager evaluation becomes symbolic operation (or graph construction) therefore they can be lazily evaluated.</p>
<p>The shape inference functionality is able to infer the data shape of every node in a graph from its input. This allows Owl to calculate how much memory is required to evaluate the graph and pre-allocate this space. Owl can further track the reference number of each function node and reuse the allocated memory as much as possible, which reduces both memory footprint and Garbage Collector (GC) overhead, significantly improves the computation speed.</p>
<p>The Optimiser functor searches for various structural patterns in a graph, removes unnecessary computations and fusing computation nodes if possible. All the patterns are defined in <a href="https://github.com/owlbarn/owl/blob/master/src/base/compute/owl_computation_optimiser.ml">owl_computation_optimiser.ml</a>, and it is very straightforward to plug in more patterns to extend Optimiser’s capability. Here are some example patterns.</p>
<p><em>Constant folding</em> is a very basic pattern to reduce graph size. We can pre-calculate some subgraphs. For example, the inputs which node <code>#241</code> depends on are all constants, so the value of <code>#241</code> is already decided. We can fold all the constants to node <code>#241</code> before evaluating the whole graph.</p>
<figure>
<img style="width:90.0%" id="fig:cgraph:opt_0" alt="Optimisation techniques in computation graph: constant folding" title="owl_cgraph_opt_0" src="images/cgraph/owl_cgraph_opt_0.png"><figcaption>Optimisation techniques in computation graph: constant folding</figcaption>
</figure>
<p><em>Fusing operations</em> can effectively reduce the round trips to the memory, which saves a lot of time when operating large ndarrys. In the figure below, nodes <code>#421</code>, <code>#463</code>, and <code>#464</code> are fused into one <code>fma</code> node (i.e.&nbsp;fused-multiply-add operation), which also improves numerical accuracy. Owl also recognises quite complicated patterns, e.g.&nbsp;pattern formed by nodes <code>#511</code> – <code>#515</code> appears a lot in DNN training that uses Adagrad (Adaptive Subgradient Methods), the Optimiser is able to fuse all these operations into one-pass calculation.</p>
<figure>
<img style="width:90.0%" id="fig:cgraph:opt_1" alt="Optimisation techniques in computation graph: fusing operations" title="owl_cgraph_opt_1" src="images/cgraph/owl_cgraph_opt_1.png"><figcaption>Optimisation techniques in computation graph: fusing operations</figcaption>
</figure>
<p>In the next example, the <em>Adding zero</em> pattern is firstly detected hence <code>#164</code> and <code>#166</code> are removed and others are folded. Moreover, nodes <code>#255</code> for <code>repeat</code> operation is also removed because <code>add</code> operation already supports broadcasting operation. Removing <code>#255</code> can save some runtime memory in the evaluation.</p>
<figure>
<img style="width:90.0%" id="fig:cgraph:opt_2" alt="Optimisation techniques in computation graph: remove zero" title="owl_cgraph_opt_2" src="images/cgraph/owl_cgraph_opt_2.png"><figcaption>Optimisation techniques in computation graph: remove zero</figcaption>
</figure>
<p>To understand how effective the Optimiser works, we present both the <a href="images/cgraph/owl_cgraph_mnist_raw.png">original computation graph</a> and the <a href="images/cgraph/owl_cgraph_mnist_opt.png">optimised graph</a> taken from <a href="https://github.com/owlbarn/owl/blob/master/examples/lazy_mnist.ml">lazy_mnist.ml</a>. Comparing to the original network which has 201 nodes, 239 edges, the optimised one contains only 103 nodes, 140 edges.</p>
<p>Engine functor sits on top of the stack. This is where a computation graph finally gets executed. Engine functor contains two sub modules, one for initialising the graph and the other for evaluating graph.</p>
<p>Before we finish this section, we can try the following snippet in <code>utop</code>. Both snippets generate a module for DNN applications, the difference is that the first one uses the old stack whereas the second one uses the new stack with computation graph.</p>
<div class="highlight">
<pre><code class="language-text">
  module M =
    Owl_neural_generic.Flatten (
      Owl_neural_graph.Make (
        Owl_neural_neuron.Make (
          Owl_optimise_generic.Make (
            Owl_algodiff_generic.Make (
              Dense.Ndarray.S)))));;
</code></pre>
</div>
<p>For the new stack, we can see it is indeed much deeper.</p>
<div class="highlight">
<pre><code class="language-text">
  module M =
    Owl_neural_generic.Flatten (
      Owl_neural_graph.Make (
        Owl_neural_neuron.Make (
          Owl_optimise_generic.Make (
            Owl_algodiff_generic.Make (
              Owl_computation_engine.Flatten (
                Owl_computation_cpu_engine.Make_Nested (
                  Owl_computation_graph.Make (
                    Owl_computation_optimiser.Make (
                      Owl_computation_operator.Make (
                        Owl_computation_symbol.Make (
                          Owl_computation_shape.Make (
                            Owl_computation_type.Make (
                              Owl_computation_cpu_device.Make (
                                Dense.Ndarray.S))))))))))))));;
</code></pre>
</div>
</section>
<section class="level2" id="optimisation-of-cgraph">
<h2>Optimisation of CGraph</h2>
<p>The design of Owl is often driven by real-world applications. Besides the MNIST example, we find the image segmentation another challenging application for Owl. Seeking to push the performance of this application, we manage to further optimise the design of CGraph module. This work is done by Pierre Vandenhove, and you can visit his <a href="http://math.umons.ac.be/staff/Vandenhove.Pierre/resources/ocamllabs_internship_report.pdf">report</a> for more details. It starts with the MRCNN-based Object Detection application we introduce in the <a href="https://ocaml.xyz/book/case-obj-detect.html">Case - Object Detection</a> chapter. Please refer to this chapter for detail explanation of this application.</p>
<p>The first issue after constructing the network in Owl was that the memory usage, in inference mode, was huge. The network has over 400 layers and to avoid reinitialising the network for every picture, it is good to keep its input size fixed and to resize instead all the images to that size — a larger size takes more time and memory but yields more accurate results. A reasonable input size for this network is a 1024-pixel-wide square. Unfortunately, obtaining detections for one picture with this size required over 11 GB of RAM, which was too much for a laptop. As a comparison, the TensorFlow implementation only uses 1 GB. There was a big room for improvement!</p>
<p>This is where CGraph comes to rescue. A computation graph is always directed and acyclic. Representing the structure of a program as a computation graph has several advantages, especially for computationally-intensive code dealing with big multi-dimensional arrays. A really useful one is that prior to evaluating the nodes, you can optimise the structure of the graph: for instance, useless calculations such as adding an array with nothing but zeros can be removed, common patterns can be merged into one node and executed more efficiently, etc. This helps a bit: thanks to these optimisations, the number of nodes of Mask R-CNN drops from 4095 to 3765. Another really important feature in this case is the ability to pre-allocate a memory space to each node, to decrease the overall memory consumption and reduce the garbage collector overhead.</p>
<section class="level3" id="optimising-memory-with-pebbles">
<h3>Optimising memory with pebbles</h3>
<p>To describe the problem of allocating memory in a computation graph, it is interesting to look at the <em>pebble game</em>, which was introduced <a href="http://perso.ens-lyon.fr/loris.marchal/scheduling/sethi_complete_register_allocation.pdf">in 1973</a> to explain register allocation.</p>
<p>The <em>pebble game</em> is played on a directed acyclic graph. Each node can store at most one pebble. The game begins with no pebble on any node. At each step, the player can do one of the following moves:</p>
<ol type="1">
<li>if a vertex <span class="math inline">\(v\)</span> has no predecessor, the player can place a pebble on <code>v</code>.</li>
<li>if all predecessors of a vertex <span class="math inline">\(v\)</span> are pebbled, the player can place a pebble on <code>v</code> or <code>slide</code> a pebble from one of its predecessors to <code>v</code>.</li>
<li>the player can remove any pebble from a vertex (and reuse that pebble later).</li>
</ol>
<p>The goal of the game is to place a pebble at least once on some fixed output vertices of the graph.</p>
<p>Here is an example of an optimal pebbling strategy using the previous computation graph (gray nodes are pebbled), using moves 1 -&gt; 2 -&gt; 3 -&gt; 1 -&gt; 2 -&gt; 2. We assume that the goal is to pebble node 5:</p>
<figure>
<img id="fig:cgraph:pebble" alt="Modelling computation graph memory optimisation problem as a pebble game" src="images/cgraph/owl_vision_pebble.png"><figcaption>Modelling computation graph memory optimisation problem as a pebble game</figcaption>
</figure>
<p>This relates to the memory allocation of the computation graph if we see pebbles as memory blocks used to store the output value of a node. We assume that the values of the inputs are known (move 1). We can only compute the value of a vertex if all its predecessors are simultaneously stored in memory (move 2). The <em>sliding</em> move means that the memory of a node can be overwritten by its successor during its computation (<em>inplace reuse</em>). We can always reuse a memory block from any other node (move 3). Given a graph, the idea is thus to find a strategy to pebble it using a minimum number of pebbles (in other words, using as little memory as possible).</p>
<p>We also want to avoid pebbling any node twice (in order the keep the execution time as low as possible, because that would mean that we compute the same node twice). Given these constraints, finding a strategy using the least amount of pebbles is unfortunately <a href="http://perso.ens-lyon.fr/loris.marchal/scheduling/sethi_complete_register_allocation.pdf">NP-complete</a>. Since computation graphs can have a few thousand nodes, we will be looking for a fast heuristic instead of an exact algorithm.</p>
</section>
<section class="level3" id="allocation-algorithm">
<h3>Allocation Algorithm</h3>
<p>The initially implemented strategy to allocate memory to a node <span class="math inline">\(u\)</span> in Owl’s computation graph module was simply to reuse the memory of a direct predecessor with same output shape as <span class="math inline">\(u\)</span> when that is possible. This optimisation decreases the memory consumption of Mask R-CNN from 11 GB to 7 GB — much better, but still quite far from the 1 GB of the TensorFlow implementation!</p>
<p>We can actually make it much better by sharing memory between nodes</p>
<ul>
<li>that are not necessarily a parent/child pair;</li>
<li>that do not have the same output size (by allocating a large block of memory once, without necessarily using all of it all the time).</li>
</ul>
<p>To do this efficiently, we first have to fix an evaluation order (in practice, any topological order). Given this order, we can pinpoint the moment when the memory of a node becomes useless by keeping a counter of how many times it has been used. When it has been used by all its children, we can recycle its memory. Then to allocate memory to a node, we simply check which blocks are available and we select the one with the closest size (in order not to waste too much memory). If no block is available, we allocate a new one. This can be executed in <span class="math inline">\(\mathcal{O}(n * \log(n))\)</span> time, which is negligible compared to the actual cost of evaluating the graph.</p>
<p>Then we just have to be careful that some operations cannot overwrite their inputs while they are being computed (the <em>sliding</em> move from the pebble game is forbidden) and that some nodes cannot be overwritten for practical purposes (typically constant nodes or neural network weights). Implementing this effectively reduced the memory consumption of Mask R-CNN from 7 GB to 1 GB for a 1024x1024 picture, making it as efficient as the TensorFlow implementation! A summary of the changes can be found in <a href="https://github.com/owlbarn/owl/pull/318">this pull request</a>. Here are some more statistics illustrating what the computation graph with this new algorithm achieves:</p>
<table style="width:98%;">
<caption>Evaluation of the effect of CGraph memory optimisation using different DNN architectures {#tbl:cgraph:perf}</caption>
<colgroup>
<col style="width: 18%">
<col style="width: 14%">
<col style="width: 29%">
<col style="width: 17%">
<col style="width: 17%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">Architecture</td>
<td style="text-align: left;">Time without CG (s)</td>
<td style="text-align: left;">Time with CG (building + evaluating) (s)</td>
<td style="text-align: left;">Memory without CG (MB)</td>
<td>Memory with CG (MB)</td>
</tr>
<tr class="even">
<td style="text-align: left;">InceptionV3</td>
<td style="text-align: left;">0.565</td>
<td style="text-align: left;">0.107 + 0.228 = 0.335</td>
<td style="text-align: left;">625.76</td>
<td>230.10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ResNet50</td>
<td style="text-align: left;">0.793</td>
<td style="text-align: left;">0.140 + 0.609 = 0.749</td>
<td style="text-align: left;">1309.9</td>
<td>397.07</td>
</tr>
<tr class="even">
<td style="text-align: left;">MNIST (training)</td>
<td style="text-align: left;">20.422</td>
<td style="text-align: left;">0.144 + 10.920 = 11.064</td>
<td style="text-align: left;">3685.3</td>
<td>895.32</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mask R-CNN</td>
<td style="text-align: left;">11.538</td>
<td style="text-align: left;">0.363 + 8.379 = 8.742</td>
<td style="text-align: left;">6483.4</td>
<td>870.48</td>
</tr>
</tbody>
</table>
<p>InceptionV3 and ResNet50 networks are tested with a 299x299 image; Mask R-CNN is tested with a 768x768 image. The MNIST line refers to a small neural network trained to recognize hand-written digits whose implementation can be found <a href="https://github.com/owlbarn/owl/blob/master/examples/lazy_mnist.ml">in this code repository</a>. The time is the average over 30 evaluations, without reusing pre-computed nodes when a computation graph is used. The graph building phase includes graph construction, optimisation and memory initialisation. The memory is the maximum resident set size of the program. This was evaluated on a laptop with an Intel i5-6300HQ and 8 GB of RAM.</p>
<p>For instance, when evaluated in the right order, the following computation graph, which can be used to recognise hand-written digits, needs only two different blocks of memory (each colour corresponds to a memory block, white nodes always need to be kept in memory). Part of the generated computation graph is shown in <span data-cites="fig:cgraph:lazy" class="citation">[@fig:cgraph:lazy]</span>.</p>
<figure>
<img style="width:50.0%" id="fig:cgraph:lazy" alt="Optimised memory allocation" title="allocation" src="images/cgraph/owl_vision_lazymnistinf_small.png"><figcaption>Optimised memory allocation</figcaption>
</figure>
<p>You can find bigger visualisations of the allocation performed by the new algorithm in this <a href="https://drive.google.com/drive/folders/12KCY9OC6GjuHiH2pRiAjqNi-pz2sNcc1?usp=sharing">link</a>. You can also check <a href="http://demo.ocaml.xyz/mrcnn.html">this page</a> for a demo of this Owl-powered network. If you want to apply it on videos, large images or experiment a bit more, see the <a href="https://github.com/pvdhove/owl-mask-rcnn">GitHub repository</a>. Pre-trained weights on 80 classes of common objects are provided, which have been converted from the TensorFlow implementation mentioned above.</p>
</section>
</section>
<section class="level2" id="as-intermediate-representations">
<h2>As Intermediate Representations</h2>
<p>Programming a GPU is very much like programming a computer cluster. The gain of parallel computing comes with inevitable synchronisation and communication overhead. Therefore GPU computing only makes sense when the computation complexity is high enough to dwarf other overhead.</p>
<p>When offloading the computation to a GPU, we should avoid transmitting data back and forth between the host and the device memory, so eager evaluation is not ideal in this context because the performance will be throttled by copying. This is the gap between CPU computing and a language with eager evaluation. Computation graph essentially fills the gap between Owl and GPU computing simply because the laziness can be simulated now.</p>
<p>From implementation perspective, we only need to write a new engine functor for GPU devices to evaluate a graph; all the others remain the same. I am currently working on the <a href="https://github.com/owlbarn/owl/blob/master/src/opencl/compute/owl_computation_opencl_engine.ml">OpenCL engine</a>. The amount of code for implementing OpenCL engine is surprisingly small, only around 700 ~ 900 LOC. Comparing to the <a href="https://github.com/owlbarn/owl/blob/master/src/base/compute/owl_computation_cpu_engine.ml">CPU engine</a>, the OpenCL engine maintains the memory allocated on both host and device for each node, copying only happens whenever it is necessary, the allocated memory on the device is reused as much as possible.</p>
</section>
<section class="level2" id="summary">
<h2>Summary</h2>
<p>In this chapter, we have introduced the core Computation Graph module in Owl. We start with the general introduction of the computation graph in numerical computing and why we build that in Owl. Then we use several examples to demonstrate how the computation graph module is used in Owl. This is followed by the internal design of this module, most importantly the CGraph stack and its position in the Owl architecture. The computation graph creates a large optimisation space, and this chapter we present one of them in detail, which is to use the pebble game to optimise the memory allocation in Owl computation.</p>
<p>The computation graph is a hot research topic, and there is still much we can do to improve Owl’s performance based on this module. For example, the Neural Compiler still takes extra time to convert and optimise a graph. Both tasks can actually be moved into compilation phase using MetaOCaml, which will squeeze out some extra performance gain for us.</p>
</section>
</section>
</article></div><a href="zoo.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 22</small>Scripting and Zoo System</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>