<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><meta content="OCaml Scientific and Engineering Computing - Tutorial Book" name="description"><meta content="OCaml, Data Science, Data Analytics, Analytics, Functional Programming, Machine Learning, Deep Neural Network, Scientific Computing, Numerical Algorithm, Tutorial, Linear Algebra, Matrix" name="keywords"><meta content="Liang Wang" name="author"><title>Linear Algebra - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script><script data-ad-client="ca-pub-1868946892712371" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-123353217-1"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-123353217-1');</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="linear-algebra">
<h1>Linear Algebra</h1>
<p>Linear Algebra is a key mathematics field behind computer science and numerical computating. A thorough coverage of this topic is apparently beyond the scope of this book. Please refer to <span data-cites="strang2006linear" class="citation">(Strang 2006)</span> for this subject. In this chapter we will follow the basic structure of this book, first giving you a overall picture, then focussing on how to use the functions provided in Owl to solve problems and better understand some basic linear algebra concepts.</p>
<p>The high level APIs of Linear Algebra are provided in the <code>Linalg</code> module. The module provides four types of number types: single precision, double precision, complex single precision, and complex double precision. They are included in <code>Linalg.S</code>, <code>Linalg.D</code>, <code>Linalg.C</code> and <code>Linalg.Z</code> modules respectively. Besides, the <code>Linalg.Generic</code> can do everything that <code>S/D/C/Z</code> can but needs some extra type information.</p>
<section class="level2" id="vectors-and-matrices">
<h2>Vectors and Matrices</h2>
<p>The fundamental problem of linear algebra: solving linear equations. This is more efficiently expressed with vectors and matrices. Therefore, we need to first get familiar with these basic structures in Owl.</p>
<p>Similar to the <code>Linalg</code> module, all the matrix functions can be accessed from the <code>Dense.Matrix</code> module, and support four different type of modules. The <code>Mat</code> module is an alias of <code>Dense.Matrix.D</code>. Except for some functions such as <code>re</code>, most functions are shared by these four submodules. Note that that matrix module is actually built on the <code>Ndarray</code> module, and thus the supported functions are quite similar, and matrices and ndarrays can interoperate with each other. The vectors are expressed using Matrix in Owl.</p>
<section class="level3" id="creating-matrices">
<h3>Creating Matrices</h3>
<p>There are multiple functions to help you in creating an initial matrix to start with.</p>
<div class="highlight">
<pre><code class="language-ocaml">  Mat.empty 5 5;;        (* create a 5 x 5 matrix with initialising elements *)
  Mat.create 5 5 2.;;    (* create a 5 x 5 matrix and initialise all to 2. *)
  Mat.zeros 5 5;;        (* create a 5 x 5 matrix of all zeros *)
  Mat.ones 5 5;;         (* create a 5 x 5 matrix of all ones *)
  Mat.eye 5;;            (* create a 5 x 5 identity matrix *)
  Mat.uniform 5 5;       (* create a 5 x 5 random matrix of uniform distribution *)
  Mat.uniform_int 5 5;;  (* create a 5 x 5 random integer matrix *)
  Mat.sequential 5 5;;   (* create a 5 x 5 matrix of sequential integers *)
  Mat.semidef 5;;        (* create a 5 x 5 random semi-definite matrix *)
  Mat.gaussian 5 5;;     (* create a 5 x 5 random Gaussian matrix *)
  Mat.bernoulli 5 5      (* create a 5 x 5 random Bernoulli  matrix *)</code></pre>
</div>
<p>Owl can create some special matrices with specific properties. For example, a <em>magic square</em> is a <code>n x n</code> matrix (where n is the number of cells on each side) filled with distinct positive integers in the range <span class="math inline">\(1,2,...,n^{2}\)</span> such that each cell contains a different integer and the sum of the integers in each row, column and diagonal is equal.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let x = Mat.magic 5;;
&gt;val x : Mat.mat =
&gt;
&gt;   C0 C1 C2 C3 C4
&gt;R0 17 24  1  8 15
&gt;R1 23  5  7 14 16
&gt;R2  4  6 13 20 22
&gt;R3 10 12 19 21  3
&gt;R4 11 18 25  2  9
</code></pre>
</div>
<p>We can validate this property with the following code. The summation of all the elements on each column is 65.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.sum_rows x;;
&gt;- : Mat.mat =
&gt;   C0 C1 C2 C3 C4
&gt;R0 65 65 65 65 65
</code></pre>
</div>
<p>You can try the similar <code>sum_cols</code>. The summation of all the diagonal elements is also 65.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.trace x;;
&gt;- : float = 65.
</code></pre>
</div>
</section>
<section class="level3" id="accessing-elements">
<h3>Accessing Elements</h3>
<p>Similar to ndarray, the matrix module support <code>set</code> and <code>get</code> to access and modify matrix elements. The only difference is that instead of accessing according to an array, an element in matrix is accessed using two integers.</p>
<div class="highlight">
<pre><code class="language-ocaml">let x = Mat.uniform 5 5;;
Mat.set x 1 2 0.;;             (* set the element at (1,2) to 0. *)
Mat.get x 0 3;;                (* get the value of the element at (0,3) *)</code></pre>
</div>
<p>For dense matrices, i.e., <code>Dense.Matrix.*</code>, you can also use shorthand <code>.%{i; j}</code> to access elements.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">open Mat;;
x.%{1;2} &lt;- 0.;;         (* set the element at (1,2) to 0. *);;
&gt;- : unit = ()
let a = x.%{0;3};;       (* get the value of the element at (0,3) *);;
&gt;val a : float = 0.563556290231645107
</code></pre>
</div>
<p>The modifications to a matrix using <code>set</code> are in-place. This is always true for dense matrices. For sparse matrices, the thing can be complicated because of performance issues.</p>
<p>We can take some rows out of <code>x</code> by calling <code>rows</code> function. The selected rows will be used to assemble a new matrix. Similarly, we can also select some columns using <code>cols</code>.</p>
</section>
<section class="level3" id="iterate-map-fold-and-filter">
<h3>Iterate, Map, Fold, and Filter</h3>
<p>In reality, a matrix usually represents a collections of measurements (or points). We often need to go through these data over and over again for various reasons. Owl provides very convenient functions to help you to iterate these elements. There is one thing I want to emphasise: Owl uses row-major matrix for storage format in the memory, which means accessing rows are much faster than those column operations.</p>
<p>Let’s first create a <code>4 x 6</code> matrix of sequential numbers as below.</p>
<div class="highlight">
<pre><code class="language-ocaml">
let x = Mat.sequential 4 6;;
</code></pre>
</div>
<p>You should be able to see the following output in your <code>utop</code>.</p>
<div class="highlight">
<pre><code class="language-text">
     C0 C1 C2 C3 C4 C5
  R0  1  2  3  4  5  6
  R1  7  8  9 10 11 12
  R2 13 14 15 16 17 18
  R3 19 20 21 22 23 24
</code></pre>
</div>
<p>Iterating all the elements can be done by using <code>iteri</code> function. The following example prints out all the elements on the screen.</p>
<div class="highlight">
<pre><code class="language-ocaml">
Mat.iteri_2d (fun i j a -&gt; Printf.printf "(%i,%i) %.1f\n" i j a) x;;
</code></pre>
</div>
<p>If you want to create a new matrix out of the existing one, you need <code>mapi</code> and <code>map</code> function. E.g., we create a new matrix by adding one to each element in <code>x</code>.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.map ((+.) 1.) x;;
&gt;- : Mat.mat =
&gt;
&gt;   C0 C1 C2 C3 C4 C5
&gt;R0  1  2  3  4  5  6
&gt;R1  7  8  9 10 11 12
&gt;R2 13 14 15 16 17 18
&gt;R3 19 20 21 22 23 24
</code></pre>
</div>
<p>Iterating rows and columns are similar to iterating elements, by using <code>iteri_rows</code>, <code>mapi_rows</code>, and etc. The following example prints the sum of each row.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  Mat.iteri_rows (fun i r -&gt;
    Printf.printf "row %i: %.1f\n" i (Mat.sum' r)
  ) x;;
</code></pre>
</div>
<p>You can also fold elements, rows, and columns. We can calculate the summation of all column vectors by using <code>fold_cols</code> function.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  let v = Mat.(zeros (row_num x) 1) in
  Mat.(fold_cols add v x);;
</code></pre>
</div>
<p>It is also possible to change a specific row or column. E.g., we make a new matrix out of <code>x</code> by setting row <code>2</code> to zero vector.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  Mat.map_at_row (fun _ -&gt; 0.) x 2;;
</code></pre>
</div>
<p>The filter functions is also commonly used in manipulating matrix. Here are some examples. The first one is to filter out the elements in <code>x</code> greater than <code>20</code>.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.filter ((&lt;) 20.) x;;
&gt;- : int array = [|21; 22; 23|]
</code></pre>
</div>
<p>You can compare the next example which filters out the two-dimensional indices.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.filteri_2d (fun i j a -&gt; a &gt; 20.) x;;
&gt;- : (int * int) array = [|(3, 3); (3, 4); (3, 5)|]
</code></pre>
</div>
<p>The second example is to filter out the rows whose summation is less than <code>22</code>.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.filter_rows (fun r -&gt; Mat.sum' r &lt; 22.) x;;
&gt;- : int array = [|0|]
</code></pre>
</div>
<p>If we want to check whether there is one or (or all) element in <code>x</code> satisfying some condition, then</p>
<div class="highlight">
<pre><code class="language-ocaml">
  Mat.exists ((&gt;) 5.) x;;      (* is there someone smaller than 5. *)
  Mat.not_exists ((&gt;) 5.) x;;  (* is no one smaller than 5. *)
  Mat.for_all ((&gt;) 5.) x;;     (* is everyone smaller than 5. *)
</code></pre>
</div>
</section>
<section class="level3" id="math-operations">
<h3>Math Operations</h3>
<p>The math operations can be generally categorised into several groups.</p>
<p><strong>Comparison</strong> Suppose we have two matrices:</p>
<div class="highlight">
<pre><code class="language-ocaml">let x = Mat.uniform 2 2;;
let y = Mat.uniform 2 2;;</code></pre>
</div>
<p>We can compare the relationship of <code>x</code> and <code>y</code> element-wisely as below.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  Mat.(x = y);;    (* is x equal to y *)
  Mat.(x &lt;&gt; y);;   (* is x unequal to y *)
  Mat.(x &gt; y);;    (* is x greater to y *)
  Mat.(x &lt; y);;    (* is x smaller to y *)
  Mat.(x &gt;= y);;   (* is x not smaller to y *)
  Mat.(x &lt;= y);;   (* is x not greater to y *)
</code></pre>
</div>
<p>All aforementioned infix have their corresponding functions in the module, e.g., <code>=@</code> has <code>Mat.is_equal</code>.</p>
<p><strong>Matrix Arithmetic</strong></p>
<p>The arithmetic operation also heavily uses infix. Similar to matrix comparison, each infix has its corresponding function in the module.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  Mat.(x + y);;    (* add two matrices *)
  Mat.(x - y);;    (* subtract y from x *)
  Mat.(x * y);;    (* element-wise multiplication *)
  Mat.(x / y);;    (* element-wise division *)
  Mat.(x *@ y);;    (* dot product of x and y *)
</code></pre>
</div>
<p>If you do match between a matrix and a scalar value, you need to be careful about their order. Please see the examples below. In the following examples, <code>x</code> is a matrix as we used before, and <code>a</code> is a <code>float</code> scalar value.</p>
<div class="highlight">
<pre><code class="language-ocaml">  let a = 2.5;;

  Mat.(x +$ a);;    (* add a to every element in x *)
  Mat.(a $+ x);;    (* add a to every element in x *)
</code></pre>
</div>
<p>Similarly, we have the following examples for other math operations.</p>
<div class="highlight">
<pre><code class="language-ocaml">
  Mat.(x -$ a);;    (* sub a from every element in x *)
  Mat.(a $- x);;
  Mat.(x *$ a);;    (* mul a with every element in x *)
  Mat.(a $* x);;
  Mat.(x /$ a);;    (* div a to every element in x *)
  Mat.(a $/ x);;
  Mat.(x **$ a);;   (* power of every element in x *)
</code></pre>
</div>
<p>There are some ready-made math functions such as <code>Mat.log</code> and <code>Mat.abs</code> ect. to ease your life when operating matrices. These math functions apply to every element in the matrix.</p>
<p>There are other functions such as concatenation:</p>
<div class="highlight">
<pre><code class="language-ocaml">  Mat.(x @= y);;    (* concatenate x and y vertically *)
  Mat.(x @|| y);;   (* concatenate x and y horizontally *)</code></pre>
</div>
</section>
</section>
<section class="level2" id="gaussian-elimination">
<h2>Gaussian Elimination</h2>
<p>Solving linear equations systems is the core problem in Linear Algebra and is frequently used in scientific computation. <em>Gaussian Elimination</em> is a classic method to do that. With a bit of techniques, elimination works surprisingly well in modern numerical libraries as one way of implementation. Here is a simple example.</p>
<p><span class="math display">\[2x_1 + 2x_2 + 2x_3 = 4\]</span> <span id="eq:linear-algebra:gauss01"><span class="math display">\[2x_1 + 2x_2 + 3x_3 = 5\qquad(1)\]</span></span> <span class="math display">\[3w_1 + 4x_2 + 5x_3 = 7\]</span></p>
<p>Divide the first equation by 2:</p>
<p><span class="math display">\[x_1 + x_2 + x_3 = 2\]</span> <span id="eq:linear-algebra:gauss02"><span class="math display">\[2x_1 + 2x_2 + 3x_3 = 5\qquad(2)\]</span></span> <span class="math display">\[3w_1 + 4x_2 + 5x_3 = 7\]</span></p>
<p>Multiply the first equation by <code>-2</code>, then add it to the second one. Also, multiply the first equation by <code>-3</code>, then add it to the third one. We have:</p>
<p><span class="math display">\[x_1 + x_2 + x_3 = 2\]</span> <span id="eq:linear-algebra:gauss03"><span class="math display">\[x_3 = 1\qquad(3)\]</span></span> <span class="math display">\[x_2 + 2x_3 = 1\]</span></p>
<p>Finally, swap the second and third line:</p>
<p><span class="math display">\[x_1 + x_2 + x_3 = 2\]</span> <span id="eq:linear-algebra:gauss04"><span class="math display">\[x_2 + 2x_3 = 1\qquad(4)\]</span></span> <span class="math display">\[x_3 = 1\]</span></p>
<p>Here <span class="math inline">\(x_3 = 1\)</span>, and we can put it back in the second equation and get <span class="math inline">\(x_2 = -1\)</span>. Put both back to the first equation and we have <span class="math inline">\(x_1 = 2\)</span></p>
<p>This process demonstrate the basic process of elimination: eliminate unknown variables until this group of linear equations is easy to solve, and then do the back-substitution. There are three kinds of basic operations we can use: multiplication, adding one line to another, and swap two lines.</p>
<p>The starting eq.&nbsp;1 can be more concisely expressed with vector:</p>
<p><span class="math display">\[x_1\left[\begin{matrix}2\\2\\3\end{matrix} \right] + x_2\left[\begin{matrix}2\\2\\4\end{matrix} \right] + x_3\left[\begin{matrix}2\\3\\5\end{matrix} \right] = \left[\begin{matrix}4\\5\\7\end{matrix} \right]\]</span></p>
<p>or it can be expressed as <span class="math inline">\(Ax=b\)</span> using matrix notation.</p>
<p><span class="math display">\[
\left[\begin{matrix}2 &amp; 2 &amp; 2\\2 &amp; 2 &amp; 3\\3 &amp; 4 &amp; 5\end{matrix} \right] \left[\begin{matrix}x_1\\x_2\\x_3\end{matrix} \right] = \left[\begin{matrix}4\\5\\7\end{matrix} \right]
\Longrightarrow
\left[\begin{matrix}x_1\\x_2\\x_3\end{matrix} \right] = \left[\begin{matrix}2\\-1\\1\end{matrix} \right]
\]</span></p>
<p>Here A is a matrix, b is a column vector, and x is the unknown vector. The matrix notation is often used to describe the linear equation systems as a concise way.</p>
<section class="level3" id="lu-factorisation">
<h3>LU Factorisation</h3>
<p>Let’s check the gaussian elimination example again. The final form in eq.&nbsp;4 can be expressed with the matrix notation as:</p>
<p><span class="math display">\[\left[\begin{matrix}1 &amp; 1 &amp; 1\\0 &amp; 1 &amp; 2\\0 &amp; 0 &amp; 1\end{matrix} \right]\]</span></p>
<p>Here all the elements below the diagonal of this square matrix is zero. Such matrix is called an <em>upper triangular matrix</em>, usually denoted by <span class="math inline">\(U\)</span>. Similarly, a square matrix that all the elements below the diagonal of this square matrix is zero is called <em>lower triangular matrix</em>, denoted by <span class="math inline">\(L\)</span>. We can use the <code>is_triu</code> and <code>is_tril</code> to verify if a matrix is triangular.</p>
<p>The diagonal elements of <span class="math inline">\(U\)</span> are called pivots. The i-th pivot is the coefficient of the i-th variable in the i-th equation at the i-th step during the elimination.</p>
<p>In general, a square matrix can often be factorised into the dot product of a lower and a upper triangular matrices: <span class="math inline">\(A = LU\)</span>. It is called the <em>LU factorisation</em>. It embodies the process of Gauss elimination. Back to the initial problem of solving the linear equation <span class="math inline">\(Ax=b\)</span>. One reason the LU Factorisation is important is that if the matrix A in <span class="math inline">\(Ax=b\)</span> is triangular, then solving it would be straightforward, as we have seen in the previous example. Actually, we can use <code>triangular_solve</code> to efficiently solve the linear equations if we already know that the matrix is triangular.</p>
<p>For a normal square matrix that can be factorised into <span class="math inline">\(LU\)</span>, we can change <span class="math inline">\(Ax=b\)</span> to <span class="math inline">\(LUx=b\)</span>. First we can find column vector <span class="math inline">\(c\)</span> so that <span class="math inline">\(Lc=b\)</span>, then we can find <span class="math inline">\(x\)</span> so that <span class="math inline">\(Ux=c\)</span>. Both triangular equations are easy to solve.</p>
<p>We use the <code>lu</code> function to perform the LU factorisation. Let’s use the previous example.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a = [|2.;2.;2.;2.;2.;3.;3.;4.;5.|];;
&gt;val a : float array = [|2.; 2.; 2.; 2.; 2.; 3.; 3.; 4.; 5.|]
let a = Arr.of_array a [|3; 3|];;
&gt;val a : Arr.arr =
&gt;   C0 C1 C2
&gt;R0  2  2  2
&gt;R1  2  2  3
&gt;R2  3  4  5
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let l, u, p = Linalg.D.lu a;;
&gt;val l : Owl_dense_matrix_d.mat =
&gt;
&gt;         C0 C1 C2
&gt;R0        1  0  0
&gt;R1 0.666667  1  0
&gt;R2 0.666667  1  1
&gt;
&gt;val u : Owl_dense_matrix_d.mat =
&gt;
&gt;   C0        C1        C2
&gt;R0  3         4         5
&gt;R1  0 -0.666667 -0.333333
&gt;R2  0         0        -1
&gt;
&gt;val p : Linalg.D.int32_mat =
&gt;   C0 C1 C2
&gt;R0  3  2  3
</code></pre>
</div>
<p>The first two returned matrix are the lower and upper triangular matrices. However, if we try to check the correctness of this factorisation with dot product, the result does not fit:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a' = Mat.dot l u;;
&gt;val a' : Mat.mat =
&gt;   C0 C1 C2
&gt;R0  3  4  5
&gt;R1  2  2  3
&gt;R2  2  2  2
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">a' = a;;
&gt;- : bool = false
</code></pre>
</div>
<p>It turns out that we need to some extra row exchange to get the right answer. That’s because the row exchange is required in certain cases, such as when the number we want to use as the pivot could be zero. This process is called <em>pivoting</em>. It is closely related to the numerical computation stability. Choosing the improper pivots can lead to wrong linear system solution. It can be expressed with a permutation matrix that has the same rows as the identity matrix, each row and column has exactly one “1” element. The full LU factorisation can be expressed as:</p>
<p><span class="math display">\[PA = LU.\]</span></p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let p = Mat.of_array  [|0.;0.;1.;0.;1.;0.;1.;0.;0.|] 3 3;;
&gt;val p : Mat.mat =
&gt;   C0 C1 C2
&gt;R0  0  0  1
&gt;R1  0  1  0
&gt;R2  1  0  0
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.dot p a = Mat.dot l u;;
&gt;- : bool = true
</code></pre>
</div>
<p>How do we translate the third output, the permutation vector, to the required permutation matrix? Each element <span class="math inline">\(p_i\)</span> in the vector represents a updated identity matrix. On this identity matrix, we set (i, i) and (<span class="math inline">\(p_i\)</span>, <span class="math inline">\(p_i\)</span>) to zero, and then (i, <span class="math inline">\(p_i\)</span>) and (<span class="math inline">\(p_i\)</span>, i) to one. Multiply these <span class="math inline">\(n\)</span> matrices, we can get the permutation matrix <span class="math inline">\(P\)</span>. Here is a brief implementation of this process in OCaml:</p>
<div class="highlight">
<pre><code class="language-ocaml">let perm_vec_to_mat vec =
    let n = Array.length vec in
    let mat = ref (Mat.eye n) in
    for i = n - 1 downto 0 do
      let j = vec.(i) in
      let a = Mat.eye n in
      Arr.set a [| i; i |] 0.;
      Arr.set a [| j; j |] 0.;
      Arr.set a [| i; j |] 1.;
      Arr.set a [| j; i |] 1.;
      mat := Arr.dot a !mat
    done;
    !mat</code></pre>
</div>
<p>Note that there is more than one way to do the LU factorisation. For example, for the same matrix, we can have:</p>
<p><span class="math display">\[\left[\begin{matrix}1 &amp; 0 &amp; 0\\0 &amp; 0 &amp; 1\\0 &amp; 1 &amp; 0\end{matrix} \right] \left[\begin{matrix}2 &amp; 2 &amp; 2\\2 &amp; 2 &amp; 3\\3 &amp; 4 &amp; 5\end{matrix} \right] = \left[\begin{matrix}1 &amp; 0 &amp; 0\\1.5 &amp; 1 &amp; 0\\1 &amp; 0 &amp; 1\end{matrix} \right] \left[\begin{matrix}2 &amp; 2 &amp; 2\\0 &amp; 1 &amp; 2\\0 &amp; 0 &amp; 1\end{matrix} \right]\]</span></p>
</section>
<section class="level3" id="inverse-and-transpose">
<h3>Inverse and Transpose</h3>
<p>The concept of inverse matrix is related with the identity matrix, which can be built with <span class="math inline">\(Mat.eye n\)</span>, where n is the size of the square matrix. The identity matrix is a special form of <em>Diagonal Matrix</em>, which is a square matrix that only contains non-zero element along its diagonal. You can check if a matrix is diagonal with <code>is_diag</code> function.</p>
<div class="highlight">
<pre><code class="language-ocaml">Mat.eye 5 |&gt; Linalg.D.is_diag</code></pre>
</div>
<p>The inverse of a <span class="math inline">\(n\)</span> by <span class="math inline">\(n\)</span> square matrix <span class="math inline">\(A\)</span> is denoted by $A^{-1}, so that $: <span class="math inline">\(AA^{-1} = I_n\)</span>. Note that not all square matrix has inverse.<br>
There are many sufficient and necessary conditions to decide if <span class="math inline">\(A\)</span> is invertible, one of them is that A has <span class="math inline">\(n\)</span> pivots.</p>
<p>We use function <code>inv</code> to do the inverse operation. It’s straightforward and easy to verify according to the definition. Here we use the <code>semidef</code> function to produce a matrix that is certainly invertible.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let x = Mat.semidef 5;;
&gt;val x : Mat.mat =
&gt;
&gt;        C0       C1       C2      C3       C4
&gt;R0 2.56816   1.0088  1.57793  2.6335  2.06612
&gt;R1  1.0088 0.441613 0.574465 1.02067 0.751004
&gt;R2 1.57793 0.574465  2.32838 2.41251  2.13926
&gt;R3  2.6335  1.02067  2.41251 3.30477  2.64877
&gt;R4 2.06612 0.751004  2.13926 2.64877  2.31124
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let y = Linalg.D.inv x;;
&gt;val y : Owl_dense_matrix_d.mat =
&gt;
&gt;         C0       C1       C2       C3       C4
&gt;R0   12.229  -15.606  6.12229 -1.90254 -9.34742
&gt;R1  -15.606  33.2823 -4.01361 -4.96414  12.5403
&gt;R2  6.12229 -4.01361  7.06372 -2.62899 -7.69399
&gt;R3 -1.90254 -4.96414 -2.62899   8.1607 -3.60533
&gt;R4 -9.34742  12.5403 -7.69399 -3.60533  15.9673
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.(x *@ y =~ eye 5);;
&gt;- : bool = true
</code></pre>
</div>
<p>The next frequently used special matrix is the <em>Transpose Matrix</em>. Denoted by <span class="math inline">\(A^T\)</span>, its <span class="math inline">\(i\)</span>th row is taken from the <span class="math inline">\(i\)</span>-th column of the original matrix A. It has properties such as <span class="math inline">\((AB)^T=B^T~A^T\)</span>. We can check this property using the matrix function <code>Mat.transpose</code>. Note that this function is deemed basic ndarray operations and is not included in the <code>Linalg</code> module.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let flag =
  let a = Mat.uniform 4 4 in
  let b = Mat.uniform 4 4 in
  let m1 = Mat.(dot a b |&gt; transpose) in
  let m2 = Mat.(dot (transpose b) (transpose a)) in
  Mat.(m1 =~ m2)
;;
&gt;val flag : bool = true
</code></pre>
</div>
<p>A related special matrix is the <em>Symmetric Matrix</em>, which equals to its own transpose. This simple test can be done with the <code>is_symmetric</code> function.</p>
</section>
</section>
<section class="level2" id="vector-spaces">
<h2>Vector Spaces</h2>
<p>We have talked about solving the <span class="math inline">\(Ax=b\)</span> linear equations with elimination, and A is a square matrix. Now we need to further discuss, how do we know if there exists one or maybe more than one solution. To answer such question, we need to be familiar with the concepts of <em>vector space</em>.</p>
<p>A vector space, denoted by <span class="math inline">\(R^n\)</span>, contains all the vectors that has <span class="math inline">\(n\)</span> elements. In this vector space we have the <code>add</code> and <code>multiplication</code> operation. Applying them to the vectors is called <em>linear combination</em>. Then a <em>subspace</em> in a vector space is a non-empty set that linear combination of the vectors in this subspace still stays in the same subspace.</p>
<p>There are four fundamental subspaces concerning solving linear systems <span class="math inline">\(Ax=b\)</span>, where <span class="math inline">\(A\)</span> is a <span class="math inline">\(m\)</span> by <span class="math inline">\(n\)</span> matrix. The <em>column space</em> consists of all the linear combinations of the columns of A. It is a subspace of <span class="math inline">\(R^m\)</span>. Similarly, the <em>row space</em> consists of all the linear combinations of the rows of A. The <em>nullspace</em> contains all the vectors <span class="math inline">\(x\)</span> so that <span class="math inline">\(Ax=0\)</span>, denoted by <span class="math inline">\(N(A)\)</span>. It is a subspace of <span class="math inline">\(R^n\)</span>. The <em>left nullspace</em> is similar. It is the nullspace of <span class="math inline">\(A^T\)</span>.</p>
<section class="level3" id="rank-and-basis">
<h3>Rank and Basis</h3>
<p>In the Gaussian Elimination section, we assume an ideal situation: the matrix A is <span class="math inline">\(n\times~n\)</span> square, and we assume that there exists one solution. But that does not happen every time. In many cases <span class="math inline">\(A\)</span> is not an square matrix. It is possible that these <span class="math inline">\(m\)</span> equations are not enough to solve a <span class="math inline">\(n\)</span>-variable linear system when <span class="math inline">\(m &lt; n\)</span>. Or there might not exist a solution when <span class="math inline">\(m &gt; n\)</span>. Besides, even it is a square matrix, the information provided by two of the equations are actually repeated. For example, one equation is simply a multiplication of the other.</p>
<p>For example, if we try to apply LU factorisation to such a matrix:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let x = Mat.of_array [|1.; 2.; 3.; 0.; 0.; 1.; 0.; 0.; 2.|] 3 3;;
&gt;val x : Mat.mat =
&gt;   C0 C1 C2
&gt;R0  1  2  3
&gt;R1  0  0  1
&gt;R2  0  0  2
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Linalg.D.lu x;;
&gt;Exception: Failure "LAPACKE: 2".
</code></pre>
</div>
<p>Obviously, we cannot have pivot in the second column, and therefore this matrix is singular and cannot be factorised into <span class="math inline">\(LU\)</span>. As can be seen in this example, we cannot expect the linear algebra functions to be a magic lamb and do our bidding every time. Understanding the theory of linear algebra helps to better understand how these functions work.</p>
<p>To decide the general solutions to <span class="math inline">\(Ax=b\)</span>, we need to understand the concept of <em>rank</em>. The rank of a matrix is the number of pivots in the elimination process. To get a more intuitive understanding of rank, we need to know the concept of *linear independent. In a linear combination <span class="math inline">\(\sum_{i=1}^nc_iv_i\)</span> where <span class="math inline">\(v_i\)</span> are vectors and <span class="math inline">\(c_i\)</span> are numbers, if <span class="math inline">\(\sum_{i=1}^nc_iv_i = 0\)</span> only happens when <span class="math inline">\(c_i = 0\)</span> for all the <span class="math inline">\(i\)</span>’s, then the vectors <span class="math inline">\(v_1, v_2, \ldots, v_n\)</span> are linearly independent. Then the rank of a matrix is the number of independent rows in the matrix. We can understand rank as the number of “effective” rows in the matrix.</p>
<p>As an example, we can check the rank of the previous matrix.</p>
<div class="highlight">
<pre><code class="language-ocaml">Linalg.D.rank x</code></pre>
</div>
<p>As can be example, the rank is 2, which means only two effective rows, and thus cannot be factorised to find the only solution.</p>
<p>One application of rank is in a crucial linear algebra idea: basis. A sequence of vectors is the <em>basis</em> of a space or subspace if: 1) these vectors are linear independent and 2) all the the vectors in the space can be represented as the linear combination of vectors in the basis.</p>
<p>A space can have infinitely different bases, but the number of vectors in these bases are the same. This number is called the <em>dimension</em> of this vector space. For example, a <span class="math inline">\(m\)</span> by <span class="math inline">\(n\)</span> matrix A has rank of <span class="math inline">\(r\)</span>, then the dimension of its null space is <span class="math inline">\(n-r\)</span>, and the dimension of its column space is <span class="math inline">\(r\)</span>. Think about a full-rank matrix where <span class="math inline">\(r=n\)</span>, then the dimension of column matrix is <span class="math inline">\(n\)</span>, which means all its columns can be a basis of the column space, and that the null space dimension is zero so that the only solution of <span class="math inline">\(Ax=0\)</span> is a zero vector.</p>
</section>
<section class="level3" id="orthogonality">
<h3>Orthogonality</h3>
<p>We can think of the basis of a vector space as the Cartesian coordinate system in a three-dimensional space, where every vector in the space can be represented with the three vectors ni the space: the x, y and z axis. Actually, we can use many three vectors system as the coordinate bases, but the x. y, z axis is used is because they are orthogonal to each other. An orthogonal basis can greatly reduce the complexity of problems. The same can be applied in the basis of vector spaces.</p>
<p>Orthogonality is not limited to vectors. Two vectors <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are orthogonal are orthogonal if <span class="math inline">\(a^Tb = 0\)</span>. Two subspaces A and B are orthogonal if every vector in A is orthogonal to every vector in B. For example, the nullspace and row space of a matrix are perpendicular to each other.</p>
<p>Among the bases of a subspace, if every vector is perpendicular to each other, it is called an orthogonal matrix. Moreover, if the length of each vector is normalised to one unit, it becomes the <em>orthonormal basis</em>.</p>
<p>For example, we can use the <code>null</code> function to find an orthonormal basis vector <span class="math inline">\(x\)</span> or the null space of a matrix, i.e.&nbsp;<span class="math inline">\(Ax=0\)</span>.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a = Mat.magic 4;;
&gt;val a : Mat.mat =
&gt;
&gt;   C0 C1 C2 C3
&gt;R0  1 15 14  4
&gt;R1 12  6  7  9
&gt;R2  8 10 11  5
&gt;R3 13  3  2 16
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let x = Linalg.D.null a;;
&gt;val x : Owl_dense_matrix_d.mat =
&gt;
&gt;          C0
&gt;R0 -0.223607
&gt;R1  -0.67082
&gt;R2   0.67082
&gt;R3  0.223607
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.dot a x |&gt; Mat.l2norm';;
&gt;- : float = 2.87802701599908967e-15
</code></pre>
</div>
<p>Now that we know what is orthogonal basis, the next question is, how to build one? The <em>QR Factorisation</em> is used to construct orthogonal basis in a subspace. Specifically, it decomposes a matrix A into the product of an orthogonal matrix Q and an upper triangular matrix R, i.e.&nbsp;A = QR. It is provided in the linear algebra module.</p>
<div class="highlight">
<pre><code class="language-clike">val qr : ?thin:bool -&gt; ?pivot:bool -&gt; ('a, 'b) t -&gt; ('a, 'b) t * ('a, 'b) t * (int32, int32_elt) t</code></pre>
</div>
<p>The <code>qr x</code> function calculates QR decomposition for an <code>m</code> by <code>n</code> matrix <code>x</code>. The function returns a 3-tuple, the first two are <code>Q</code> and <code>R</code>, and the third is the permutation vector of columns. The default value of parameter <code>pivot</code> is <code>false</code>, setting pivot to true lets <code>qr</code> performs pivoted factorisation. Note that the returned indices are not adjusted to 0-based C layout. By default, <code>qr</code> performs a reduced QR factorisation, full factorisation can be enabled by setting <code>thin</code> parameter to <code>false</code>.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a = Mat.of_array [|12.; -51.; 4.; 6.; 167.; -68.; -4.; 24.; -41.|] 3 3;;
&gt;val a : Mat.mat =
&gt;
&gt;   C0  C1  C2
&gt;R0 12 -51   4
&gt;R1  6 167 -68
&gt;R2 -4  24 -41
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let q, r, _ = Linalg.D.qr a;;
&gt;val q : Owl_dense_matrix_d.mat =
&gt;
&gt;          C0        C1         C2
&gt;R0 -0.857143  0.394286   0.331429
&gt;R1 -0.428571 -0.902857 -0.0342857
&gt;R2  0.285714 -0.171429   0.942857
&gt;
&gt;val r : Owl_dense_matrix_d.mat =
&gt;
&gt;    C0   C1  C2
&gt;R0 -14  -21  14
&gt;R1   0 -175  70
&gt;R2   0    0 -35
</code></pre>
</div>
</section>
<section class="level3" id="solving-ax-b">
<h3>Solving Ax = b</h3>
<p>We can now discuss the general solution structure to <span class="math inline">\(Ax=0\)</span> and <span class="math inline">\(Ax=b\)</span>. Again, here <span class="math inline">\(A\)</span> is a <span class="math inline">\(m\times~n\)</span> matrix. The theorems declare that, there exists non-zero solution(s) to <span class="math inline">\(Ax=0\)</span> if and only if <span class="math inline">\(\textrm{rank}(a) &lt;= n\)</span>. If <span class="math inline">\(r(A) &lt; n\)</span>, then the nullspace of <span class="math inline">\(A\)</span> is of dimension <span class="math inline">\(n - r\)</span> and the <span class="math inline">\(n-r\)</span> orthogonal basis can be found with <code>null</code> function. Here is an example.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a = Mat.of_array [|1.;5.;-1.;-1.;1.;-2.;1.;3.;3.;8.;-1.;1.;1.;-9.;3.;7.|] 4 4;;
&gt;val a : Mat.mat =
&gt;
&gt;   C0 C1 C2 C3
&gt;R0  1  5 -1 -1
&gt;R1  1 -2  1  3
&gt;R2  3  8 -1  1
&gt;R3  1 -9  3  7
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Linalg.D.rank a;;
&gt;- : int = 2
</code></pre>
</div>
<p>This a rank 2 matrix, so the nullspace contains 4 - 2 = 2 vectors:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Linalg.D.null a;;
&gt;- : Owl_dense_matrix_d.mat =
&gt;
&gt;          C0        C1
&gt;R0 -0.851419 0.0136382
&gt;R1  0.273706  0.143885
&gt;R2 0.0762491  0.962526
&gt;R3   0.44086 -0.229465
</code></pre>
</div>
<p>These two vectors are called the <em>fundamental system of solutions</em> of <span class="math inline">\(Ax=0\)</span>. All the solutions of <span class="math inline">\(Ax=0\)</span> can then be expressed using the fundamental system:</p>
<p><span class="math display">\[c_1\left[\begin{matrix}-0.85 \\ 0.27 \\ 0.07 \\0.44\end{matrix} \right] + c_2\left[\begin{matrix}0.013\\ 0.14 \\ 0.95 \\-0.23\end{matrix} \right]\]</span></p>
<p>Here <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> can be any constant numbers.</p>
<p>For solving the general form <span class="math inline">\(Ax=b\)</span> where b is <span class="math inline">\(m\times~1\)</span> vector, there exist only one solution if and only if <span class="math inline">\(\textrm{rank}(A) = \textrm{rank}([A, b]) = n\)</span>. Here <span class="math inline">\([A, b]\)</span> means concatenating <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span> along the column. If <span class="math inline">\(\textrm{rank}(A) = \textrm{rank}([A, b]) &lt; n\)</span>, <span class="math inline">\(Ax=b\)</span> has infinite number of solutions. These solutions has a general form:</p>
<p><span class="math display">\[x_0 + c_1x_1 + c2x2 + \ldots +c_kx_k\]</span></p>
<p>Here <span class="math inline">\(x_0\)</span> is a particular solution to <span class="math inline">\(Ax=b\)</span>, and <span class="math inline">\(x_1, x_2, \ldots, x_k\)</span> are the fundamental solution system of <span class="math inline">\(Ax=0\)</span>.</p>
<p>We can use <code>linsolve</code> function to find one particular solution. In the Linear Algebra, the function <code>linsolve a b -&gt; x</code> solves a linear system of equations <code>a * x = b</code>. By default, the function uses LU factorisation with partial pivoting when <code>a</code> is square and QR factorisation with column pivoting otherwise. The number of rows of <code>a</code> must be equal to the number of rows of <code>b</code>. If <code>a</code> is a upper or lower triangular matrix, the function calls the <code>solve_triangular</code> function.</p>
<p>Here is an example.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a = Mat.of_array [|2.;3.;1.;1.;-2.;4.;3.;8.;-2.;4.;-1.;9.|] 4 3;;
&gt;val a : Mat.mat =
&gt;
&gt;   C0 C1 C2
&gt;R0  2  3  1
&gt;R1  1 -2  4
&gt;R2  3  8 -2
&gt;R3  4 -1  9
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let b = Mat.of_array [|4.;-5.;13.;-6.|] 4 1;;
&gt;val b : Mat.mat =
&gt;   C0
&gt;R0  4
&gt;R1 -5
&gt;R2 13
&gt;R3 -6
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let x0 = Linalg.D.linsolve a b;;
&gt;val x0 : Owl_dense_matrix_d.mat =
&gt;   C0
&gt;R0 -5
&gt;R1  4
&gt;R2  2
</code></pre>
</div>
<p>Then we use <code>null</code> to find the fundamental solution system. You can verify that matrix <code>a</code> is of rank 2, so that the solution system for <span class="math inline">\(ax=0\)</span> should contain only 3 - 2 = 1 vector.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let x1 = Linalg.D.null a;;
&gt;val x1 : Owl_dense_matrix_d.mat =
&gt;
&gt;          C0
&gt;R0 -0.816497
&gt;R1  0.408248
&gt;R2  0.408248
</code></pre>
</div>
<p>So the solutions to <span class="math inline">\(Ax=b\)</span> can be expressed as:</p>
<p><span class="math display">\[\left[\begin{matrix}-1 \\ 2 \\ 0 \end{matrix} \right] + c_1\left[\begin{matrix}-0.8\\ 0.4 \\ 0.4 \end{matrix} \right]\]</span></p>
<p>So the takeaway from this chapter is that the using these linear algebra functions often requires solid background knowledge. Blindly using them could leads to wrong or misleading answers.</p>
</section>
<section class="level3" id="matrix-sensitivity">
<h3>Matrix Sensitivity</h3>
<p>The <em>sensitivity</em> of a matrix is perhaps not the most important issue in the traditional linear algebra, but is crucial in the numerical computation related problems. It answers this question: in <span class="math inline">\(Ax=b\)</span>, if we change the <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span> slightly, how much will the <span class="math inline">\(x\)</span> be affected? The <em>Condition Number</em> is a measurement of the sensitivity of a square matrix.</p>
<p>First, we need to understand the <em>Norm</em> of a matrix. The norm, or 2-norm of a matrix <span class="math inline">\(\|A\|\)</span> is calculated as square root of the maximum eigenvalue of <span class="math inline">\(A^HA\)</span>. The norm of a matrix is a upper limit so that for any <span class="math inline">\(x\)</span> we can be certain that <span class="math inline">\(\|Ax\| \leq \|A\|\|x\|\)</span>. Here <span class="math inline">\(\|Ax\|\)</span> and <span class="math inline">\(\|x\|\)</span> are the L2-Norm for vectors. The $|A|$ bounds the how large the <span class="math inline">\(A\)</span> can amplify the input <span class="math inline">\(x\)</span>. We can calculate the norm with <code>norm</code> in the linear algebra module.</p>
<p>The most frequently used condition number is that represent the sensitivity of inverse matrix. With the definition of norm, the <em>condition number for inversion</em> of a matrix can be expressed as <span class="math inline">\(\|A\|\|A^{-1}\|\)</span>. We can calculate it using the <code>cond</code> function.</p>
<p>Let’s look at an example:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a = Mat.of_array [|4.1; 2.8; 9.7; 6.6 |] 2 2;;;;
&gt;val a : Mat.mat =
&gt;    C0  C1
&gt;R0 4.1 2.8
&gt;R1 9.7 6.6
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let c = Linalg.D.cond a;;
&gt;val c : float = 1622.99938385651058
</code></pre>
</div>
<p>Its condition number for inversion is much larger than one. Therefore, a small change in <span class="math inline">\(A\)</span> should leads to a large change of <span class="math inline">\(A^{-1}\)</span>.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a' = Linalg.D.inv a;;
&gt;val a' : Owl_dense_matrix_d.mat =
&gt;    C0  C1
&gt;R0 -66  28
&gt;R1  97 -41
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a2 = Mat.of_array [|4.1; 2.8; 9.67; 6.607 |] 2 2;;
&gt;val a2 : Mat.mat =
&gt;     C0    C1
&gt;R0  4.1   2.8
&gt;R1 9.67 6.607
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a2' = Linalg.D.inv a2;;
&gt;val a2' : Owl_dense_matrix_d.mat =
&gt;
&gt;         C0       C1
&gt;R0  520.236 -220.472
&gt;R1 -761.417  322.835
</code></pre>
</div>
<p>We can see that by changing the matrix by only a tiny bit, the inverse of <span class="math inline">\(A\)</span> changes dramatically, and so is the resulting solution vector <span class="math inline">\(x\)</span>.</p>
</section>
</section>
<section class="level2" id="determinants">
<h2>Determinants</h2>
<p>Other than pivots, another basic quantity in linear algebra is the <em>determinants</em>. For a square matrix A:</p>
<p><span class="math display">\[\left[\begin{matrix}a_{11} &amp; a_{12} &amp; \ldots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \ldots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\ a_{n1} &amp; a_{n2} &amp; \ldots &amp; a_{nn} \end{matrix} \right]\]</span></p>
<p>its determinants <code>det(A)</code> is defined as:</p>
<p><span class="math display">\[\sum_{j_1~j_2~\ldots~j_n}(-1)^{\tau(j_1~j_2~\ldots~j_3)}a_{1{j_1}}a_{2j_2}\ldots~a_{nj_n}.\]</span></p>
<p>Here <span class="math inline">\(\tau(j_1~j_2~\ldots~j_n) = i_1 + i_2 + \ldots + i_{n-1}\)</span>, where <span class="math inline">\(i_k\)</span> is the number of <span class="math inline">\(j_p\)</span> that is smaller than <span class="math inline">\(j_k\)</span> for <span class="math inline">\(p \in [k+1, n]\)</span>.</p>
<p>Mathematically, there are many techniques that can be used to simplify this calculation. But as far as this book is concerned, it is sufficient for us to use the <code>det</code> function to calculate the determinants of a matrix.</p>
<p>Why is the concept of determinant important? Its most important application is to using determinant to decide if a square matrix A is invertible or singular. The determinant <span class="math inline">\(\textrm{det}(A) \neq 0\)</span> if and only if <span class="math inline">\(\textrm{A} = n\)</span>. Also it can be expressed as <span class="math inline">\(\textrm{det}(A) \neq 0\)</span> if and only if matrix A is invertible.</p>
<p>We can also use it to understand the solution of <span class="math inline">\(Ax=b\)</span>: if <span class="math inline">\(\textrm{det}(A) \neq 0\)</span>, then <span class="math inline">\(Ax=b\)</span> has one and only one solution. This theorem is part of the <em>Cramer’s rule</em>. These properties are widely used in finding <em>eigenvalues</em>. As will be shown in the next section.</p>
<p>Since sometimes we only care about if the determinant is zero or not, instead of the value itself, we can also use a similar function <code>logdet</code>. It computes the logarithm of the determinant, but it avoids the possible overflow or underflow problems in computing determinant of large matrices.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let x = Mat.magic 5;;
&gt;val x : Mat.mat =
&gt;
&gt;   C0 C1 C2 C3 C4
&gt;R0 17 24  1  8 15
&gt;R1 23  5  7 14 16
&gt;R2  4  6 13 20 22
&gt;R3 10 12 19 21  3
&gt;R4 11 18 25  2  9
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Linalg.D.det x;;
&gt;- : float = 5070000.00000000093
Linalg.D.logdet x;;
&gt;- : float = 15.4388513755673653
</code></pre>
</div>
</section>
<section class="level2" id="eigenvalues-and-eigenvectors">
<h2>Eigenvalues and Eigenvectors</h2>
<section class="level3" id="solving-axlambdax">
<h3>Solving <span class="math inline">\(Ax=\lambda~x\)</span></h3>
<p>Now we need to change the topic from <span class="math inline">\(Ax=b\)</span> to <span class="math inline">\(Ax=\lambda~x\)</span>. For an <span class="math inline">\(n\times~n\)</span> square matrix, if there exist number <span class="math inline">\(\lambda\)</span> and non-zero column vector <span class="math inline">\(x\)</span> to satisfy:</p>
<p><span id="eq:linear-algebra:eigen"><span class="math display">\[(\lambda~I - A)x = 0,\qquad(5)\]</span></span></p>
<p>then <span class="math inline">\(\lambda\)</span> is called <em>eigenvalue</em>, and <span class="math inline">\(x\)</span> is called the <em>eigenvector</em> of <span class="math inline">\(A\)</span>.</p>
<p>To find the eigenvalues of A, we need to find the roots of the determinant of <span class="math inline">\(\lambda~I - A\)</span>. <span class="math inline">\(\textrm{det}(\lambda~I - A) = 0\)</span> is called the <em>characteristic equation</em>. For example, for</p>
<p><span class="math display">\[A = \left[\begin{matrix}3 &amp; 1 &amp; 0 \\ -4 &amp; -1 &amp; 0 \\ 4 &amp; -8 &amp; 2 \end{matrix} \right]\]</span></p>
<p>Its characteristic matrix <span class="math inline">\(\lambda~I - A\)</span> is:</p>
<p><span class="math display">\[\left[\begin{matrix}\lambda-3 &amp; 1 &amp; 0 \\ -4 &amp; \lambda+1 &amp; 0 \\ 4 &amp; -8 &amp; \lambda-2 \end{matrix} \right]\]</span></p>
<p>According to the definition of determinant,</p>
<p><span class="math display">\[\textrm{det}(\lambda~I - A) = (\lambda-1)^2(\lambda-2) = 0.\]</span></p>
<p>According to the theory of polynomials, this characteristic polynomials has and only has <span class="math inline">\(n\)</span> roots in the complex space. Specifically, here we have three eigenvalues: <span class="math inline">\(\lambda_1=1, \lambda_2 = 1, \lambda=2\)</span>.</p>
<p>Put <span class="math inline">\(\lambda_1\)</span> back to characteristic equation, we have: <span class="math inline">\((I - A)x = 0\)</span>. Therefore, we can find the fundamental solution system of <span class="math inline">\(I - A\)</span> with:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let basis =
  let ia = Mat.((eye 3) - (of_array [|3.;1.;0.;-4.;-1.;0.;4.;-8.;2.|] 3 3)) in
  Linalg.D.null ia
;;
&gt;val basis : Owl_dense_matrix_d.mat =
&gt;
&gt;           C0
&gt;R0 -0.0496904
&gt;R1  0.0993808
&gt;R2   0.993808
</code></pre>
</div>
<p>We have a fundamental solution <span class="math inline">\(x_0 = [-0.05, 0.1, 1]^T\)</span>. Therefore all the <span class="math inline">\(k_0x_0\)</span> are the corresponding eigenvector of the eigenvalue <span class="math inline">\(1\)</span>. Similarly, we can calculate that eigenvectors for the eigenvalue <span class="math inline">\(2\)</span> are <span class="math inline">\(k_1[0, 0, 1]^T\)</span>.</p>
<p>We can use <code>eig</code> to find the eigenvectors and eigenvalues of a matrix. <code>eig x -&gt; v, w</code> computes the right eigenvectors <code>v</code> and eigenvalues <code>w</code> of an arbitrary square matrix <code>x</code>. The eigenvectors are column vectors in <code>v</code>, their corresponding eigenvalues have the same order in <code>w</code> as that in <code>v</code>.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let eigvec, eigval =
  let a = Mat.of_array [|3.;1.;0.;-4.;-1.;0.;4.;-8.;2.|] 3 3 in
  Linalg.D.eig a
;;
&gt;val eigvec : Owl_dense_matrix_z.mat =
&gt;
&gt;        C0               C1               C2
&gt;R0 (0, 0i)  (0.0496904, 0i)  (0.0496904, 0i)
&gt;R1 (0, 0i) (-0.0993808, 0i) (-0.0993808, 0i)
&gt;R2 (1, 0i)  (-0.993808, 0i)  (-0.993808, 0i)
&gt;
&gt;val eigval : Owl_dense_matrix_z.mat =
&gt;
&gt;        C0      C1      C2
&gt;R0 (2, 0i) (1, 0i) (1, 0i)
</code></pre>
</div>
<p>Note that the result are expressed as complex numbers. If we only want the eigenvalues, we can use the <code>eigvals</code> function. Both functions provide the boolean <code>permute</code> and <code>scale</code> arguments to indicate whether the input matrix should be permuted and/or diagonally scaled. One reason that eigenvalue and eigenvector are important is that the pattern <span class="math inline">\(Ax=\lambda~x\)</span> frequently appears in scientific and engineering analysis to describe the change of dynamic system over time.</p>
</section>
<section class="level3" id="complex-matrices">
<h3>Complex Matrices</h3>
<p>As can be seen in the previous example, complex matrices are frequently used in eigenvalues in eigenvectors. In this section we re-introduce some previous concepts in the complex space.</p>
<p>We have seen the Symmetric Matrix. It can be extended to the complex numbers, called <em>Hermitian Matrix</em>, denoted by <span class="math inline">\(A^H\)</span>. Instead of requiring it to be the same as its transpose, a hermitian matrix equals to its conjugate transpose. A conjugate transpose means that during transposing, each element <span class="math inline">\(a+bi\)</span> changes to its conjugate <span class="math inline">\(a-bi\)</span>. Hermitian is thus a generalisation of the symmetric matrix. We can use the <code>is_hermitian</code> function to check if a matrix is hermitian, as can be shown in the next example.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a = Dense.Matrix.Z.of_array [|{re=1.; im=0.}; {re=2.; im=(-1.)}; {re=2.; im=1.}; {re=3.; im=0.}|] 2 2;;
&gt;val a : Dense.Matrix.Z.mat =
&gt;
&gt;        C0       C1
&gt;R0 (1, 0i) (2, -1i)
&gt;R1 (2, 1i)  (3, 0i)
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Linalg.Generic.is_hermitian a;;
&gt;- : bool = true
</code></pre>
</div>
<p>We can use the <code>conj</code> function of a complex matrix to perform the conjugate transpose:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Dense.Matrix.Z.(conj a |&gt; transpose);;
&gt;- : Dense.Matrix.Z.mat =
&gt;
&gt;         C0       C1
&gt;R0 (1, -0i) (2, -1i)
&gt;R1  (2, 1i) (3, -0i)
</code></pre>
</div>
<p>A theorem declares that if a matrix is hermitian, then for all complex vectors <span class="math inline">\(x\)</span>, <span class="math inline">\(x^HAx\)</span> is real, and every eigenvalue is real.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Linalg.Z.eigvals a;;
&gt;- : Owl_dense_matrix_z.mat =
&gt;
&gt;                         C0                      C1
&gt;R0 (-0.44949, 1.50231E-17i) (4.44949, 2.07021E-16i)
</code></pre>
</div>
<p>A related concept is the <em>Unitary Matrix</em>. A matrix <span class="math inline">\(U\)</span> is unitary if <span class="math inline">\(U^HU=I\)</span>. The inverse and conjugate transpose of <span class="math inline">\(U\)</span> are the same. It can be compared to the orthogonal vectors in the real space.</p>
</section>
<section class="level3" id="similarity-transformation-and-diagonalisation">
<h3>Similarity Transformation and Diagonalisation</h3>
<p>For a <span class="math inline">\(nxn\)</span> matrix A, and any invertible <span class="math inline">\(nxn\)</span> matrix M, the matrix <span class="math inline">\(B = M^{-1}AM\)</span> is <em>similar</em> to A. One important property is that similar matrices share the same eigenvalues. The intuition is that, think of M as the change of basis matrix, and A itself is a linear transformation, so <span class="math inline">\(M^{-1}AM\)</span> means changing the basis first, applying the linear transformation, and then change the basis back. Therefore, changing from A to B actually changes the linear transformation using one set of basis to another.</p>
<p>In a three dimensional space, if we can change using three random vectors as the basis of linear transformation to using the standard basis <span class="math inline">\([1, 0, 0]\)</span>, <span class="math inline">\([0, 1, 0]\)</span>, <span class="math inline">\([0, 0, 1]\)</span>, the related problem can be greatly simplified. Finding the suitable similar matrix is thus important in simplifying the calculation in many scientific and engineering problems.</p>
<p>One possible kind of simplification is to find a triangular matrix as similar. The <em>Schur’s Lemma</em> declares that A can be decomposed into <span class="math inline">\(UTU^{-1}\)</span> where <span class="math inline">\(U\)</span> is a unitary function, and T is an upper triangular matrix.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a = Dense.Matrix.Z.of_array [|{re=1.; im=0.}; {re=1.; im=0.}; {re=(-2.); im=0.}; {re=3.; im=0.}|] 2 2;;
&gt;val a : Dense.Matrix.Z.mat =
&gt;
&gt;         C0      C1
&gt;R0  (1, 0i) (1, 0i)
&gt;R1 (-2, 0i) (3, 0i)
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let t, u, eigvals = Linalg.Z.schur a;;
&gt;val t : Owl_dense_matrix_z.mat =
&gt;
&gt;        C0                    C1
&gt;R0 (2, 1i) (2.10381, -0.757614i)
&gt;R1 (0, 0i)              (2, -1i)
&gt;
&gt;val u : Owl_dense_matrix_z.mat =
&gt;
&gt;                       C0                      C1
&gt;R0 (-0.408248, 0.408248i)  (0.563384, -0.590987i)
&gt;R1        (-0.816497, 0i) (-0.577185, 0.0138014i)
&gt;
&gt;val eigvals : Owl_dense_matrix_z.mat =
&gt;
&gt;        C0       C1
&gt;R0 (2, 1i) (2, -1i)
</code></pre>
</div>
<p>The returned result <code>t</code> is apparent a upper triangular matrix, and the <code>u</code> can be verified to be a unitary matrix:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Dense.Matrix.Z.(dot u (conj u |&gt; transpose));;
&gt;- : Dense.Matrix.Z.mat =
&gt;
&gt;                             C0                          C1
&gt;R0                      (1, 0i) (7.97973E-17, 5.81132E-17i)
&gt;R1 (7.97973E-17, -5.81132E-17i)                     (1, 0i)
</code></pre>
</div>
<p>Another very important similar transformation is <em>diagonalisation</em>. Suppose A has <span class="math inline">\(n\)</span> linear-independent eigenvectors, and make them the columns of a matrix Q, then <span class="math inline">\(Q^{-1}AQ\)</span> is a diagonal matrix <span class="math inline">\(\Lambda\)</span>, and the eigenvalues of A are the diagonal elements of <span class="math inline">\(\Lambda\)</span>. It’s inverse <span class="math inline">\(A = Q\Lambda~Q^{-1}\)</span> is called <em>Eigendecomposition</em>. Analysing A’s diagonal similar matrix <span class="math inline">\(\Lambda\)</span> instead of A itself can greatly simplify the problem.</p>
<p>Not every matrix can be diagonalised. If any two of the <span class="math inline">\(n\)</span> eigenvalues of A are not the same, then its <span class="math inline">\(n\)</span> eigenvectors are linear-independent ana thus A can be diagonalised. Specifically, every real symmetric matrix can be diagonalised by an orthogonal matrix. Or put into the complex space, every hermitian matrix can be diagonalised by a unitary matrix.</p>
</section>
</section>
<section class="level2" id="positive-definite-matrices">
<h2>Positive Definite Matrices</h2>
<section class="level3" id="positive-definiteness">
<h3>Positive Definiteness</h3>
<p>In this section we introduce the concept of <em>Positive Definite Matrix</em>, which unifies the three most basic ideas in linear algebra: pivots, determinants, and eigenvalues.</p>
<p>A matrix is called <em>Positive Definite</em> if it is symmetric and that <span class="math inline">\(x^TAx &gt; 0\)</span> for all non-zero vectors <span class="math inline">\(x\)</span>. There are several necessary and sufficient condition for testing if a symmetric matrix A is positive definite:</p>
<ol type="1">
<li><span class="math inline">\(x^TAx&gt;0\)</span> for all non-zero real vectors x</li>
<li><span class="math inline">\(\lambda_i &gt;0\)</span> for all eigenvalues <span class="math inline">\(\lambda_i\)</span> of A</li>
<li>all the upper left matrices have positive determinants</li>
<li>all the pivots without row exchange satisfy <span class="math inline">\(d &gt; 0\)</span></li>
<li>there exists invertible matrix B so that A=B^TB</li>
</ol>
<p>For the last condition, we can use the <em>Cholesky Decomposition</em> to find the matrix B. It decompose a Hermitian positive definite matrix into the product of a lower triangular matrix and its conjugate transpose <span class="math inline">\(LL^H\)</span>:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a = Mat.of_array [|4.;12.;-16.;12.;37.;-43.;-16.;-43.;98.|] 3 3;;
&gt;val a : Mat.mat =
&gt;
&gt;    C0  C1  C2
&gt;R0   4  12 -16
&gt;R1  12  37 -43
&gt;R2 -16 -43  98
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let l = Linalg.D.chol a;;
&gt;val l : Owl_dense_matrix_d.mat =
&gt;
&gt;   C0 C1 C2
&gt;R0  2  6 -8
&gt;R1  0  1  5
&gt;R2  0  0  3
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.(dot (transpose l) l);;
&gt;- : Mat.mat =
&gt;
&gt;    C0  C1  C2
&gt;R0   4  12 -16
&gt;R1  12  37 -43
&gt;R2 -16 -43  98
</code></pre>
</div>
<p>If in <span class="math inline">\(Ax=b\)</span> we know that <span class="math inline">\(A\)</span> is hermitian and positive definite, then we can instead solve <span class="math inline">\(L^Lx=b\)</span>. As we have seen previously, solving linear system that expressed with triangular matrices is easy. The Cholesky decomposition is more efficient than the LU decomposition.</p>
<p>In the Linear Algebra module, we use <code>is_posdef</code> function to do this test. If you look at the code in Owl, it is implemented by checking if the Cholesky decomposition can be performed on the input matrix.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let is_pos =
  let a = Mat.of_array [|4.;12.;-16.;12.;37.;-43.;-16.;-43.;98.|] 3 3 in
  Linalg.D.is_posdef a
;;
&gt;val is_pos : bool = true
</code></pre>
</div>
<p>The definition of <em>semi-positive definite</em> is similar, only that it allows the “equals to zero” part. For example, <span class="math inline">\(x^TAx \leq 0\)</span> for all non-zero real vectors x.</p>
<p>The pattern <span class="math inline">\(Ax=\lambda~Mx\)</span> exists in many engineering analysis problems. If <span class="math inline">\(A\)</span> and <span class="math inline">\(M\)</span> are positive definite, this pattern is parallel to the <span class="math inline">\(Ax=\lambda~x\)</span> where <span class="math inline">\(\lambda &gt; 0\)</span>. For example, a linear system <span class="math inline">\(y'=Ax\)</span> where <span class="math inline">\(x = [x_1, x_2, \ldots, x_n]\)</span> and <span class="math inline">\(y' = [\frac{dx_1}{dt}, \frac{dx_2}{dt}, \ldots, \frac{dx_n}{dt}]\)</span>. We will see such an example in the Ordinary Differential Equation chapter. In a linearised differential equations the matrix A is the Jacobian matrix. The eigenvalues decides if the system is stable or not. A theorem declares that this system is stable if and only if there exists positive and definite matrix <span class="math inline">\(V\)</span> so that <span class="math inline">\(-(VA+A^TV)\)</span> is semi-positive definite.</p>
</section>
<section class="level3" id="singular-value-decomposition">
<h3>Singular Value Decomposition</h3>
<p>The singular value decomposition (SVD) is among the most important matrix factorizations of the computational era. The SVD provides a numerically stable matrix decomposition that can be used for a variety of purposes and is guaranteed to exist.</p>
<p>Any m by n matrix can be factorised in the form:</p>
<p><span id="eq:linear-algebra:svg"><span class="math display">\[A=U\Sigma~V^T\qquad(6)\]</span></span></p>
<p>Here <span class="math inline">\(U\)</span> is is a <span class="math inline">\(m\times~m\)</span> matrix. Its columns are the eigenvectors of <span class="math inline">\(AA^T\)</span>. Similarly, <span class="math inline">\(V\)</span> is a <span class="math inline">\(n\times~n\)</span> matrix, and the columns of V are eigenvectors of <span class="math inline">\(A^TA\)</span>. The <span class="math inline">\(r\)</span> (rank of A) singular value on the diagonal of the <span class="math inline">\(m\times~n\)</span> diagonal matrix <span class="math inline">\(\Sigma\)</span> are the square roots of the nonzero eigenvalues of both <span class="math inline">\(AA^T\)</span> and <span class="math inline">\(A^TA\)</span>. It’s close related with eigenvector factorisation of a positive definite matrix. For a positive definite matrix, the SVD factorisation is the same as the <span class="math inline">\(Q\Lambda~Q^T\)</span>.</p>
<p>The SVD has a profound intuition. A matrix <span class="math inline">\(A\)</span> represents a linear transformation. SVD states that, any such linear transformation, can be decomposed into three simple transformation: a rotation (<span class="math inline">\(V\)</span>), a scaling transformation (<span class="math inline">\(\Sigma\)</span>), and another rotation (<span class="math inline">\(U\)</span>). These three transformations are much easier to analyse than a random transformation <span class="math inline">\(A\)</span>. After applying <span class="math inline">\(A\)</span> to a domain, the columns of <span class="math inline">\(V\)</span> is and a set of orthonormal basis in the original domain, and columns of <span class="math inline">\(U\)</span> is the new set of orthonormal basis of the domain that is transferred after applying <span class="math inline">\(A\)</span>. The <span class="math inline">\(\Sigma\)</span> diagonal matrix contains the scaling factors on different dimensions, and a singular value in <span class="math inline">\(\Sigma\)</span> thus represents the <em>significance</em> of that certain dimension in the linear space. A small singular value indicates that the information contained in a matrix is somehow redundant and can be compressed/removed without affecting the information carried in this matrix. This is why SVD can be used for <em>Principal Component Analysis</em> (PCA), as we will show in the NLP chapter later in this book.</p>
<p>We can use the <code>svd</code> function to perform this factorisation. Let’s use the positive definite matrix as an example:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let a = Mat.of_array [|4.;12.;-16.;12.;37.;-43.;-16.;-43.;98.|] 3 3;;
&gt;val a : Mat.mat =
&gt;
&gt;    C0  C1  C2
&gt;R0   4  12 -16
&gt;R1  12  37 -43
&gt;R2 -16 -43  98
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let u, s, vt = Linalg.D.svd ~thin:false a;;
&gt;val u : Owl_dense_matrix_d.mat =
&gt;
&gt;          C0        C1        C2
&gt;R0 -0.163007 -0.212727  0.963419
&gt;R1 -0.457324 -0.848952  -0.26483
&gt;R2  0.874233 -0.483764 0.0410998
&gt;
&gt;val s : Owl_dense_matrix_d.mat =
&gt;
&gt;        C0     C1       C2
&gt;R0 123.477 15.504 0.018805
&gt;
&gt;val vt : Owl_dense_matrix_d.mat =
&gt;
&gt;          C0        C1        C2
&gt;R0 -0.163007 -0.457324  0.874233
&gt;R1 -0.212727 -0.848952 -0.483764
&gt;R2  0.963419  -0.26483 0.0410998
</code></pre>
</div>
<p>Note that the diagonal matrix <code>s</code> is represented as a vector. We can extend it with</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let s = Mat.diagm s;;
&gt;val s : Mat.mat =
&gt;
&gt;        C0     C1       C2
&gt;R0 123.477      0        0
&gt;R1       0 15.504        0
&gt;R2       0      0 0.018805
</code></pre>
</div>
<p>However, it is only possible when we know that the original diagonal matrix is square, otherwise the vector contains the <span class="math inline">\(min(m, n)\)</span> diagonal elements.</p>
<p>Also, we can find to the eigenvectors of <span class="math inline">\(AA^T\)</span> to verify that it equals to the eigenvector factorisation.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Linalg.D.eig Mat.(dot a (transpose a));;
&gt;- : Owl_dense_matrix_z.mat * Owl_dense_matrix_z.mat =
&gt;(
&gt;                C0              C1             C2
&gt;R0  (0.163007, 0i)  (0.963419, 0i) (0.212727, 0i)
&gt;R1  (0.457324, 0i)  (-0.26483, 0i) (0.848952, 0i)
&gt;R2 (-0.874233, 0i) (0.0410998, 0i) (0.483764, 0i)
&gt;,
&gt;
&gt;              C0                C1            C2
&gt;R0 (15246.6, 0i) (0.000353627, 0i) (240.373, 0i)
&gt;)
</code></pre>
</div>
<p>In this example we ues the <code>thin</code> parameter. By default, the <code>svd</code> function performs a reduced SVD, where <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(m\times~m\)</span> matrix and <span class="math inline">\(V^T\)</span> is a m by n matrix.</p>
<p>Besides, <code>svd</code>, we also provide <code>svdvals</code> that only returns the singular values, i.e.&nbsp;the vector of diagonal elements. The function <code>gsvd</code> performs a generalised SVD. <code>gsvd x y -&gt; (u, v, q, d1, d2, r)</code> computes the generalised SVD of a pair of general rectangular matrices <code>x</code> and <code>y</code>. <code>d1</code> and <code>d2</code> contain the generalised singular value pairs of <code>x</code> and <code>y</code>. The shape of <code>x</code> is <code>m x n</code> and the shape of <code>y</code> is <code>p x n</code>. Here is an example:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let x = Mat.uniform 5 5;;
&gt;val x : Mat.mat =
&gt;
&gt;         C0       C1       C2       C3        C4
&gt;R0 0.548998 0.623231  0.95821 0.440292  0.551542
&gt;R1 0.406659 0.631188 0.434482 0.519169 0.0841121
&gt;R2 0.439047 0.459974 0.767078 0.148038  0.445326
&gt;R3 0.307424 0.129056 0.998469 0.163971  0.718515
&gt;R4 0.474817 0.176199 0.316661 0.476701  0.138534
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let y = Mat.uniform 2 5;;
&gt;val y : Mat.mat =
&gt;
&gt;         C0       C1       C2       C3         C4
&gt;R0 0.523882 0.150938 0.718397   0.1573 0.00542669
&gt;R1 0.714052 0.874704 0.436799 0.198898   0.406196
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let u, v, q, d1, d2, r = Linalg.D.gsvd x y;;
&gt;val u : Owl_dense_matrix_d.mat =
&gt;
&gt;          C0        C1        C2        C3        C4
&gt;R0 -0.385416 -0.294725 -0.398047 0.0383079 -0.777614
&gt;R1   0.18222 -0.404037 -0.754063 -0.206208  0.438653
&gt;R2 -0.380469 0.0913876 -0.199462  0.847599  0.297795
&gt;R3 -0.807427 -0.147819  0.194202 -0.418909  0.336172
&gt;R4  0.146816 -0.848345  0.442095  0.249201 0.0347409
&gt;
&gt;val v : Owl_dense_matrix_d.mat =
&gt;
&gt;         C0        C1
&gt;R0 0.558969  0.829189
&gt;R1 0.829189 -0.558969
&gt;
&gt;val q : Owl_dense_matrix_d.mat =
&gt;
&gt;          C0        C1        C2        C3        C4
&gt;R0 -0.436432 -0.169817  0.642272 -0.603428 0.0636394
&gt;R1 -0.124923  0.407939 -0.376937 -0.494889 -0.656494
&gt;R2  0.400859  0.207482 -0.268507 -0.567199  0.634391
&gt;R3 -0.283012 -0.758558 -0.559553 -0.173745 0.0347457
&gt;R4  0.743733 -0.431612  0.245375 -0.197629  -0.40163
&gt;
&gt;val d1 : Owl_dense_matrix_d.mat =
&gt;
&gt;   C0 C1 C2       C3        C4
&gt;R0  1  0  0        0         0
&gt;R1  0  1  0        0         0
&gt;R2  0  0  1        0         0
&gt;R3  0  0  0 0.319964         0
&gt;R4  0  0  0        0 0.0583879
&gt;
&gt;val d2 : Owl_dense_matrix_d.mat =
&gt;
&gt;   C0 C1 C2      C3       C4
&gt;R0  0  0  0 0.94743        0
&gt;R1  0  0  0       0 0.998294
&gt;
&gt;val r : Owl_dense_matrix_d.mat =
&gt;
&gt;         C0       C1        C2       C3         C4
&gt;R0 -0.91393 0.196148 0.0738038  1.45659  -0.268024
&gt;R1        0 0.463548  0.286501  1.38499 -0.0595374
&gt;R2        0        0  0.346057 0.954629   0.167467
&gt;R3        0        0         0 -1.56104  -0.124984
&gt;R4        0        0         0        0   0.555067
</code></pre>
</div>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Mat.(u *@ d1 *@ r *@ transpose q =~ x);;
&gt;- : bool = true
Mat.(v *@ d2 *@ r *@ transpose q =~ y);;
&gt;- : bool = true
</code></pre>
</div>
<p>The SVD is not only important linear algebra concept, but also has a wide and growing applications. For example, the <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose pseudo-inverse</a> that works for non-invertible matrix can be implemented efficiently using SVD (we provide <code>pinv</code> function in the linear algebra module for the pseudo inverse). It can also be used for information compression such as in image processing. As we have said, in the Natural Language Processing chapter we will see how SVD plays a crucial role in the language processing field to perform principal component analysis.</p>
</section>
</section>
<section class="level2" id="internal-cblas-and-lapacke">
<h2>Internal: CBLAS and LAPACKE</h2>
<p>This section is for those of you who are eager for more low level information. The BLAS (Basic Linear Algebara Subprogramms) is a specification that describes a set of low-level routines for common linear algebra operation. The LAPACKE contains more linear algebra routines, such as solving linear systems and matrix factorisations, etc. Efficient implementation of these function has been practices for a long time in many softwares. Interfacing to them can provide easy access to high performance routines.</p>
<section class="level3" id="low-level-interface-to-cblas-lapacke">
<h3>Low-level Interface to CBLAS &amp; LAPACKE</h3>
<p>Owl has implemented the full interface to CBLAS and LAPACKE. Comparing to Julia which chooses to interface to BLAS/LAPACK, you might notice the extra <code>C</code> in <code>CBLAS</code> and <code>E</code> in <code>LAPACKE</code> because they are the corresponding C-interface of Fortran implementations. It is often believed that C-interface may introduce some extra overhead. However, it turns out that we cannot really notice any difference at all in practice when dealing with medium or large problems.</p>
<ul>
<li><p><a href="https://github.com/owlbarn/owl/blob/master/src/owl/cblas/owl_cblas.mli">Owl_cblas module</a> provides the raw interface to CBLAS functions, from level-1 to level-3. The interfaced functions have the same names as those in CBLAS.</p></li>
<li><p><a href="https://github.com/owlbarn/owl/blob/master/src/owl/lapacke/owl_lapacke_generated.mli">Owl_lapacke_generated module</a> provides the raw interface to LAPACKE functions (over 1,000) which also have the same names defined in <a href="https://github.com/owlbarn/owl/blob/master/src/owl/lapacke/lapacke.h">lapacke.h</a>.</p></li>
<li><p><a href="https://github.com/owlbarn/owl/blob/master/src/owl/lapacke/owl_lapacke.ml">Owl_lapacke module</a> is a very thin layer of interface between <a href="https://github.com/owlbarn/owl/blob/master/src/owl/lapacke/owl_lapacke_generated.mli">Owl_lapacke_generated module</a> and <a href="https://github.com/owlbarn/owl/blob/master/src/owl/linalg/owl_linalg_generic.mli">Linalg module</a>. The purpose is to provide a unified function to make generic functions over different number types.</p></li>
</ul>
<p>The functions in <a href="https://github.com/owlbarn/owl/blob/master/src/owl/cblas/owl_cblas.mli">Owl_cblas</a> and <a href="https://github.com/owlbarn/owl/blob/master/src/owl/lapacke/owl_lapacke_generated.mli">Owl_lapacke_generated</a> are very low-level, e.g., you need to deal with calculating parameters, allocating workspace, post-processing results, and many other tedious details. You do not really want to use them directly unless you have enough background in numerical analysis and chase after the performance. So for example, the LU factorisation is performed using the <code>sgetrf</code> or <code>dgetrf</code> function in the <code>Owl_lapacke_generated</code> module, the signature of which look like this:</p>
<div class="highlight">
<pre><code class="language-clike">val sgetrf:  layout:int -&gt; m:int -&gt; n:int -&gt; a:float ptr -&gt; lda:int -&gt; ipiv:int32 ptr -&gt; int</code></pre>
</div>
<p>Instead of worrying about all these parameters, the <code>getrf</code> function in the <code>Owl_lapacke</code> module provides interface that are more straightforward:</p>
<div class="highlight">
<pre><code class="language-clike">val getrf : a:('a, 'b) t -&gt; ('a, 'b) t * (int32, int32_elt) t</code></pre>
</div>
<p>These low-level functions provides more general access for users. If this still looks a bit unfamiliar to your, in the <code>Linalg</code> module we have:</p>
<div class="highlight">
<pre><code class="language-clike">val lu : ('a, 'b) t -&gt; ('a, 'b) t * ('a, 'b) t * (int32, int32_elt) t</code></pre>
</div>
<p>Here the function <code>lu x -&gt; (l, u, ipiv)</code> calculates LU decomposition of input matrix <code>x</code>, and returns the L, U matrix together with the pivoting index. In practice, you should always use <a href="https://github.com/owlbarn/owl/blob/master/src/owl/linalg/owl_linalg_generic.mli">Linalg</a> module which gives you a high-level wrapper for frequently used functions.</p>
<p>Besides these function, the linear algebra module also provides some helper functions. For example, the <code>peakflops ~n ()</code> function returns the peak number of float point operations using <code>Owl_cblas_basic.dgemm</code> function. The default matrix size is <code>2000 x 2000</code>, but the user can change this by setting <code>n</code> arguments.</p>
</section>
</section>
<section class="level2" id="sparse-matrices">
<h2>Sparse Matrices</h2>
<p>What we have mentioned so far are dense matrix. But when the elements are sparsely distributed in the matrix, such as the identity matrix, the <em>sparse</em> structure might be more efficient. The sparse matrix is proivded in the <code>Sparse.Matrix</code> module, and also support the four types of number in the <code>S</code>, <code>D</code>, <code>C</code>, and <code>Z</code> submodules.</p>
<p>(Perhaps these contents are better to discuss in Ndarray module.)</p>
<p>Very brief. Focusing on introducing the data structure (CSC, CSR, etc), no the method. Mention the <a href="https://github.com/owlbarn/owl_suitesparse">owl_suitesparse</a> TODO: Introduce the sparse data structure in owl, and introduce CSR, CSC, tuples, and other formats.</p>
</section>
<section class="level2" id="summary">
<h2>Summary</h2>
</section>
<section class="level2 unnumbered" id="references">
<h2 class="unnumbered">References</h2>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-strang2006linear">
<p>Strang, Gilbert. 2006. <em>Linear Algebra and Its Applications</em>. Belmont, CA: Thomson, Brooks/Cole. <a href="http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676">http://www.amazon.com/Linear-Algebra-Its-Applications-Edition/dp/0030105676</a>.</p>
</div>
</div>
</section>
</section>
</article></div><a href="diffequation.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 09</small>Ordinary Differential Equations</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>