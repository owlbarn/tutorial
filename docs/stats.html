<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><meta content="OCaml Scientific and Engineering Computing - Tutorial Book" name="description"><meta content="OCaml, Data Science, Data Analytics, Analytics, Functional Programming, Machine Learning, Deep Neural Network, Scientific Computing, Numerical Algorithm, Tutorial, Linear Algebra, Matrix" name="keywords"><meta content="Liang Wang" name="author"><title>Statistical Functions - Owl Online Tutorials</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script><script data-ad-client="ca-pub-1868946892712371" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-123353217-1"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-123353217-1');</script></head><body><div class="title-bar"><div class="title"><h1>Owl Online Tutorials</h1><h5></h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="https://ocaml.xyz/owl/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="statistical-functions">
<h1>Statistical Functions</h1>
<p>Statistics is an indispensable tool for data analysis, it helps us to gain the insights from data. The statistical functions in Owl can be categorised into three groups: descriptive statistics, distributions, and hypothesis tests.</p>
<section class="level2" id="random-variables">
<h2>Random Variables</h2>
<p>We start from assigning probabilities to <em>events</em>. A event may comprise of finite or infinite number of possible outcomes. All possible output make up the <em>sample space</em>. To better capture this assigning processes, we need the idea of <em>Random Variables</em>.</p>
<p>A random variable is a function that associate sample output of events with some numbers of interests. Imagine the classic tossing coin game, we toss the coin four times, and the result is “head”, “head”, “tail”, “head”. We are interested in the number of “head” in this outcome. So we make a Random Variable “X” to denote this number, and <code>X(["head", "head", "tail", "head"]) = 3</code>. You can see that using random variables can greatly reduce the event sample space.</p>
<p>Depending on the number of values it can be, a random variable can be broadly categorised into <em>Discrete</em> Random Variable (with finite number of possible output), and <em>Continuous</em> Random Variable (with infinite number of possible output).</p>
<section class="level3" id="discrete-random-variables">
<h3>Discrete Random Variables</h3>
<p>Back to the coin tossing example. Suppose that the coin is specially minted so that the probability of tossing head is <span class="math inline">\(p\)</span>. In this scenario, we toss for three times. Use the number of heads as a random variable <span class="math inline">\(X\)</span>, and it contains four possible outcomes: 0, 1, 2, or 3.</p>
<p>We can calculate the possibility of each output result. Since each toss is a individual trial, the possibility of three heads <code>P(X=2)</code> is <span class="math inline">\(p^3\)</span>. Two heads includes three cases: HHT, HTH, THH, each has a probability of <span class="math inline">\(p^2(1-p)\)</span>, and together <span class="math inline">\(P(X=2) = 3p^2(1-p)\)</span>. Similarly <span class="math inline">\(P(X=1)=3p(1-p)^2\)</span>, and <span class="math inline">\(P(X=0)=(1-p)^3\)</span>.</p>
<p>Formally, consider a series of <span class="math inline">\(n\)</span> independent trails, each trail containing two possible results, and the result of interest happens at a possibility of <span class="math inline">\(p\)</span>, then the possibility distribution of random variable <span class="math inline">\(X\)</span> is (<span class="math inline">\(X\)</span> being the number of result of interests):</p>
<p><span class="math display">\[P(X=k) = {N\choose k} p^k(1-p)^{n-k}.\]</span> {#eq:stats:binomial_pdf}</p>
<p>This type of distribution is called the <em>Binomial Probability Distribution</em>. We can stimulate this process of tossing coins with the <code>Stats.binomial_rvs</code> function. Suppose the probability of tossing head is 0.4, and for 10 times.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let _ =
  let toss = Array.make 10 0 in
  Array.map (fun _ -&gt; Stats.binomial_rvs 0.3 1) toss
&gt;- : int array = [|0; 0; 0; 0; 0; 0; 0; 0; 1; 0|]
</code></pre>
</div>
<p>The equation <span data-cites="eq:stats:binomial_pdf" class="citation">[@eq:stats:binomial_pdf]</span> is called the <em>&amp;probability density function</em> (PDF) of this binomial distribution. Formally the PDF of random variable X is denoted with <span class="math inline">\(p_X(k)\)</span> and is defined as:</p>
<p><span class="math display">\[p_X(k)=P({s \in S | X(s) = k}),\]</span></p>
<p>where <span class="math inline">\(S\)</span> is the sample space. This can also be expressed with the code:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let x = [|0; 1; 2; 3|]
&gt;val x : int array = [|0; 1; 2; 3|]
let p = Array.map (Stats.binomial_pdf ~p:0.3 ~n:3) x
&gt;val p : float array =
&gt;  [|0.342999999999999916; 0.440999999999999837; 0.188999999999999918;
&gt;    0.0269999999999999823|]
Array.fold_left (+.) 0. p
&gt;- : float = 0.999999999999999778
</code></pre>
</div>
<p>Aside from the PDF, another related and frequently used idea is to see the probability of random variable <span class="math inline">\(X\)</span> being within a certain range: <span class="math inline">\(P(a \leq X \leq b)\)</span>. It can be rewritten as <span class="math inline">\(P(X \leq b) - P(X \leq a - 1)\)</span>. Here the term <span class="math inline">\(P(X \leq t)\)</span> is called the <em>Cumulative Distribution Function</em> of random variable <span class="math inline">\(X\)</span>. For the binomial distribution, it CDF is:</p>
<p><span class="math display">\[p(X\leq~k)=\sum_{i=0}^k{N\choose i} p^k(1-p)^{n-i}.\]</span></p>
<p>We can calculate the CDF in the 3-tossing problem with code again.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let x = [|0; 1; 2; 3|]
&gt;val x : int array = [|0; 1; 2; 3|]
let p = Array.map (Stats.binomial_cdf ~p:0.3 ~n:3) x
&gt;val p : float array = [|0.342999999999999972; 0.784; 0.973; 1.|]
</code></pre>
</div>
</section>
<section class="level3" id="continuous-random-variables">
<h3>Continuous Random Variables</h3>
<p>Unlike discrete random variable, a continuous random variable has infinite number of possible outcomes. For example, in uniform distribution, we can pick a random real number between 0 and 1. Apparently there can be infinite number of outputs.</p>
<p>One of the most widely used continuous distribution is no doubt the <em>Gaussian distribution</em>. It’s probability function is a continuous one: <span class="math display">\[p(x) = \frac{1}{\sqrt{2\pi~\delta}}e^{-\frac{1}{2}\left(\frac{t - \mu}{\sigma}\right)^2}\]</span> {#eq:stats:gaussian_pdf}</p>
<p>Here the <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are parameters. Depending on them, the <span class="math inline">\(p(x)\)</span> can take different shapes. Let’s look at an example.</p>
<p>We generate two data sets in this example, and both contain 999 points drawn from different Gaussian distribution <span class="math inline">\(\mathcal{N} (\mu, \sigma^{2})\)</span>. For the first one, the configuration is <span class="math inline">\((\mu = 1, \sigma = 1)\)</span>; whilst for the second one, the configuration is <span class="math inline">\((\mu = 12, \sigma = 3)\)</span>.</p>
<div class="highlight">
<pre><code class="language-ocaml">let noise sigma = Stats.gaussian_rvs ~mu:0. ~sigma;;
let x = Array.init 999 (fun _ -&gt; Stats.gaussian_rvs ~mu:1. ~sigma:1.);;
let y = Array.init 999 (fun _ -&gt; Stats.gaussian_rvs ~mu:12. ~sigma:3.);;</code></pre>
</div>
<p>We can visualise the data sets using histogram plot as below. When calling <code>histogram</code>, we also specify 30 bins explicitly. You can also fine tune the figure using <code>spec</code> named parameter to specify the colour, x range, y range, etc. We will discuss in details on how to use Owl to plot in a separate chapter.</p>
<div class="highlight">
<pre><code class="language-ocaml">(* convert arrays to matrices *)

let x' = Mat.of_array x 1 999;;
let y' = Mat.of_array y 1 999;;

(* plot the figures *)

let h = Plot.create ~m:1 ~n:2 "plot_02.png" in

Plot.subplot h 0 0;
Plot.set_ylabel h "frequency";
Plot.histogram ~bin:30 ~h x';
Plot.histogram ~bin:30 ~h y';

Plot.subplot h 0 1;
Plot.set_ylabel h "PDF p(x)";
Plot.plot_fun ~h (fun x -&gt; Stats.gaussian_pdf ~mu:1. ~sigma:1. x) (-2.) 6.;
Plot.plot_fun ~h (fun x -&gt; Stats.gaussian_pdf ~mu:12. ~sigma:3. x) 0. 25.;

Plot.output h;;</code></pre>
</div>
<p>In subplot 1, we can see the second data set has much wider spread. In subplot 2, we also plot corresponding the probability density functions of the two data sets.</p>
<figure>
<img style="width:90.0%" id="fig:stats:plot_02" alt="Probability density functions of two data sets" title="plot 02" src="images/stats/plot_02.png"><figcaption>Probability density functions of two data sets</figcaption>
</figure>
<p>The CDF of Gaussian can be calculated with infinite summation, i.e.&nbsp;integration:</p>
<p><span class="math display">\[p(x\leq~k)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^k~e^{-t^2/2}dt.\]</span></p>
<p>We can observe this function with <code>gaussian_cdf</code>.</p>
<div class="highlight">
<pre><code class="language-ocaml">let h = Plot.create "plot_gaussian_cdf.png" in
Plot.set_ylabel h "CDF";
Plot.plot_fun ~h ~spec:[ RGB (66,133,244); LineStyle 1; LineWidth 2.; Marker "*" ] (fun x -&gt; Stats.gaussian_cdf ~mu:1. ~sigma:1. x) (-2.) 6.;
Plot.plot_fun ~h ~spec:[ RGB (219,68,55);  LineStyle 2; LineWidth 2.; Marker "+" ] (fun x -&gt; Stats.gaussian_cdf ~mu:12. ~sigma:3. x) 0. 25.;
Plot.(legend_on h ~position:SouthEast [|"mu=1,sigma=1"; "mu=12, sigma=3"|]);
Plot.output h</code></pre>
</div>
<figure>
<img style="width:70.0%" id="fig:stats:plot_gaussian_cdf" alt="Cumulated density functions of two data sets" title="plot gaussian cdf" src="images/stats/plot_gaussian_cdf.png"><figcaption>Cumulated density functions of two data sets</figcaption>
</figure>
</section>
<section class="level3" id="descriptive-statistics">
<h3>Descriptive Statistics</h3>
<p>A random variables describes one individual event. A whole collection of individuals that of certain interests becomes a <em>population</em>. A population can be characterised with multiple descriptive statistics. Two of the most frequently used of them are <em>mean</em> and <em>variance</em>. The mean of a population <span class="math inline">\(X\)</span> with <span class="math inline">\(n\)</span> elements is defined as:</p>
<p><span class="math display">\[E(X) = \frac{1}{n}\sum_{i}x_i,\]</span> {#eq:stats:mean} where <span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span>-th element in population. And the definition of variance is similar:</p>
<p><span class="math display">\[Var(X) = \frac{1}{n}\sum_{i}(x_i - E(X))^2.\]</span> {#eq:stats:variance}</p>
<p>A similar and commonly used idea is <em>standard deviation</em>, which is the square root of variance. The meaning of both the mean (or <em>expected value</em>) and the variance are plain to see, the first being a representative central value of a population, and the second being how the values spread around the central expectation.</p>
<p>These definitions are for discrete random variables, but they can easily be extended to the continuous cases. To make it more general, we define the <em>n-th moment</em> of a real variable about a value X as:</p>
<p><span class="math display">\[M_n(X) = \int_x~(x_i - c)^2~f(x_i)dx,\]</span> {#eq:stats:moment}</p>
<p>where <span class="math inline">\(f(x)\)</span> is the the continuous function of the variable <span class="math inline">\(X\)</span>, and <span class="math inline">\(c\)</span> is certain constant. You can see that the mean value is actually the first order moment, and variance is the second order.<br>
The third order moment is called <em>skewness</em>, indicating the asymmetry of the probability distribution of a real random variable. The fourth order moment is called <em>kurtosis</em>, and it shows how long a “tail” the probability distribution has.</p>
<p>Let’s look at one simple example. We first draw one hundred random numbers which are uniformly distributed between 0 and 10. Here we use <code>Stats.uniform_rvs</code> function to generate numbers following uniform distribution.</p>
<div class="highlight">
<pre><code class="language-ocaml">let data = Array.init 100 (fun _ -&gt; Stats.uniform_rvs 0. 10.);;</code></pre>
</div>
<p>Then We use <code>mean</code> function calculate sample average. As can be expected, it is around 5. We can also calculate other higher moments easily with corresponding functions. We can do a very rough and quick interpretation about these results. It has a widely spread distribution (about 3 to the left and right), and the distribution is not skew, according to a very small skewness number. Finally, a small kurtosis shows that the distribution does not have an obvious tail.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Stats.mean data
&gt;- : float = 5.18160409659184573
Stats.std data
&gt;- : float = 2.92844832850280135
Stats.var data
&gt;- : float = 8.57580961271085229
Stats.skew data
&gt;- : float = -0.109699186612116223
Stats.kurtosis data
&gt;- : float = 1.75165078829330856
</code></pre>
</div>
<p>The following code calculates different central moments of the distribution. A central moment is a moment of a probability distribution of a random variable about the random variable’s mean. The zero-th central moment is always 1, and the first is close to zero, and the second is close to the variance.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Stats.central_moment 0 data
&gt;- : float = 1.
Stats.central_moment 1 data
&gt;- : float = -3.13082892944294137e-15
Stats.central_moment 2 data
&gt;- : float = 8.49005151658374224
Stats.central_moment 3 data
&gt;- : float = -2.75496511397836663
</code></pre>
</div>
<p>Besides the moments, we also use <em>order statistics</em> frequently to understand data. Order statistics and rank statistics are among the most fundamental tools in non-parametric statistics and inference. The <span class="math inline">\(k^{th}\)</span> order statistic of a statistical sample is equal to its k-th smallest value. The example functions of</p>
<p>There are many ordered statistical functions in the <code>Stat</code> module in Owl for you to explore. Some of the most frequently used are shown as follows:</p>
<div class="highlight">
<pre><code class="language-ocaml">Stats.min;;
Stats.max;;
Stats.median;;
Stats.quantile;;
Stats.first_quartile;;
Stats.third_quartile;;
Stats.percentile;;</code></pre>
</div>
<p>The <code>min</code> and <code>max</code> is plain to use. The <code>median</code> is the middle number in a sorted list of numbers of the whole samples. It is sometimes more descriptive than the <code>mean</code> about the data, since the later is more prone to outliers.</p>
<p>A similar idea is <code>quartile</code>: there are 75% of the measurements in the sample are larger than the first quartile, and 25% are larger than the third quartile. The <code>median</code> is also the second quartile. A more general idea is the <code>percentile</code>, a measure at which that percentage of the total values are below that measure. For example, the first quartile is also the 25th percentile.</p>
</section>
</section>
<section class="level2" id="special-distribution">
<h2>Special Distribution</h2>
<p>All distributions are equal, but some are more equal than others. Certain types of special distributions are used again and again in practice and are given special names. A small number of them are listed in the table below.</p>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Distribution name</th>
<th style="text-align: left;">PDF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Gaussian distribution</td>
<td style="text-align: left;"><span class="math inline">\(\frac{1}{\sigma {\sqrt {2\pi }}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Gamma distribution</td>
<td style="text-align: left;"><span class="math inline">\(\frac{1}{\Gamma(k)\theta^k}x^{k-1}e^{-x\theta^-{1}}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Beta distribution</td>
<td style="text-align: left;"><span class="math inline">\(\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Cauchy distribution</td>
<td style="text-align: left;"><span class="math inline">\((\pi~\gamma~(1 + (\frac{x-x_0}{\gamma})^2))^{-1}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Student’s <span class="math inline">\(t\)</span>-distribution</td>
<td style="text-align: left;"><span class="math inline">\(\frac{\Gamma((v+1)/2)}{\sqrt{v\pi}\Gamma(v/2)}(1 + \frac{x^2}{v})^{-\frac{v+1}{2}}\)</span></td>
</tr>
</tbody>
</table>
<p>Here <span class="math inline">\(\Gamma(x)\)</span> is the Gamma function. These different kinds of distributions are supported in the <code>Stats</code> module in Owl. For each distribution, there is a set of related functions using the distribution name as their common prefix. For example, for the gaussian distribution, we can utilise the function below:</p>
<ul>
<li><code>gaussian_rvs</code> : random number generator.</li>
<li><code>gaussian_pdf</code> : probability density function.</li>
<li><code>gaussian_cdf</code> : cumulative distribution function.</li>
<li><code>gaussian_ppf</code> : percent point function (inverse of CDF).</li>
<li><code>gaussian_sf</code> : survival function (1 - CDF).</li>
<li><code>gaussian_isf</code> : inverse survival function (inverse of SF).</li>
<li><code>gaussian_logpdf</code> : logarithmic probability density function.</li>
<li><code>gaussian_logcdf</code> : logarithmic cumulative distribution function.</li>
<li><code>gaussian_logsf</code> : logarithmic survival function.</li>
</ul>
<p>Stats module supports many distributions. For each distribution, there is a set of related functions using the distribution name as their common prefix. As an example, the code below plots the probability density function of the Gamma distribution using <code>gamma_pdf</code>. The result is shown in <span data-cites="fig:stats:gamma_pdf" class="citation">[@fig:stats:gamma_pdf]</span>.</p>
<div class="highlight">
<pre><code class="language-ocaml">module N = Dense.Ndarray.D

let _ =
  let x = N.linspace 0. 16. 100 in

  let f1 x = Owl_stats.gamma_pdf x ~shape:1. ~scale:2. in
  let f2 x = Owl_stats.gamma_pdf x ~shape:2. ~scale:2. in
  let f3 x = Owl_stats.gamma_pdf x ~shape:5. ~scale:1. in
  let f4 x = Owl_stats.gamma_pdf x ~shape:7.5 ~scale:1. in

  let y1 = N.map f1 x in
  let y2 = N.map f2 x in
  let y3 = N.map f3 x in
  let y4 = N.map f4 x in

  let h = Plot.create "gamma_pdf.png" in
  let open Plot in
  set_xlabel h "";
  set_ylabel h "";
  set_title h "Gamma distribution probablility density functions";
  plot ~h ~spec:[ RGB (66, 133, 244); LineStyle 1; LineWidth 2. ] x y1;
  plot ~h ~spec:[ RGB (219, 68,  55); LineStyle 1; LineWidth 2. ] x y2;
  plot ~h ~spec:[ RGB (244, 180,  0); LineStyle 1; LineWidth 2. ] x y3;
  plot ~h ~spec:[ RGB (15, 157,  88); LineStyle 1; LineWidth 2. ] x y4;

  Plot.(legend_on h ~position:NorthEast [|"k=1, theta=2"; "k=2, theta=2"; "k=5, theta=1"; "k=7.5, theta=1"|]);

  output h</code></pre>
</div>
<figure>
<img style="width:60.0%" id="fig:stats:gamma_pdf" alt="Probability density functions of Gamma distribution" title="gamma_pdf" src="images/stats/gamma_pdf.png"><figcaption>Probability density functions of Gamma distribution</figcaption>
</figure>
</section>
<section class="level2" id="multiple-variables">
<h2>Multiple Variables</h2>
<p>So far we have talked about one single random variable, but a problem often involves multiple variables. For example, in a data centre, if we know the probability that the servers stop working, and the probability that the network links break, we might want to consider the probability that a data centre functions normally. The <em>joint probability</em> of two random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is expressed as <span class="math inline">\(p(X, Y)\)</span>, or <span class="math inline">\(P(X~\cap~Y)\)</span>, indicating the probability of the two events happened at the same time.</p>
<p>There is one special case where the joint probability is intuitive to compute. If the two events are <em>independent</em>, i.e.&nbsp;not related with each other, then the probability of result <span class="math inline">\(X=x\)</span> and <span class="math inline">\(Y=y\)</span> is:</p>
<p><span class="math display">\[p(xy) = p(X=x \textrm{AND } Y=y) = p(X=x) * p(Y=y) = p_X(x)p_Y(y).\]</span></p>
<p>Another related concept is the <em>conditional probability</em>. Intuitively, many events in the real world are not totally independent with each other. For example, consider the probability that a person put a raincoat, and the probability that a person put on a raincoat <em>in a rainy day</em>. The events “putting on raincoat” and “rainy day” are apparently related. Formally, the probability of event A given event B is computed as:</p>
<p><span class="math display">\[P(X | Y) = \frac{P(X~\cap~Y)}{P(Y)}.\]</span></p>
<p>There is not doubt that the most important application of conditional probabilities is the <em>Bayes’ Theorem</em>, proposed first by Thomas Bayes in 1991 <span data-cites="bayes1991essay" class="citation">[@bayes1991essay]</span>. It is expressed by an simple form as shown in <span data-cites="eq:stats:bayes" class="citation">[@eq:stats:bayes]</span>, e.g.&nbsp;it provides a way to compute the condition probability when it is not directly available.</p>
<p><span class="math display">\[P(X|Y) = \frac{P(Y|X)P(X)}{P(Y)}\]</span> {#eq:stats:bayes}</p>
<p>One powerful application of this theorem is that it provides the tool to calibrate your knowledge about something (“it has 10% percentage to happen”) based on observed evidence. For example, a novice hardly tell if a dice is normal or loaded. If I show you a dice and ask you to estimate the probability that this dice a fake one, you would say “hmm, I don’t know, perhaps 10%”. Define event <span class="math inline">\(X\)</span> to be “the dice is loaded”, and you just set a <strong>prior</strong> that <span class="math inline">\(P(X) = 0.1\)</span>. Now I begin to roll for three times, and somehow, I got three 6’s. Now I ask you again, <em>given the evidence you just observed</em>, estimate again the probability that the dice is loaded. Define <span class="math inline">\(Y\)</span> as the event “get all 6’s of all three rolling”.</p>
<p>We can easily calculate that in the normal case <span class="math inline">\(P(Y) = 1 / 6^3 \approx 0.005\)</span>, and the probability this “normal case” happens, is 90%, according to our prior knowledge.<br>
In total, <span class="math inline">\(P(Y) = P(Y|X)P(X) + P(Y|X')P(X')\)</span>, where <span class="math inline">\(P(X')\)</span> denotes the probability the dice is normal one. Besides, we can say that getting all 6’s if the dice is loaded <span class="math inline">\(P(Y | X)\)</span> would be pretty high, for example 0.99. Therefore, we can calculate that, now that given the observed evidence, the dice is loaded with a probability <span class="math inline">\(P(X|Y) = \frac{0.99 * 0.1}{0.99~\times~0.1 + 0.005~\times~0.9} \approx 0.96\)</span>. This is the <strong>posterior</strong> that we get after observing the evidence, which improves our previous knowledge significantly. This process can be widely applied to numerous scientific fields, where existing theory or knowledge are often put to test with new evidences.</p>
</section>
<section class="level2" id="sampling">
<h2>Sampling</h2>
<p>We have talked about using random variables to describe certain events of interests. The whole of individuals constitutes <em>population</em>. It can be characterised by statistics such as mean, standard deviation as we have shown before. However, in the real world, most population is difficult to enumerate, if not possible. For example, if we are interested to know the average weight of all sands on earth, then it surely difficult to measure them one by one. Instead, a <em>sample</em> is required to represent this population.</p>
<section class="level3" id="unbiased-estimator">
<h3>Unbiased Estimator</h3>
<p>There can be multiple ways to do the sampling. Random sampling is a common choice. A similar method is “stratified random sampling”, which first divide population into several groups, and then choose randomly within each group. For example, in designing a questionnaire, you want people from all age groups to be equally represented, and then stratified randomly sampling would be a more proper method. Of course, more sampling methods are also plausible as long as the sample is representative, which means that a member in the population is equally possible to be chosen into the sample.</p>
<p>After choosing a suitable sample, the next thing is to describe the population with the sample. The statistics such as mean and variance etc. are still very useful, but can we directly use the statistics of the sample and declare that they can also be used to represent the whole population? In fact, that depends on if the statistics is an <em>unbiased estimator</em>, i.e.&nbsp;the expected value of its value is the corresponding population parameter.</p>
<p>For example, let’s take a sample of <span class="math inline">\(n\)</span> elements, and its mean <span class="math inline">\(m\)</span> is:</p>
<p><span class="math display">\[m = \frac{1}{n}\sum_{i=1}^n~x_i,\]</span></p>
<p>where <span class="math inline">\(x_i\)</span> is an element in the sample. Denoting the population as <span class="math inline">\(\mu\)</span>, it can be further proved that: <span class="math inline">\(E(m) = \mu\)</span>. Therefore, the sample mean is an unbiased estimator of the population.</p>
<p>The same cannot of said of variance. The sample variance is:</p>
<p><span class="math display">\[v = \frac{1}{n}\sum_{i=1}^n(x_i - m)^2.\]</span></p>
<p>Assume the variance of population is <span class="math inline">\(\sigma^2\)</span>, then it can be proved that <span class="math inline">\(E(v) = \frac{n - 1}{n}\sigma^2\)</span>. Therefore, the unbiased estimator of population variance of not that of the sample <span class="math inline">\(v\)</span>, but <span class="math inline">\(\frac{n}{n-1}v\)</span>.</p>
</section>
<section class="level3" id="inferring-population-parameters">
<h3>Inferring Population Parameters</h3>
<p>In the previous section, we have shown how to get the expected value of the mean and variance of the population, given a sample from this population. But we perhaps need to know more than just the expected value. For example, can we locate an interval in which we can be quite sure the population mean lies? This section investigates this question.</p>
<p>First, we need to explain the <em>Central Limit Theorem</em>. It states that, if you have a population and take sufficiently large random samples from the population with replacement, the distribution of the sample means will be approximately normally distributed. If the sample size is sufficiently large (such as <span class="math inline">\(n \lt 20\)</span>), this theorem holds true regardless of the population distribution.</p>
<p>Specifically, suppose we repeatedly sample a subset of the same size <span class="math inline">\(n\)</span>, and we can then define random variable <span class="math inline">\(X\)</span> to represents the mean value of each sampled subset. According to the central limit theorem, it can be derived that, suppose the population has mean <span class="math inline">\(\mu\)</span> and variance of <span class="math inline">\(\sigma^2\)</span>, both unknown, then <span class="math inline">\(X\)</span> follows a normal distribution of mean value <span class="math inline">\(\mu\)</span>, and variance <span class="math inline">\(\frac{\sigma^2}{n}\)</span>.</p>
<p>Since both the mean and the variance of the population is unknown, apparently we cannot solve this case with mystery at both ends. To get a more precise estimation about population mean <span class="math inline">\(\mu\)</span>, let’s first assume that the population variance can be calculated directly with the sample variance: <span class="math inline">\(\sigma^2 = \frac{1}{n-1}\sum_{i=1}^n(x_i - m)^2\)</span>. This assumption is of good quality in practice when <span class="math inline">\(n\)</span> is sufficiently large.</p>
<p>Now that we know <span class="math inline">\(X\)</span> follows a normal distribution, we can utilise some of this nice properties. For example, we know that 95% of the probability mass lies within 1.96 standard deviations of this means. We can verify this point this simple code using the CDF function of normal distribution:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let f = Stats.gaussian_cdf ~mu:0. ~sigma:1. in
f 1.96 -. f (-1.96)
&gt;- : float = 0.950004209703559
</code></pre>
</div>
<p>Therefore, for any value <span class="math inline">\(x\)</span> in <span class="math inline">\(X\)</span>, we know that: <span class="math display">\[P(\mu - 1.96~\frac{\sigma}{\sqrt{n}} \le x \le  \mu + 1.96~\frac{\sigma}{\sqrt{n}}).\]</span></p>
<p>With a bit variation, it becomes:</p>
<p><span class="math display">\[P( x - 1.96~\frac{\sigma}{\sqrt{n}} \le \mu \le  x + 1.96~\frac{\sigma}{\sqrt{n}}).\]</span></p>
<p>That means that given the sample mean <span class="math inline">\(m\)</span>, the population mean <span class="math inline">\(\mu\)</span> lies within this range [<span class="math inline">\(m - 1.96~\frac{\sigma}{\sqrt{n}}\)</span>, <span class="math inline">\(m + 1.96~\frac{\sigma}{\sqrt{n}}\)</span>] with 95% probability. It is called its <em>confidence interval</em>. Again, the population variance <span class="math inline">\(\sigma^2\)</span> directly use that of the unbiased estimation from sample.</p>
<p>Let’s go back to the <em>1.96</em> number. We use this range because X is assumed to follow a normal distribution. The <span class="math inline">\(\frac{x - \mu}{\sigma/\sqrt{n}}\)</span> variable follows a standard normal distribution. It is called tne <em>standard Z variable</em>. We can check the standard normal distribution table to find the range that corresponds to 95% confidence. However, as we have explained, this does not hold when <span class="math inline">\(n\)</span> is small, since we actually uses <span class="math inline">\(\frac{x-\mu}{\sqrt{\frac{\sum_{i}(x - m)^2}{n(n-1)}}}\)</span> instead of the real <span class="math inline">\(z\)</span> variable. The latter one is called <em>standard t variable</em>, which follows the t-distribution with <span class="math inline">\(n-1\)</span> degree of freedom. When <span class="math inline">\(n\)</span> is a large number, the t distribution behave almost the same as that of a normal distribution. Therefore, if the <span class="math inline">\(n\)</span> is small, we need to look up the t table. For example, if <span class="math inline">\(n=17\)</span>, then the range parameter is about 2.12, which can be verified as:</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">let f x = Stats.t_cdf x ~df:16. ~loc:0. ~scale:1. in 
f 2.12 -. f (-2.12)
&gt;- : float = 0.950009071286895823
</code></pre>
</div>
<p>That’s all for the population mean. The estimation of population variance range uses <span class="math inline">\(\chi\)</span>-square distribution, but rarely used in practice. So we omitted it in this section.</p>
</section>
</section>
<section class="level2" id="hypothesis-tests">
<h2>Hypothesis Tests</h2>
<section class="level3" id="theory">
<h3>Theory</h3>
<p>While descriptive statistics solely concern properties of the observed data, statistical inference focusses on studying whether the data set is sampled from a larger population. In other words, statistical inference make propositions about a population. Hypothesis test is an important method in inferential statistical analysis.</p>
<p>Let’s think about the classic flip coin experiment. Suppose we have a basic assumption/hypothesis that most coins can give a result of head or tail with 50/50 chance. Now, if you flip a given coin 3 times and get 3 heads, can you make a claim that this coin is not a normal one? With how much probability? Another example is that, suppose you claim that one of your proposed algorithm improves the running speed of the state of art, and you have two samples about the execution time using two different algorithms, and then how can you be sure that your claim is justified. That’s where we need hypothesis tests.</p>
<p>There are two hypotheses proposed with regard to the statistical relationship between data sets.</p>
<ul>
<li>Null hypothesis <span class="math inline">\(H_0\)</span>: there is no relationship between two data sets.</li>
<li>Alternative hypothesis <span class="math inline">\(H_1\)</span>: there is statistically significant relationship between two data sets.</li>
</ul>
<p>The probability of an outcome assuming that a hypothesis is true is called its p-value. In practice the p-value is set to 5% (1% is also used frequently). For example, if we believe that a coin is a normal one, but the experiment result can only happen with a low probability of, e.g.&nbsp;0.03 given this belief, we can reject the the hypothesis (“this coin is normal”), at the 5% confidence level.</p>
<p>Note that, if we do not reject a hypothesis, that does not mean it is accepted. If we flip the coin three times and get three heads. Given the hypothesis that this coin is normal, this result happens with a probability of 12.5%, therefore we cannot reject this hypothesis.<br>
The non-rejection does not mean we are pretty sure the coin is totally not biased with much confidence. Therefore, one needs to be very careful in choosing hypothesis. <span class="math inline">\(H_0\)</span> should be something we believe is solid enough to explain the data unless the strongly challenged by observed data. Besides, it also helps to make the null hypothesis as precise as possible. A wide coverage only makes the hypothesis undeniable.</p>
</section>
<section class="level3" id="gaussian-distribution-in-hypothesis-testing">
<h3>Gaussian Distribution in Hypothesis Testing</h3>
<p>One of the most common test to make is to see if observed data come from a certain gaussian distribution. This is called a “z-test.” Now let’s see how to perform a z-test in Owl. We first generate two data sets, both are drawn from Gaussian distribution but with different parameterisation. The first one <code>data_0</code> is drawn from <span class="math inline">\(\mathcal{N}(0, 1)\)</span>, while the second one <code>data_1</code> is drawn from <span class="math inline">\(\mathcal{N}(3, 1)\)</span>.</p>
<div class="highlight">
<pre><code class="language-ocaml">let data_0 = Array.init 10 (fun _ -&gt; Stats.gaussian_rvs ~mu:0. ~sigma:1.);;
let data_1 = Array.init 10 (fun _ -&gt; Stats.gaussian_rvs ~mu:3. ~sigma:1.);;</code></pre>
</div>
<p>Our hypothesis is that the data set is drawn from Gaussian distribution <span class="math inline">\(\mathcal{N}(0, 1)\)</span>. From the way we generated the synthetic data, it is obvious that <code>data_0</code> will pass the test, but let’s see what Owl will test us using its <code>Stats.z_test</code> function.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Stats.z_test ~mu:0. ~sigma:1. data_0
&gt;- : Owl_stats.hypothesis =
&gt;{Owl.Stats.reject = false; p_value = 0.289340080583773251;
&gt; score = -1.05957041132113083}
</code></pre>
</div>
<p>The returned result is a record with the following type definition. The fields are self-explained: <code>reject</code> field tells whether the null hypothesis is rejected, along with the p value and score calculated with the given data set.</p>
<div class="highlight">
<pre><code class="language-ocaml">type hypothesis = {
  reject : bool;
  p_value : float;
  score : float;
}</code></pre>
</div>
<p>From the previous result, we can see <code>reject = false</code>, indicating null hypothesis is rejected, therefore the data set <code>data_0</code> is drawn from <span class="math inline">\(\mathcal{N}(0, 1)\)</span>. How about the second data set then?</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Stats.z_test ~mu:0. ~sigma:1. data_1
&gt;- : Owl_stats.hypothesis =
&gt;{Owl.Stats.reject = true; p_value = 5.06534675819424548e-23;
&gt; score = 9.88035435799393547}
</code></pre>
</div>
<p>As we expected, the null hypothesis is accepted with a very small p value. This indicates that <code>data_1</code> is drawn from a different distribution rather than assumed <span class="math inline">\(\mathcal{N}(0, 1)\)</span>.</p>
<p>In the previous section we have introduced the z-variable and t-variable. Besides the z-test, another frequently used test is to see if the given data follows a Student’s t-distribution under the null hypothesis. In the <code>Stats</code> module, the <code>t_test ~mu ~alpha ~side x</code> function returns a test decision of t-test, which is a parametric test of the location parameter when the population standard deviation is unknown. Here <code>mu</code> is population mean, and <code>alpha</code> is the significance level.</p>
</section>
<section class="level3" id="two-sample-inferences">
<h3>Two-Sample Inferences</h3>
<p>Another common type of test is to test if two samples comes from the same population. For example, we need to test performance of the improvement to an algorithm to see if it really works. The default null hypothesis is that, the two samples are drawn from the same population. The test strategy depends on if the sample sizes.</p>
<p>If the two samples are of the same size, then the test can be simplified by subtracting the elements in the two sets one-by-one. The test then becomes to check if the resulting set is taken from a population of mean value 0. This problem can be solved with the method introduced in the previous section by using t-test. Specifically, we provides the paired sample t-test function. The <code>t_test_paired ~alpha ~side x y</code> returns a test decision for the null hypothesis that the data in <code>x – y</code> comes from a normal distribution with mean equal to zero and unknown variance.</p>
<p>The problem get trickier when the size of these two subsets are not the same. We then need to discuss the two cases about their confidence interval. If both intervals do not overlap, apparently we can be fairly certain that the two samples come from different population, and thus reject the null hypothesis. If the intervals overlap, we need to make sure that some other conditions stand. For example, if we can assume that the variances of both population are the same, then it can be shown that the variable:</p>
<p><span class="math display">\[\frac{\bar{x} - \bar{y}}{\sqrt{\frac{\sum_{i=1}^a~(x_i - \bar{x})^2 + \sum_{i=1}^b~(y_i - \bar{y})^2}{a + b - 2}(\frac{1}{a} + \frac{1}{b})}},\]</span></p>
<p>is a standard t variable with <span class="math inline">\(a + b - 2\)</span> degree of freedom, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the length of the sample <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>This idea is implemented as the unpaired sample t-test. The function <code>t_test_unpaired ~alpha ~side ~equal_var x y</code> returns a test decision for the null hypothesis that the data in vectors <code>x</code> and <code>y</code> comes from independent random samples from normal distributions with equal means and equal but unknown variances. Here <code>equal_var</code> indicates whether two samples have the same variance. If the two variances are not the same, we need to use the <em>nonparametric tests</em> such as Welche’s t-test.</p>
</section>
<section class="level3" id="other-types-of-test">
<h3>Other Types of Test</h3>
<p>Besides what we have mentioned so far, the <code>Stats</code> module in Owl supports many other different kinds of hypothesis tests. This section will give them a brief introduction.</p>
<ul>
<li><p>Kolmogorov-Smirnov Test: <code>ks_test ~alpha x f</code> returns a test decision for the null hypothesis that the data in vector <code>x</code> comes from independent random samples of the distribution with CDF f.&nbsp;The alternative hypothesis is that the data in <code>x</code> comes from a different distribution. The result <code>(h,p,d)</code> : <code>d</code> is the Kolmogorov-Smirnov test statistic.</p></li>
<li><p>Two-sample Kolmogorov-Smirnov Test: <code>ks2_test ~alpha x y</code> returns a test decision for the null hypothesis that the data in vectors <code>x</code> and <code>y</code> come from independent random samples of the same distribution.</p></li>
<li><p>Chi-Square Variance Test <code>var_test ~alpha ~side ~variance x</code> returns a test decision for the null hypothesis that the data in <code>x</code> comes from a normal distribution with input <code>variance</code>, using the chi-square variance test. The alternative hypothesis is that <code>x</code> comes from a normal distribution with a different variance.</p></li>
<li><p>Jarque-Bera Test <code>jb_test ~alpha x</code> returns a test decision for the null hypothesis that the data <code>x</code> comes from a normal distribution with an unknown mean and variance, using the Jarque-Bera test.</p></li>
<li><p>Wald–Wolfowitz Runs Test <code>runs_test ~alpha ~v x</code> returns a test decision for the null hypothesis that the data <code>x</code> comes in random order, against the alternative that they do not, by running Wald–Wolfowitz runs test. The test is based on the number of runs of consecutive values above or below the mean of <code>x</code>. <code>~v</code> is the reference value, the default value is the median of <code>x</code>.</p></li>
<li><p>Mann-Whitney Rank Test <code>mannwhitneyu ~alpha ~side x y</code> Computes the Mann-Whitney rank test on samples x and y. If length of each sample less than 10 and no ties, then using exact test, otherwise using asymptotic normal distribution.</p></li>
</ul>
</section>
</section>
<section class="level2" id="covariance-and-correlations">
<h2>Covariance and Correlations</h2>
<p>Correlation studies how strongly two variables are related. There are different ways of calculating correlation. For the first example, let’s look at Pearson correlation.</p>
<p><code>x</code> is our explanatory variable and we draw 50 random values uniformly from an interval between 0 and 10. Both <code>y</code> and <code>z</code> are response variables with a linear relation to <code>x</code>. The only difference is that we add different level of noise to the response variables. The noise values are generated from Gaussian distribution.</p>
<div class="highlight">
<pre><code class="language-ocaml">let noise sigma = Stats.gaussian_rvs ~mu:0. ~sigma;;
let x = Array.init 50 (fun _ -&gt; Stats.uniform_rvs 0. 10.);;
let y = Array.map (fun a -&gt; 2.5 *. a +. noise 1.) x;;
let z = Array.map (fun a -&gt; 2.5 *. a +. noise 8.) x;;</code></pre>
</div>
<p>It is easier to see the relation between two variables from a figure. Herein we use Owl’s Plplot module to make two scatter plots.</p>
<div class="highlight">
<pre><code class="language-ocaml">(* convert arrays to matrices *)

let x' = Mat.of_array x 1 50;;
let y' = Mat.of_array y 1 50;;
let z' = Mat.of_array z 1 50;;

(* plot the figures *)

let h = Plot.create ~m:1 ~n:2 "plot_01.png" in

  Plot.subplot h 0 0;
  Plot.set_xlabel h "x";
  Plot.set_ylabel h "y (sigma = 1)";
  Plot.scatter ~h x' y';

  Plot.subplot h 0 1;
  Plot.set_xlabel h "x";
  Plot.set_ylabel h "z (sigma = 8)";
  Plot.scatter ~h x' z';

  Plot.output h;;</code></pre>
</div>
<p>The subfigure 1 shows the functional relation between <code>x</code> and <code>y</code> whilst the subfiture 2 shows the relation between <code>x</code> and <code>z</code>. Because we have added higher-level noise to <code>z</code>, the points in the second figure are more diffused.</p>
<figure>
<img style="width:90.0%" id="fig:stats:plot_01" alt="Functional relation between x and the other two variables." title="plot 01" src="images/stats/plot_01.png"><figcaption>Functional relation between <code>x</code> and the other two variables.</figcaption>
</figure>
<p>Intuitively, we can easily see there is stronger relation between <code>x</code> and <code>y</code> from the figures. But how about numerically? In many cases, numbers are preferred because they are easier to compare with by a computer. The following snippet calculates the Pearson correlation between <code>x</code> and <code>y</code>, as well as the correlation between <code>x</code> and <code>z</code>. As we see, the smaller correlation value indicates weaker linear relation between <code>x</code> and <code>z</code> comparing to that between <code>x</code> and <code>y</code>.</p>
<div class="highlight">
<pre data-filter-output=">" data-prompt="#" class="command-line"><code class="language-ocaml">Stats.corrcoef x y
&gt;- : float = 0.991145445979576656
Stats.corrcoef x z
&gt;- : float = 0.692163016204755288
</code></pre>
</div>
</section>
<section class="level2" id="summary">
<h2>Summary</h2>
<p>In this chapter, we briefly introduced several of the main topics in probability and statistics. Random variables and different types of distribution are two building blocks in this chapter. Based on them, we introduced the joint- and conditional probabilities when there are multiple variables. One important topic here is the Bayes Theorem. Then we went from descriptive statistics to inference statistics, and introduced sampling, including the idea of unbiased population estimator based on a given sample, and how to infer population parameters such as mean. Next, we covered the basic idea in hypothesis testing with examples. The difference between covariance and correlations is also discussed.</p>
</section>
<section class="level2" id="references">
<h2>References</h2>
</section>
</section>
</article></div><a href="ndarray.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 06</small>N-Dimensional Arrays</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>