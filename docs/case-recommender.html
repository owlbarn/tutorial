<html style="" lang="en" class="js flexbox fontface"><head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><meta content="OCaml Scientific and Engineering Computing - Tutorial Book" name="description"><meta content="OCaml, Data Science, Data Analytics, Analytics, Functional Programming, Machine Learning, Deep Neural Network, Scientific Computing, Numerical Algorithm, Tutorial, Linear Algebra, Matrix" name="keywords"><meta content="Liang Wang" name="author"><title>Case - Recommender System - OCaml Scientific Computing</title><link href="css/app.css" rel="stylesheet"><link href="css/prism.css" rel="stylesheet"><script src="js/min/modernizr-min.js"></script><script src="js/prism.js"></script><script src="https://use.typekit.net/gfj8wez.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script>try{Typekit.load();}catch(e){}</script><script data-ad-client="ca-pub-1868946892712371" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-123353217-1"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-123353217-1');</script></head><body><div class="title-bar"><div class="title"><h1>OCaml Scientific Computing</h1><h5>1<sup>st</sup> Edition (in progress)</h5><nav><a href="index.html">Home</a><a href="toc.html">Table of Contents</a><a href="faqs.html">FAQs</a><a href="install.html">Install</a><a href="https://ocaml.xyz/package/">API Docs</a></nav></div></div><div class="wrap"><div class="left-column"><a class="to-chapter" href="toc.html"><small>Back</small><h5>Table of Contents</h5></a></div><article class="main-body"><section class="level1" id="case---recommender-system">
<h1>Case - Recommender System</h1>
<section class="level2" id="introduction">
<h2>Introduction</h2>
<p>Our daily life heavily relies on recommendations, intelligent content provision aims to match a user’s profile of interests to the best candidates in a large repository of options. There are several parallel efforts in integrating intelligent content provision and recommendation in web browsing. They differentiate between each other by the main technique used to achieve the goal.</p>
<p>The initial effort relies on the <a href="https://en.wikipedia.org/wiki/Semantic_Web_Stack">semantic web stack</a>, which requires adding explicit ontology information to all web pages so that ontology-based applications (e.g., Piggy bank) can utilise ontology reasoning to interconnect content semantically. Though semantic web has a well-defined architecture, it suffers from the fact that most web pages are unstructured or semi-structured HTML files, and content providers lack of motivation to adopt this technology to their websites. Therefore, even though the relevant research still remains active in academia, the actual progress of adopting ontology-based methods in real-life applications has stalled in these years.</p>
<p>Collaborative Filtering (CF), which was first coined in 1992, is a thriving research area and also the second alternative solution. Recommenders built on top of CF exploit the similarities in users’ rankings to predict one user’s preference on a specific content. CF attracts more research interest these years due to the popularity of online shopping (e.g., Amazon, eBay, Taobao, etc.) and video services (e.g., YouTube, Vimeo, Dailymotion, etc.). However, recommender systems need user behaviour rather than content itself as explicit input to bootstrap the service, and is usually constrained within a single domain. Cross-domain recommenders have made progress lately, but the complexity and scalability need further investigation.</p>
<p>Search engines can be considered as the third alternative though a user needs explicitly extract the keywords from the page then launch another search. The ranking of the search results is based on multiple ranking signals such as link analysis on the underlying graph structure of interconnected pages such as PageRank. Such graph-based link analysis is based on the assumption that those web pages of related topics tend to link to each other, and the importance of a page often positively correlates to its degree. The indexing process is modelled as a random walk atop of the graph derived from the linked pages and needs to be pre-compiled offline.</p>
<p>The fourth alternative is to utilise information retrieval (IR) technique. In general, a text corpus is transformed to the suitable representation depending on the specific mathematical models (e.g., set-theoretic, algebraic, or probabilistic models), based on which a numeric score is calculated for ranking. Different from the previous CF and link analysis, the underlying assumption of IR is that the text (or information in a broader sense) contained in a document can very well indicate its (latent) topics. The relatedness of any two given documents can be calculated with a well-defined metric function atop of these topics. Since topics can have a direct connection to context, context awareness therefore becomes the most significant advantage in IR, which has been integrated into <a href="https://moz.com/learn/seo/google-hummingbird">Hummingbird</a>, Google’s new search algorithm.</p>
<p>In the rest of this chapter, we will introduce <strong>Kvasir</strong>, a system built on top of latent semantic analysis. Kvasir automatically looks for the similar articles when a user is browsing a web page and injects the search results in an easily accessible panel within the browser view for seamless integration. Kvasir belongs to the content-based filtering and emphasises the semantics contained in the unstructured web text. This chapter is based on the papers in <span data-cites="7462177" class="citation">(Wang et al. 2016)</span> and <span data-cites="7840682" class="citation">(Hyvönen et al. 2016)</span>, and you will find that many basic theory are already covered previously in the NLP chapter in Part I. Henceforth we will assume you are familiar with this part.</p>
</section>
<section class="level2" id="arch">
<h2>Architecture</h2>
<p>At the core, Kvasir implements an LSA-based index and search service, and its architecture can be divided into two subsystems as <code>frontend</code> and <code>backend</code>. Figure fig.&nbsp;1 illustrates the general workflow and internal design of the system. The frontend is currently implemented as a lightweight extension in Chrome browser. The browser extension only sends the page URL back to the KServer whenever a new tab/window is created. The KServer running at the backend retrieves the content of the given URL then responds with the most relevant documents in a database. The results are formatted into JSON strings. The extension presents the results in a friendly way on the page being browsed. From user perspective, a user only interacts with the frontend by checking the list of recommendations that may interest him.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:case-recommender:architecture" title="architecture" src="images/case-recommender/architecture.png"><figcaption>Figure 1: Kvasir architecture with components numbered based on their order in the workflow</figcaption>
</figure>
<p>To connect to the frontend, the backend exposes one simple <em>RESTful API</em> as below, which gives great flexibility to all possible frontend implementations. By loosely coupling with the backend, it becomes easy to mash-up new services on top of Kvasir. In the code below, Line 1 and 2 give an example request to Kvasir service. <code>type=0</code> indicates that <code>info</code> contains a URL, otherwise <code>info</code> contains a piece of text if <code>type=1</code>. Line 4-9 present an example response from the server, which contains the meta-info of a list of similar articles. Note that the frontend can refine or rearrange the results based on the meta-info (e.g., similarity or timestamp).</p>
<div class="highlight">
<pre><code class="language-json">POST https://api.kvasir/query?type=0&amp;info=url

{"results": [
  {"title": document title,
   "similarity": similarity metric,
   "page_url": link to the document,
   "timestamp": document create date}
]}</code></pre>
</div>
<p>The backend system implements indexing and searching functionality which consist of five components: <em>Crawler</em>, <em>Cleaner</em>, <em>DLSA</em>, <em>PANNS</em> and <em>KServer</em>. Three components (i.e., Cleaner, DLSA and PANNS) are wrapped into one library since all are implemented on top of Apache Spark. The library covers three phases as text cleaning, database building, and indexing. We briefly present the main tasks in each component as below.</p>
<p><strong>Crawler</strong> collects raw documents from the web and then compiles them into two data sets. One is the English Wikipedia dump, and another is compiled from over 300 news feeds of the high-quality content providers such as BBC, Guardian, Times, Yahoo News, MSNBC, and etc. tbl.&nbsp;1 summarises the basic statistics of the data sets. Multiple instances of the Crawler run in parallel on different machines. Simple fault-tolerant mechanisms like periodical backup have been implemented to improve the robustness of crawling process. In addition to the text body, the Crawler also records the timestamp, URL and title of the retrieved news as meta information, which can be further utilised to refine the search results.</p>
<div id="tbl:case-recommender:dataset">
<table>
<caption>Table 1: Two data sets are used in Kvasir evaluation</caption>
<thead>
<tr class="header">
<th style="text-align: center;">Data set</th>
<th style="text-align: left;"># of entries</th>
<th style="text-align: left;">Raw text size</th>
<th style="text-align: left;">Article length</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Wikipedia</td>
<td style="text-align: left;"><span class="math inline">\(3.9\times~10^6\)</span></td>
<td style="text-align: left;">47.0 GB</td>
<td style="text-align: left;">Avg. 782 words</td>
</tr>
<tr class="even">
<td style="text-align: center;">News</td>
<td style="text-align: left;"><span class="math inline">\(4.6\times~10^5\)</span></td>
<td style="text-align: left;">1.62 GB</td>
<td style="text-align: left;">Avg. 648 words</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Cleaner</strong> cleans the unstructured text corpus and converts the corpus into term frequency-inverse document frequency (TF-IDF) model. In the preprocessing phase, we clean the text by removing HTML tags and stop words, de-accenting, tokenisation, etc. The dictionary refers to the vocabulary of a language model, its quality directly impacts the model performance. To build the dictionary, we exclude both extremely rare and extremely common terms, and keep <span class="math inline">\(10^5\)</span> most popular ones as <code>features</code>. More precisely, a term is considered as rare if it appears in less than 20 documents, while a term is considered as common if it appears in more than 40% of documents.</p>
<p><strong>DLSA</strong> builds up an LSA-based model from the previously constructed TF-IDF model. Technically, the TF-IDF itself is already a vector space language model. The reason we seldom use TF-IDF directly is because the model contains too much noise and the dimensionality is too high to process efficiently even on a modern computer. To convert a TF-IDF to an LSA model, DLSA’s algebraic operations involve large matrix multiplications and time-consuming SVD. We initially tried to use MLib to implement DLSA. However, MLlib is unable to perform SVD on a data set of <span class="math inline">\(10^5\)</span> features with limited RAM, we have to implement our own stochastic SVD on Apache Spark using rank-revealing technique. The DLSA will be discussed in detail in later chapter.</p>
<p><strong>PANNS</strong> builds the search index to enable fast <span class="math inline">\(k\)</span>-NN search in high dimensional LSA vector spaces. Though dimensionality has been significantly reduced from TF-IDF (<span class="math inline">\(10^5\)</span> features) to LSA (<span class="math inline">\(10^3\)</span> features), <span class="math inline">\(k\)</span>-NN search in a <span class="math inline">\(10^3\)</span>-dimension space is still a great challenge especially when we try to provide responsive services. Naive linear search using one CPU takes over 6 seconds to finish in a database of 4 million entries, which is unacceptably long for any realistic services. <a href="https://github.com/ryanrhymes/panns">PANNS</a> implements a parallel RP-tree algorithm which makes a reasonable tradeoff between accuracy and efficiency. PANNS is the core component in the backend system and we will present its algorithm in detail in later chapter. PANNS is becoming a popular choice of Python-based approximate k-NN library for application developers. According to the PyPI’s statistics, PANNS has achieved over 27,000 downloads since it was first published in October 2014.</p>
<p><strong>KServer</strong> runs within a web server, processes the users requests and replies with a list of similar documents. KServer uses the index built by PANNS to perform fast search in the database. The ranking of the search results is based on the cosine similarity metric. A key performance metric for KServer is the service time. We wrapped KServer into a Docker image and deployed multiple KServer instances on different machines to achieve better performance. We also implemented a simple round-robin mechanism to balance the request loads among the multiple KServers.</p>
<p>Kvasir architecture provides a great potential and flexibility for developers to build various interesting applications on different devices, e.g., semantic search engine, intelligent Twitter bots, context-aware content provision, and etc. We provide the <a href="http://www.cs.helsinki.fi/u/lxwang/kvasir/#demo">live demo</a> videos of the seamless integration of Kvasir into web browsing at the official website. Kvasir is also available as <a href="https://kvasira.com/2019/10/03/Announcing-Kvasira-in-your-browser.html">browser extension</a> on Chrome and Firefox.</p>
</section>
<section class="level2" id="build-topic-models">
<h2>Build Topic Models</h2>
<p>As has been explained in the previous section, the crawler and cleaner performs data collection and processing to build vocabulary and TF-IDF model. We have already talked about this part in detail in the NLP chapter. DLSA and PANNS are the two core components responsible for building language models and indexing the high dimensional data sets in Kvasir. In this section, we first sketch out the key ideas in DLSA.</p>
<p>First, a recap of LSA from the NLP chapter. The vector space model belongs to algebraic language models, where each document is represented with a row vector. Each element in the vector represents the weight of a term in the dictionary calculated in a specific way. E.g., it can be simply calculated as the frequency of a term in a document, or slightly more complicated TF-IDF. The length of the vector is determined by the size of the dictionary (i.e., number of features). A text corpus containing <span class="math inline">\(m\)</span> documents and a dictionary of <span class="math inline">\(n\)</span> terms will be converted to an <span class="math inline">\(A = m \times n\)</span> row-based matrix. Informally, we say that <span class="math inline">\(A\)</span> grows taller if the number of documents (i.e., <span class="math inline">\(m\)</span>) increases, and grows fatter if we add more terms (i.e., <span class="math inline">\(n\)</span>) in the dictionary.</p>
<p>The core operation in LSA is to perform SVD. For that we need to calculate the covariance matrix <span class="math inline">\(C = A^T \times A\)</span>, which is a <span class="math inline">\(n \times n\)</span> matrix and is usually much smaller than <span class="math inline">\(A\)</span>. This operation poses as ad bottleneck in computing: the <span class="math inline">\(m\)</span> can be very large (a lot of documents) or the <span class="math inline">\(n\)</span> can be very large (a lot of features for each document). For the first, we can easily parallelise the calculation of <span class="math inline">\(C\)</span> by dividing <span class="math inline">\(A\)</span> into <span class="math inline">\(k\)</span> smaller chunks of size <span class="math inline">\([\frac{m}{k}] \times n\)</span>, so that the final result can be obtained by aggregating the partial results as <span class="math inline">\(C = \sum_{i=1}^{k} A^T_i \times A_i \label{eq:1}\)</span>.</p>
<p>However, a more serious problem is posed by the second issue. The SVD function in MLlib is only able to handle tall and thin matrices up to some hundreds of features. For most of the language models, there are often hundreds of thousands features (e.g., <span class="math inline">\(10^5\)</span> in our case). The covariance matrix <span class="math inline">\(C\)</span> becomes too big to fit into the physical memory, hence the native SVD operation in MLlib of Spark fails as the first subfigure of Figure fig.&nbsp;2 shows.</p>
<figure>
<img alt="" id="fig:case-recommender:revealing" title="plot_06" src="images/case-recommender/plot_06.png"><figcaption>Figure 2: Rank-revealing reduces dimensionality to perform in-memory SVD</figcaption>
</figure>
<p>In linear algebra, a matrix can be approximated by another matrix of lower rank while still retaining approximately properties of the matrix that are important for the problem at hand. In other words, we can use another thinner matrix <span class="math inline">\(B\)</span> to approximate the original fat <span class="math inline">\(A\)</span>. The corresponding technique is referred to as rank-revealing QR estimation. We won’t talk about this method in detail, but the basic idea is that, the columns are sparse and quite likely linearly dependant. If we can find the rank <span class="math inline">\(r\)</span> of a matrix <span class="math inline">\(A\)</span> and find suitable <span class="math inline">\(r\)</span> columns to replace the original matrix, we can then approximate it. A TF-IDF model having <span class="math inline">\(10^5\)</span> features often contains a lot of redundant information. Therefore, we can effectively thin the matrix <span class="math inline">\(A\)</span> then fit <span class="math inline">\(C\)</span> into the memory. Figure fig.&nbsp;2 illustrates the algorithmic logic in DLSA, which is essentially a distributed stochastic SVD implementation.</p>
<p>To sum up, we propose to reduce the size of TF-IDF model matrix to fit it into the memory, so that we can get a LSA model, where we knows the document-topic and topic-word probability distribution.</p>
</section>
<section class="level2" id="index-text-corpus">
<h2>Index Text Corpus</h2>
<p>With an LSA model at hand, finding the most relevant document is equivalent to finding the nearest neighbours for a given point in the derived vector space, which is often referred to as k-NN problem. The distance is usually measured with the cosine similarity of two vectors. In the NLP chapter we have seen how to use linear search in the LSA model. However, neither naive linear search nor conventional <code>k-d</code> tree is capable of performing efficient search in such high dimensional space even though the dimensionality has been significantly reduced from <span class="math inline">\(10^5\)</span> to <span class="math inline">\(10^3\)</span> by LSA.</p>
<p>The key observation is that, we need not locate the exact nearest neighbours in practice. In most cases, slight numerical error (reflected in the language context) is not noticeable at all, i.e., the returned documents still look relevant from the user’s perspective. By sacrificing some accuracy, we can obtain a significant gain in searching speed.</p>
<section class="level3" id="random-projection">
<h3>Random Projection</h3>
<p>To optimise the search, the basic idea is that, instead of searching in all the existing vectors, we can pre-cluster the vectors according to their distances, each cluster with only a small number of vectors. For an incoming query, as long as we can put this vector into a suitable cluster, we can then search for close vectors only in that cluster.</p>
<figure>
<img alt="" id="fig:case-recommender:projection" title="plot_01" src="images/case-recommender/plot_01.png"><figcaption>Figure 3: Projection on different random lines</figcaption>
</figure>
<p>fig.&nbsp;3 gives a naive example on a 2-dimension vector space. First, a random vector <span class="math inline">\(x\)</span> is drawn and all the points are projected onto <span class="math inline">\(x\)</span>. Then we divide the whole space into half at the mean value of all projections (i.e., the blue circle on <span class="math inline">\(x\)</span>) to reduce the problem size. For each new subspace, we draw another random vector for projection, and this process continues recursively until the number of points in the space reaches the predefined threshold on cluster size.</p>
<p>In the implementation, we can construct a binary tree to facilitate the search. Technically, this can be achieved by any tree-based algorithms. Given a tree built from a database, we answer a nearest neighbour query <span class="math inline">\(q\)</span> in an efficient way, by moving <span class="math inline">\(q\)</span> down the tree to its appropriate leaf cell, and then return the nearest neighbour in that cell. In Kvasir, we use the Randomised Partition tree (RP-tree) introduced in <span data-cites="dasgupta2013randomized" class="citation">(Dasgupta and Sinha 2013)</span> to do it. The general idea of RP-tree algorithm used here is clustering the points by partitioning the space into smaller subspaces recursively.</p>
<figure>
<img alt="" id="fig:case-recommender:search" title="plot_01" src="images/case-recommender/plot_02.png"><figcaption>Figure 4: Construct a binary search tree from the random projection</figcaption>
</figure>
<p>fig.&nbsp;4 illustrates how binary search can be built according to the dividing steps shown above. You can see the five nodes in the vector space is put into five clusters/leaves step by step. The information of the random vectors such as <code>x</code>, <code>y</code>, and <code>z</code> are also saved. Once we have this tree, given another query vector, we can put it into one of the clusters along the tree to find the cluster of vectors that are close to it.</p>
<p>Of course, we have already said that this efficiency is traded-off with search accuracy. One type of common misclassification is that it is possible that we can separate close vectors into different clusters. As we can see in the first subfigure of fig.&nbsp;3, though the projections of <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> seem close to each other on <span class="math inline">\(x\)</span>, <span class="math inline">\(C\)</span> is actually quite distant from <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. The reverse can also be true: two nearby points are unluckily divided into different subspaces, e.g., points <span class="math inline">\(B\)</span> and <span class="math inline">\(D\)</span> in the left panel of fig.&nbsp;3.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:case-recommender:union" title="plot_03" src="images/case-recommender/plot_03.png"><figcaption>Figure 5: Aggregate clustering result from multipel RP-trees</figcaption>
</figure>
<p>It has been shown that such misclassifications become arbitrarily rare as the iterative procedure continues by drawing more random vectors and performing corresponding splits. In the implementation, we follow this path and build multiple RP-trees. We expect that the randomness in tree construction will introduce extra variability in the neighbours that are returned by several RP-trees for a given query point. This can be taken as an advantage in order to mitigate the second kind of misclassification while searching for the nearest neighbours of a query point in the combined search set. As shown in fig.&nbsp;5, given an input query vector <code>x</code>, we find its neighbour in three different RP-trees, and the final set of neighbour candidates comes from the union of these three different sets.</p>
</section>
<section class="level3" id="optimising-vector-storage">
<h3>Optimising Vector Storage</h3>
<p>You may have noticed that, in this method, we need to store all the random vectors that are generated in the non-leaf nodes of the tree.<br>
That means storing a large number of random vectors at every node of the tree, each with a large number features. It introduces significant storage overhead. For a corpus of 4 million documents, if we use <span class="math inline">\(10^5\)</span> random vectors (i.e., a cluster size of <span class="math inline">\(\frac{4\times~10^6}{2\times~10^5} = 20\)</span> on average), and each vector is a <span class="math inline">\(10^3\)</span>-dimension real vector (32-bit float number), the induced storage overhead is about 381.5~MB for each RP-tree. Therefore, such a solution leads to a huge index of <span class="math inline">\(47.7\)</span>~GB given <span class="math inline">\(128\)</span> RP-trees are included, or <span class="math inline">\(95.4\)</span>~GB given <span class="math inline">\(256\)</span> RP-trees.</p>
<p>The huge index size not only consumes a significant amount of storage resources, but also prevents the system from scaling up after more and more documents are collected. One possible solution to reduce the index size is reusing the random vectors. Namely, we can generate a pool of random vectors once, then randomly choose one from the pool each time when one is needed. However, the immediate challenge emerges when we try to parallelise the tree building on multiple nodes, because we need to broadcast the pool of vectors onto every node, which causes significant network traffic.</p>
<figure>
<img alt="" style="width:100.0%" id="fig:case-recommender:randomseed" title="plot_04" src="images/case-recommender/plot_04.png"><figcaption>Figure 6: Use a random seed to generate on the fly</figcaption>
</figure>
<p>To address this challenge, we propose to use a pseudo random seed in building and storing search index. Instead of maintaining a pool of random vectors, we just need a random seed for each RP-tree. As shown in fig.&nbsp;6, in a leaf cluster, instead of storing all the vectors, only the indices of vectors in the original data set are stored. The computation node can build all the random vectors on the fly from the given seed according to the random seed.</p>
<p>From the model building perspective, we can easily broadcast several random seeds with negligible traffic overhead instead of a large matrix in the network, therefore we improve the computation efficiency. From the storage perspective, we only need to store one 4-byte random seed for each RP-tree. In such a way, we are able to successfully reduce the storage overhead from <span class="math inline">\(47.7\)</span>~GB to <span class="math inline">\(512\)</span>~B for a search index consisting of <span class="math inline">\(128\)</span> RP-trees (with cluster size 20), or from <span class="math inline">\(95.4\)</span>~GB to only <span class="math inline">\(1\)</span>~KB if <span class="math inline">\(256\)</span> RP-trees are used.</p>
</section>
<section class="level3" id="optimise-data-structure">
<h3>Optimise Data Structure</h3>
<p>Let’s consider a bit more about using multiple RP-trees. Regarding the design of PANNS, we have two design options in order to improve the searching accuracy. Namely, given the size of the aggregated cluster which is taken as the union of all the target clusters from every tree, we can either use less trees with larger leaf clusters, or use more trees with smaller leaf clusters. Increasing cluster size is intuitive: if we increase it to so large that includes all the vectors, then it is totally accurate.</p>
<p>On the other hand, we expect that when using more trees the probability of a query point to fall very close to a splitting hyperplane should be reduced, thus it should be less likely for its nearest neighbours to lie in a different cluster. By reducing such misclassifications, the searching accuracy is supposed to be improved. Based on our knowledge, although there are no previous theoretical results that may justify such a hypothesis in the field of nearest neighbour search algorithms, this concept could be considered as a combination strategy similar to those appeared in ensemble clustering, a very well established field of research. Similar to our case, ensemble clustering algorithms improve clustering solutions by fusing information from several data partitions.</p>
<p>To experimentally investigate this hypothesis we employ a subset of the Wikipedia database for further analysis. In what follows, the data set contains <span class="math inline">\(500,000\)</span> points and we always search for the <span class="math inline">\(50\)</span> nearest neighbours of a query point. Then we measure the searching accuracy by calculating the amount of actual nearest neighbours found.</p>
<figure>
<img alt="" style="width:60.0%" id="fig:case-recommender:exp01" title="exp01" src="images/case-recommender/exp01.png"><figcaption>Figure 7: The number of true nearest neighbours found for different number of trees</figcaption>
</figure>
<p>We query <span class="math inline">\(1,000\)</span> points in each experiment. The results presented in fig.&nbsp;7 correspond to the mean values of the aggregated nearest neighbours of the <span class="math inline">\(1,000\)</span> query points discovered by PANNS out of <span class="math inline">\(100\)</span> experiment runs. Note that <span class="math inline">\(x\)</span>-axis represents the “size of search space” which is defined by the number of unique points within the union of all the leaf clusters that the query point fall in. Therefore, given the same search space size, using more tress indicates that the leaf clusters become smaller. As we can see in fig.&nbsp;7, for a given <span class="math inline">\(x\)</span> value, the curves move upwards as we use more and more trees, indicating that the accuracy improves. As shown in the case of 50 trees, almost <span class="math inline">\(80\%\)</span> of the actual nearest neighbours are found by performing a search over the <span class="math inline">\(10\%\)</span> of the data set.</p>
<p>Our empirical results clearly show <em>the benefits of using more trees instead of using larger clusters for improving search accuracy</em>. Moreover, regarding the searching performance, since searching can be easily parallelised, using more trees will not impact the searching time.</p>
</section>
<section class="level3" id="optimise-index-algorithm">
<h3>Optimise Index Algorithm</h3>
<p><em>NOTE: refer to II.C in <span data-cites="7840682" class="citation">(Hyvönen et al. 2016)</span>: Compactness and speed with fewer vectors: a very interesting part. Make sure you understand the paper and express the core idea concisely here.</em></p>
<figure>
<img alt="" style="width:90.0%" id="fig:case-recommender:parallel" title="plot_05" src="images/case-recommender/plot_05.png"><figcaption>Figure 8: Illustration of parallelising the computation.</figcaption>
</figure>
<p>Blue dotted lines are critical boundaries. The computations in the child-branches cannot proceed without finishing the computation in the parent node. There is no critical boundary. All the projections can be done in just one matrix multiplication. Therefore, the parallelism can be maximised. (TODO: Revised this paragraph)</p>
<p>In classic RP trees, a different random vector is used at each inner node of a tree, whereas we use the same random vector for all the sibling nodes of a tree. This choice does not affect the accuracy at all because a query point is routed down each of the trees only once; hence, the query point is projected onto a random vector <span class="math inline">\(r_i\)</span> sampled from the same distribution at each level of a tree. This means that the query point is projected onto i.i.d. random vectors <span class="math inline">\(r_1, \ldots, r_l\)</span>.</p>
<p>An RP-tree has <span class="math inline">\(2^l-1\)</span> inner nodes. Therefore, if each node of a tree had a different random vector as in classic RP-trees, <span class="math inline">\(2^l-1\)</span> different random vectors would be required for one tree. However, when a single vector is used on each level, only <span class="math inline">\(l\)</span> vectors are required. This reduces the amount of memory required by the random vectors from exponential to linear with respect to the depth of the trees.</p>
<p>Having only <span class="math inline">\(l\)</span> random vectors in one tree also speeds up the index construction significantly. While some of the observed speed-up is explained by a decreased amount of the random vectors that have to be generated, mostly it is due to enabling the computation of all the projections of the tree in one matrix multiplication: the projected data set <span class="math inline">\(P\)</span> can be computed from the dataset <span class="math inline">\(X\)</span> and a random matrix <span class="math inline">\(R\)</span> as <span class="math inline">\(P = XR\)</span>. Although the total amount of computation stays the same, in practice this speeds up the index construction significantly due to the cache effects and low-level parallelisation through vectorisation.</p>
</section>
</section>
<section class="level2" id="search-articles">
<h2>Search Articles</h2>
<p>By using RP-tree we have already limit the search range from the whole text corpus to only a cluster of small number of documents (vectors), where we can do a linear searching. We have also introduced several optimisations on the RP-tree itself, including using multiple trees, using random seed to remove the storage of random vectors, improving computation efficiency etc. But we don’t stop here: can we further improve the linear searching itself? It turns out, we can.</p>
<p>To select the best candidates from a cluster of points, we need to use the coordinates in the original space to calculate their relative distance to the query point. This however, first increases the storage overhead since we need to keep the original high dimensional data set which is usually huge; second increases the query overhead since we need to access such data set. The performance becomes more severely degraded if the original data set is too big to load into the physical memory. Moreover, computing the distance between two points in the high dimensional space per se is very time-consuming.</p>
<p>Nonetheless, we will show that it is possible to completely get rid of the original data set while keeping the accuracy at a satisfying level. The core idea of is simple. Let’s look at the second subfigure in fig.&nbsp;3. Imagine that we add a new point to search for similar vectors. The normal approach is that we compute the distance between this node and <code>A</code>, <code>B</code>, <code>C</code> etc. But if you look at it close, all the existing nodes are already projected on the vector <code>y</code>, and we can also project the incoming query vector on <code>y</code>, and check to which of these points it is close to. Instead of computing the distances of two vectors, now we only compute the absolute value of subtraction of two numbers (since we can always project a vector onto another one and get a real number as result) as the distance. By replacing the original space with the projected one, we are able to achieve a significant reduction in storage and non-trivial gains in searching performance.</p>
<p>Of course, it is not always an accurate estimation. In the first subfigure of fig.&nbsp;3, a node can be physically close to <code>A</code> or <code>B</code>, but its projection could be closest to that of <code>C</code>. That again requires us to consider using multiple RP-trees. But instead of the actual vector content, in the leaf node of the trees we store only <code>(index, projected value)</code>. Now for the input query vector, we run it in the <span class="math inline">\(N\)</span> RP-trees and get <span class="math inline">\(N\)</span> set of <code>(index, value)</code> pairs. Here each <code>value</code> is the absolute value of the difference of projected values between the vector in the tree and the query vector itself. Each vector of course is label by a unique index.</p>
<p>For each index, we propose to use this metric: <span class="math inline">\(\frac{\sum~d_i}{\sum~c_i}\)</span> to measure how close it is to the query vector. Here <span class="math inline">\(d_i\)</span> is the distance between node <span class="math inline">\(i\)</span> and query node on projected space, and <span class="math inline">\(c_i\)</span> is the count of total number of node <span class="math inline">\(i\)</span> in all the candidate sets from all the RP-trees. Smaller measurement means closer distance. The intuition is that, if distance value of a node on the projected space is small, then it is possibly close to the query node; or, if a node appears many times from the candidate sets of different RP-trees, it is also quite likely a possible close neighbour.</p>
<p>As a further improvement, we update this metric to <span class="math inline">\(\frac{\sum~d_i}{(\sum~c_i)^3}\)</span>. By so doing, we give much more weight on the points which have multiple occurrences from different RP-trees by assuming that such points are more likely to be the true k-NN. Experiment results confirm that by using this metric it is feasible to use only the projected space in the actual system implementation. Please refer to the original paper if you are interested with more detail.</p>
</section>
<section class="level2" id="code-implementation">
<h2>Code Implementation</h2>
<p>Naive implementation …. let’s decide how to use them.</p>
<p><strong>The following is about how to do the random projection, for sparse projection</strong></p>
<div class="highlight">
<pre><code class="language-ocaml">(* make projection matrix, return as row vectors *)
let make_projection_matrix seed m n =
  Owl_stats_prng.init seed;
  Mat.gaussian m n |&gt; Mat.to_arrays

(* make transposed projection matrix of shape [n x m] *)
let make_projection_transpose seed m n =
  Owl_stats_prng.init seed;
  Mat.gaussian m n |&gt; Mat.transpose |&gt; Mat.to_arrays

(* make the matrix to save projected results *)
let make_projected_matrix m n =
  Array.init m (fun _ -&gt; Array.make n 0.)

(* the actual project function, for doc [i] on level [j] *)
let project i j s projection projected =
  let r = ref 0. in
  Array.iter (fun (w, a) -&gt;
    r := !r +. a *. projection.(w).(j);
  ) s;
  projected.(j).(i) &lt;- !r

(* project vector [s] to up to [level] *)
let project_vec s level projection =
  let projected = Array.make level 0. in
  for j = 0 to level - 1 do
    let r = ref 0. in
    Array.iter (fun (w, a) -&gt;
      r := !r +. a *. projection.(w).(j);
    ) s;
    projected.(j) &lt;- !r
  done;
  projected

(* random projection of sparse data set *)
let random seed cluster tfidf =
  (* matrix to be projected: num_doc x vocab *)
  let num_doc = Nlp.Tfidf.length tfidf in
  let vocab_len = Nlp.Tfidf.vocab_len tfidf in
  let level = Maths.log2 (float_of_int num_doc /. cluster) |&gt; ceil |&gt; int_of_float in

  (* random projection matrix: vocab x level *)
  let projection = make_projection_matrix seed vocab_len level in
  (* projected result transpose: level x num_doc *)
  let projected = make_projected_matrix level num_doc in

  Nlp.Tfidf.iteri (fun i s -&gt;
    for j = 0 to level - 1 do
      project i j s projection projected;
    done;
  ) tfidf;

  (* return the shape of projection, the projected *)
  vocab_len, level, projected</code></pre>
</div>
<p><strong>The following is about how to do the random projection, for dense projection</strong></p>
<div class="highlight">
<pre><code class="language-ocaml">(* make projection matrix *)
let make_projection_matrix seed m n =
  Owl_stats_prng.init seed;
  Mat.gaussian m n


(* random projection of dense data set *)
let random seed cluster data =
  (* data to be projected: m x n *)
  let m = Mat.row_num data in
  let n = Mat.col_num data in
  let level = Maths.log2 (float_of_int m /. cluster) |&gt; ceil |&gt; int_of_float in

  (* random projection matrix: n x level *)
  let projection = make_projection_matrix seed n level in
  (* projected result transpose: level x m *)
  let projected = Mat.dot data projection |&gt; Mat.transpose in

  (* return the shape of projection, the projected *)
  n, level, projected, projection</code></pre>
</div>
<p><strong>The following is about how to build the index - a binary saerch tree.</strong></p>
<p>Split the space into subspaces …</p>
<div class="highlight">
<pre><code class="language-ocaml">type t =
  | Node of float * t * t  (* intermediate nodes: split, left, right *)
  | Leaf of int array      (* leaves only contains doc_id *)


(* divide the projected space into subspaces to assign left and rigt subtrees,
  the criterion of division is the median value.
  The passed in [space] is the projected values on a specific level.
 *)
let split_space_median space =
  let space_size = Array.length space in
  let size_of_l = space_size / 2 in
  let size_of_r = space_size - size_of_l in
  (* sort into increasing order for median value *)
  Array.sort (fun x y -&gt; Pervasives.compare (snd x) (snd y)) space;
  let median =
    match size_of_l &lt; size_of_r with
    | true  -&gt; snd space.(size_of_l)
    | false -&gt; (snd space.(size_of_l-1) +. snd space.(size_of_l)) /. 2.
  in
  let l_subspace = Array.sub space 0 size_of_l in
  let r_subspace = Array.sub space size_of_l size_of_r in
  median, l_subspace, r_subspace


(* based on the doc_id of the points in the subspace, filter the projected space,
  both spaces and the reutrn are of the same format (doc_id, projected value).
  The purpose of this function is to update the projected value using specified
  level so the recursion can continue.
 *)
let filter_projected_space level projected subspace =
  let plevel = projected.(level) in
  Array.map (fun (doc_id, _) -&gt; doc_id, plevel.(doc_id)) subspace</code></pre>
</div>
<p>Build the binary tree …</p>
<div class="highlight">
<pre><code class="language-ocaml">(* recursively grow the subtree to make a whole tree.
  [projected] is of shape [level x num_doc].
  [subspace] is of shape [1 x num_doc].
 *)
let rec make_subtree level projected subspace =
  let num_levels = Array.length projected in
  match level = num_levels with
  | true  -&gt; (
      (* only keep the doc_id in the leaf *)
      let leaf = Array.map fst subspace in
      Leaf leaf
    )
  | false -&gt; (
      let median, l_space, r_space = split_space_median subspace in
      (* update the projected values for the next level *)
      let l_space = match level &lt; num_levels - 1 with
        | true  -&gt; filter_projected_space (level+1) projected l_space
        | false -&gt; l_space
      in
      let r_space = match level &lt; num_levels - 1 with
        | true  -&gt; filter_projected_space (level+1) projected r_space
        | false -&gt; r_space
      in
      (* NOTE: this is NOT tail recursion *)
      let l_subtree = make_subtree (level+1) projected l_space in
      let r_subtree = make_subtree (level+1) projected r_space in
      Node (median, l_subtree, r_subtree)
    )

(* build binary search tree, the passed in [projected] variable contains the
  projected points of shape [level x num_doc]. Currently everything is done in
  memory for efficiency consideration.
 *)
let grow projected =
  (* initialise the first subspace at level 0 *)
  let subspace = Array.mapi (fun doc_id x -&gt; (doc_id, x)) projected.(0) in
  (* start recursively making the subtrees from level 0 *)
  let tree_root = make_subtree 0 projected subspace in
  tree_root</code></pre>
</div>
<p>How search is done …</p>
<div class="highlight">
<pre><code class="language-ocaml">(* traverse the whole tree to locate the cluster for a projected vector [x],
  level: currrent level of the tree/recursion.
 *)
let rec traverse node level x =
  match node with
  | Leaf n         -&gt; n
  | Node (s, l, r) -&gt; (
      (* NOTE: be consistent with split_space_median *)
      match x.(level) &lt; s with
      | true  -&gt; traverse l (level+1) x
      | false -&gt; traverse r (level+1) x
    )

(* iterate all the leaves in a tree and apply function [f] *)
let rec iter_leaves f node =
  match node with
  | Leaf n         -&gt; f n
  | Node (s, l, r) -&gt; iter_leaves f l; iter_leaves f r

(* return the leaves which have [id] inside *)
let search_leaves node id =
  let leaf = ref [||] in
  (
    try iter_leaves (fun l -&gt;
      if Array.mem id l = true then (
        leaf := l;
        failwith "found";
      )
    ) node
    with exn -&gt; ()
  );
  Array.copy !leaf

(* wrapper of traverse function *)
let query tree x = traverse tree 0 x</code></pre>
</div>
<p><strong>How to count votes?</strong></p>
<div class="highlight">
<pre><code class="language-ocaml">let count_votes nn =
  let h = Hashtbl.create 128 in
  Owl_utils.aarr_iter (fun x -&gt;
    match Hashtbl.mem h x with
    | true  -&gt; (
        let c = Hashtbl.find h x in
        Hashtbl.replace h x (c + 1)
      )
    | false -&gt; Hashtbl.add h x 1
  ) nn;
  let r = Array.make (Hashtbl.length h) (0,0) in
  let l = ref 0 in
  Hashtbl.iter (fun doc_id votes -&gt;
    r.(!l) &lt;- (doc_id, votes);
    l := !l + 1;
  ) h;
  Array.sort (fun x y -&gt; Pervasives.compare (snd y) (snd x)) r;
  r</code></pre>
</div>
</section>
<section class="level2" id="make-it-live">
<h2>Make It Live</h2>
<p>LWT-web etc. OCaml code, split the following code into smaller snippets. … require http, so use text first</p>
<div class="highlight">
<pre><code class="language-text">(* some simple preprocessing using regular expression. This needs some fine
  tuning in the final product, but needs to be simple and fast.
 *)
let simple_preprocess_query_string s =
  let regex = Str.regexp "[=+%0-9]+" in
  Str.global_replace regex " " s

(* parse the query and extract the parameters *)
let extract_query_params s =
  let regex = Str.regexp "num=\\([0-9]+\\)" in
  let _ = Str.search_forward regex s 0 in
  let num = Str.matched_group 1 s |&gt; int_of_string in

  let regex = Str.regexp "mode=\\([a-z]+\\)" in
  let _ = Str.search_forward regex s 0 in
  let mode = Str.matched_group 1 s in

  let regex = Str.regexp "doc=\\(.+\\)" in
  let _ = Str.search_forward regex s 0 in
  let doc = Str.matched_group 1 s in

  (num, mode, doc)

(* core query service that runs forever *)
let start_service lda idx =
  let num_query = ref 0 in
  let callback _conn req body =
    body |&gt; Cohttp_lwt_body.to_string &gt;|= (fun body -&gt;
      let query_len = String.length body in
      match query_len &gt; 1 with
      | true  -&gt; (
          try (
            let num, mode, doc = extract_query_params body in
            Log.info "process query #%i ... %i words" !num_query query_len;
            num_query := !num_query + 1;

            let doc = simple_preprocess_query_string doc in
            match mode with
            | "linear" -&gt; query_linear_search ~k:num lda doc
            | "kvasir" -&gt; query_kvasir_idx ~k:num idx lda doc
            | _        -&gt; failwith "kvasir:unknown search mode"
          )
          with exn -&gt; "something bad happened :("
      )
      | false -&gt; (
          (* ignore empty queries *)
          Log.warn "ignore an empty query";
          ""
        )
    )
    &gt;&gt;= (fun body -&gt; Server.respond_string ~status:`OK ~body ())
  in
  Server.create ~mode:(`TCP (`Port 8000)) (Server.make ~callback ())</code></pre>
</div>
</section>
<section class="level2 unnumbered" id="references">
<h2 class="unnumbered">References</h2>
<div role="doc-bibliography" class="references hanging-indent" id="refs">
<div id="ref-dasgupta2013randomized">
<p>Dasgupta, Sanjoy, and Kaushik Sinha. 2013. “Randomized Partition Trees for Exact Nearest Neighbor Search.” In <em>Conference on Learning Theory</em>, 317–37.</p>
</div>
<div id="ref-7840682">
<p>Hyvönen, V., T. Pitkänen, S. Tasoulis, E. Jääsaari, R. Tuomainen, L. Wang, J. Corander, and T. Roos. 2016. “Fast Nearest Neighbor Search Through Sparse Random Projections and Voting.” In <em>2016 Ieee International Conference on Big Data (Big Data)</em>, 881–88. <a href="https://doi.org/10.1109/BigData.2016.7840682">https://doi.org/10.1109/BigData.2016.7840682</a>.</p>
</div>
<div id="ref-7462177">
<p>Wang, L., S. Tasoulis, T. Roos, and J. Kangasharju. 2016. “Kvasir: Scalable Provision of Semantically Relevant Web Content on Big Data Framework.” <em>IEEE Transactions on Big Data</em> 2 (3): 219–33. <a href="https://doi.org/10.1109/TBDATA.2016.2557348">https://doi.org/10.1109/TBDATA.2016.2557348</a>.</p>
</div>
</div>
</section>
</section>
</article></div><a href="case-finance.html" class="next-chapter"><div class="content"><h1><small>Next: Chapter 32</small>Case - Applications in Finance</h1></div></a><footer><div class="content"><ul><li><a href="http://ocaml.xyz">ocaml.xyz</a></li><li><a href="https://github.com/ryanrhymes">GitHub</a></li></ul><p>Copyright 2017-2020 Liang Wang.</p></div></footer><script src="js/jquery.min.js"></script><script src="js/min/app-min.js"></script></body></html>